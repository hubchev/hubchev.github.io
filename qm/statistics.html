<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Statistics | Quantitative Methods</title>
  <meta name="description" content="3 Statistics | Quantitative Methods" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Statistics | Quantitative Methods" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Statistics | Quantitative Methods" />
  
  
  

<meta name="author" content="© Prof. Dr. Stephan Huber (Stephan.Huber@hs-fresenius.de)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="identification.html"/>
<link rel="next" href="hands-on-experiments.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="preamble.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="doing-research.html"><a href="doing-research.html"><i class="fa fa-check"></i><b>1</b> Doing research</a>
<ul>
<li class="chapter" data-level="1.1" data-path="doing-research.html"><a href="doing-research.html#everybody-can-do-research"><i class="fa fa-check"></i><b>1.1</b> Everybody can do research</a></li>
<li class="chapter" data-level="1.2" data-path="doing-research.html"><a href="doing-research.html#its-difficult-to-do-good-research"><i class="fa fa-check"></i><b>1.2</b> It’s difficult to do good research</a></li>
<li class="chapter" data-level="1.3" data-path="doing-research.html"><a href="doing-research.html#asking-questions-lika-a-good-researcher"><i class="fa fa-check"></i><b>1.3</b> Asking questions lika a good researcher</a></li>
<li class="chapter" data-level="1.4" data-path="doing-research.html"><a href="doing-research.html#features-of-good-research"><i class="fa fa-check"></i><b>1.4</b> Features of good research</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="doing-research.html"><a href="doing-research.html#reliability-and-validity"><i class="fa fa-check"></i><b>1.4.1</b> Reliability and validity</a></li>
<li class="chapter" data-level="1.4.2" data-path="doing-research.html"><a href="doing-research.html#generalizability"><i class="fa fa-check"></i><b>1.4.2</b> Generalizability</a></li>
<li class="chapter" data-level="1.4.3" data-path="doing-research.html"><a href="doing-research.html#replicability-reproducibility-transparency-and-other-criteria"><i class="fa fa-check"></i><b>1.4.3</b> Replicability, reproducibility, transparency, and other criteria</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="doing-research.html"><a href="doing-research.html#the-role-of-resources-data-and-ethics"><i class="fa fa-check"></i><b>1.5</b> The role of resources, data and ethics</a></li>
<li class="chapter" data-level="" data-path="doing-research.html"><a href="doing-research.html#glossary"><i class="fa fa-check"></i>Glossary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="identification.html"><a href="identification.html"><i class="fa fa-check"></i><b>2</b> Identification</a>
<ul>
<li class="chapter" data-level="2.1" data-path="identification.html"><a href="identification.html#how-to-get-data"><i class="fa fa-check"></i><b>2.1</b> How to get data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="identification.html"><a href="identification.html#interviews"><i class="fa fa-check"></i><b>2.1.1</b> Interviews</a></li>
<li class="chapter" data-level="2.1.2" data-path="identification.html"><a href="identification.html#surveys"><i class="fa fa-check"></i><b>2.1.2</b> Surveys</a></li>
<li class="chapter" data-level="2.1.3" data-path="identification.html"><a href="identification.html#case-studies"><i class="fa fa-check"></i><b>2.1.3</b> Case studies</a></li>
<li class="chapter" data-level="2.1.4" data-path="identification.html"><a href="identification.html#experiments"><i class="fa fa-check"></i><b>2.1.4</b> Experiments</a></li>
<li class="chapter" data-level="2.1.5" data-path="identification.html"><a href="identification.html#observational-data"><i class="fa fa-check"></i><b>2.1.5</b> Observational data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="identification.html"><a href="identification.html#cornotcaus"><i class="fa fa-check"></i><b>2.2</b> Correlation does not imply causation</a></li>
<li class="chapter" data-level="2.3" data-path="identification.html"><a href="identification.html#the-fundamental-problem-of-causal-inference"><i class="fa fa-check"></i><b>2.3</b> The fundamental problem of causal inference</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="identification.html"><a href="identification.html#simpsons-paradox"><i class="fa fa-check"></i><b>2.3.1</b> Simpsons Paradox</a></li>
<li class="chapter" data-level="2.3.2" data-path="identification.html"><a href="identification.html#rubin-causal-model"><i class="fa fa-check"></i><b>2.3.2</b> Rubin causal model</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="identification.html"><a href="identification.html#its-difficult-to-overcome-the-fundamental-problem"><i class="fa fa-check"></i><b>2.4</b> Its difficult to overcome the fundamental problem</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="identification.html"><a href="identification.html#ignorability"><i class="fa fa-check"></i><b>2.4.1</b> Ignorability</a></li>
<li class="chapter" data-level="2.4.2" data-path="identification.html"><a href="identification.html#unconfoundedness"><i class="fa fa-check"></i><b>2.4.2</b> Unconfoundedness</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>3</b> Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="statistics.html"><a href="statistics.html#sampling"><i class="fa fa-check"></i><b>3.1</b> Sampling</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="statistics.html"><a href="statistics.html#the-hite-report"><i class="fa fa-check"></i><b>3.1.1</b> The Hite Report</a></li>
<li class="chapter" data-level="3.1.2" data-path="statistics.html"><a href="statistics.html#sample-design"><i class="fa fa-check"></i><b>3.1.2</b> Sample design</a></li>
<li class="chapter" data-level="3.1.3" data-path="statistics.html"><a href="statistics.html#sample-size-matters"><i class="fa fa-check"></i><b>3.1.3</b> Sample size matters</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>3.2</b> Descriptive statistics</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="statistics.html"><a href="statistics.html#univariate-data"><i class="fa fa-check"></i><b>3.2.1</b> Univariate data</a></li>
<li class="chapter" data-level="3.2.2" data-path="statistics.html"><a href="statistics.html#bivariate-data"><i class="fa fa-check"></i><b>3.2.2</b> Bivariate data</a></li>
<li class="chapter" data-level="3.2.3" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.2.3</b> Simple linear regression</a></li>
<li class="chapter" data-level="3.2.4" data-path="statistics.html"><a href="statistics.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.2.4</b> Multiple linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hands-on-experiments.html"><a href="hands-on-experiments.html"><i class="fa fa-check"></i><b>4</b> Hands on experiments</a>
<ul>
<li class="chapter" data-level="4.1" data-path="hands-on-experiments.html"><a href="hands-on-experiments.html#natural-experiments"><i class="fa fa-check"></i><b>4.1</b> Natural experiments</a></li>
<li class="chapter" data-level="4.2" data-path="hands-on-experiments.html"><a href="hands-on-experiments.html#empirical-evidence-bombing"><i class="fa fa-check"></i><b>4.2</b> Empirical evidence: Bombing</a></li>
<li class="chapter" data-level="4.3" data-path="hands-on-experiments.html"><a href="hands-on-experiments.html#field-experiments-would-you-work-more-if-wages-are-high"><i class="fa fa-check"></i><b>4.3</b> Field experiments: Would you work more if wages are high?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="hands-on-observational-data.html"><a href="hands-on-observational-data.html"><i class="fa fa-check"></i><b>5</b> Hands on observational data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="hands-on-observational-data.html"><a href="hands-on-observational-data.html#difference-in-difference"><i class="fa fa-check"></i><b>5.1</b> Difference in difference</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>6</b> Appendix</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li>&copy; Prof. Dr. Stephan Huber.</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Statistics<a href="statistics.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="sampling" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Sampling<a href="statistics.html#sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we learn…</p>
<ul>
<li>… what characterizes a <em>good</em> sample, i.e., a sample that can be used for empirical analysis using statistical inference.</li>
<li>… to distinguish between a sample and a population.</li>
<li>… to identify biased samples.</li>
<li>… to distinguish between random sampling and random assignment.</li>
<li>… that a simple random sample is the gold standard because of its properties.</li>
<li>… different ways to collect data and draw a sample, respectively (stratification, clustering, opportunity)</li>
</ul>
<div id="the-hite-report" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> The Hite Report<a href="statistics.html#the-hite-report" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 1976, when the <em>The Hite Report</em> <span class="citation">(see <a href="references.html#ref-Hite1976Hite" role="doc-biblioref">Hite, 1976</a>)</span> was published it instantly became a best seller.
Hite used an individualistic research method. Thousands of responses from anonymous questionnaires were used as a framework to develop a discourse on human responses to gender and sexuality. The following comic concludes the main results.</p>
<div class="figure"><span style="display:block;" id="fig:hitereport"></span>
<img src="fig/hite.jpeg" style="width:25.0%" alt="" />
<p class="caption">Figure 3.1:  The Hite Report</p>
</div>
<div class="figure"><span style="display:block;" id="fig:hitedo"></span>
<img src="fig/hitereport.jpg" style="width:75.0%" alt="" />
<p class="caption">Figure 3.2:  Comic on the Hite Report<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
</div>
<p>The picture of womens’ sexuality in <span class="citation">Hite (<a href="references.html#ref-Hite1976Hite" role="doc-biblioref">1976</a>)</span> was probably a bit biased as the sample can hardly be considered to be a <strong>random and unbiased</strong> one:</p>
<ul>
<li>Less than 5% of all questionnaires which were sent out were filled out and returned (response bias).</li>
<li>The questions were only sent out to women’s organizations (an opportunity sample).</li>
</ul>
<p>Thus, the results were based on a sample of women who were highly motivated to answer survey’s questions, for whatever reason.</p>
</div>
<div id="sample-design" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Sample design<a href="statistics.html#sample-design" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In statistics and quantitative research methodology, a sample is a group of individuals or objects that are collected or selected from a statistical population using a defined procedure. The elements of a sample are called sample points, sampling units, or observations.</p>
<p>Usually, the population is very large, and therefore, conducting a census or complete enumeration of all individuals in the population is either impractical or impossible. Therefore, a sample is taken to represent a manageable subset of the population. Data is collected from the sample, and statistics are calculated to make inferences or extrapolations from the sample to the population.</p>
<p>In statistics, we often rely on a <strong>sample</strong>, that is, a small subset of a larger set of
data, to draw inferences about the larger set. The larger set is known as the
<strong>population</strong> from which the sample is drawn.</p>
<p>Researchers adopt a variety of sampling strategies. The most straightforward is
<strong>simple random sampling</strong>. Such sampling requires every member of the population
to have an equal chance of being selected into the sample. In addition, the selection
of one member must be independent of the selection of every other member. That
is, picking one member from the population must not increase or decrease the
probability of picking any other member (relative to the others). In this sense, we
can say that simple random sampling chooses a sample by pure chance. To check
your understanding of simple random sampling, consider the following example.
What is the population? What is the sample? Was the sample picked by simple
random sampling? Is it biased?</p>
<div id="random-sampling" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Random sampling<a href="statistics.html#random-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Random sampling is a sampling procedure by which each member of a population has an equal chance of being included in the sample. Random sampling ensures a representative sample. There are several types of random sampling. In simple random sampling, not only each item in the population but each sample has an equal probability of being picked. In systematic sampling, items are selected from the population at uniform intervals of time, order, or space (as in picking every one-hundredth name from a telephone directory). Systematic sampling can be biased easily, such as, for example, when the amount of household garbage is measured on Mondays (which includes the weekend garbage). In stratified and cluster sampling, the population is divided into strata (such as age groups) and clusters (such as blocks of a city) and then a proportionate number of elements is picked at random from each stratum and cluster. Stratified sampling is used when the variations within each stratum are small in relation to the variations between strata. Cluster sampling is used when the opposite is the case. In what follows, we assume simple random sampling. Sampling can be from a finite population (as in picking cards from a deck without replacement) or from an infinite population (as in picking parts produced by a continuous process or cards from a deck with replacement).</p>
<p>In statistics, a <strong>simple random sample</strong> is a subset of individuals (a sample) chosen from a larger set (a population). Each individual is chosen randomly and entirely by chance, such that each individual has the same probability of being chosen at any stage during the sampling process, and each subset of k individuals has the same probability of being chosen for the sample as any other subset of k individuals.</p>
<p>The simple random sample has two important properties:</p>
<ol style="list-style-type: decimal">
<li><strong>UNBIASED:</strong> Each unit has the same chance of being chosen.</li>
<li><strong>INDEPENDENCE:</strong> Selection of one unit has no influence on the selection of other units.</li>
</ol>
<div class="exercise">
<p><span id="exr:randomsampling" class="exercise"><strong>Exercise 3.1  </strong></span>Random sampling</p>
<ul>
<li>What is meant by random sampling (simple random sample)?</li>
<li>What is its importance?</li>
<li>Why is having a large sample always better than having a small(er) one?</li>
</ul>
<p>Please find solution to the exercise <a href="appendix.html#sol:randomsampling">here.</a></p>
</div>
<div class="exercise">
<p><span id="exr:examplesampleerror" class="exercise"><strong>Exercise 3.2  </strong></span>Examples of sample errors</p>
<p>Watch <a href="https://creativemaths.net/videos/video-variation-sources">Sampling error and variation</a> and read the following examples:</p>
<p><strong>Example 1:</strong> You have been hired by the National Election Commission to
examine how the American people feel about the fairness of the voting procedures in the U.S. Who will you ask?</p>
<p>It is not practical to ask every single American how he or she feels about the
fairness of the voting procedures. Instead, we query a relatively small number of
Americans, and draw inferences about the entire country from their responses. The
Americans actually queried constitute our sample of the larger population of all
Americans. The mathematical procedures whereby we convert information about
the sample into intelligent guesses about the population fall under the rubric of
inferential statistics.
A sample is typically a small subset of the population. In the case of voting
attitudes, we would sample a few thousand Americans drawn from the hundreds of
millions that make up the country. In choosing a sample, it is therefore crucial that
it not over-represent one kind of citizen at the expense of others. For example,
something would be wrong with our sample if it happened to be made up entirely
of Florida residents. If the sample held only Floridians, it could not be used to infer
the attitudes of other Americans. The same problem would arise if the sample were
comprised only of Republicans. __Inferential statistics are based on the assumption
that sampling is random}. We trust a random sample to represent different segments
of society in close to the appropriate proportions (provided the sample is large
enough; see below).</p>
<p><strong>Example 2:</strong> We are interested in examining how many math classes have
been taken on average by current graduating seniors at American colleges
and universities during their four years in school. Whereas our population in
the last example included all US citizens, now it involves just the graduating
seniors throughout the country. This is still a large set since there are
thousands of colleges and universities, each enrolling many students. It would be
prohibitively costly to examine the transcript of every college senior. We
therefore take a sample of college seniors and then make inferences to the
ntire population based on what we find. To make the sample, we might first
choose some public and private colleges and universities across the United
States. Then we might sample 50 students from each of these institutions.
Suppose that the average number of math classes taken by the people in our
sample were 3.2. Then we might speculate that 3.2 approximates the number
we would find if we had the resources to examine every senior in the entire
population. But we must <strong>be careful about the possibility that our sample is
non-representative of the population</strong>. Perhaps we chose an overabundance of
math majors, or chose too many technical institutions that have heavy math
requirements. Such bad sampling makes our sample unrepresentative of the
population of all seniors.
To solidify your understanding of sampling bias, consider the following
example. Try to identify the population and the sample, and then reflect on
whether the sample is likely to yield the information desired.</p>
<p><strong>Example 3:</strong> A substitute teacher wants to know how students in the class
did on their last test. The teacher asks the 10 students sitting in the front row
to state their latest test score. He concludes from their report that the class
did extremely well. What is the sample? What is the population? Can you
identify any problems with choosing the sample in the way that the teacher
did?</p>
<p>In Example 3, the population consists of all students in the class. The sample is
made up of just the 10 students sitting in the front row. <strong>The sample is not likely to
be representative of the population</strong>. Those who sit in the front row tend to be more
interested in the class and tend to perform higher on tests. Hence, the sample may
perform at a higher level than the population.</p>
<p><strong>Example 4:</strong> A coach is interested in how many cartwheels the average
college freshmen at his university can do. Eight volunteers from the
freshman class step forward. After observing their performance, the coach
concludes that college freshmen can do an average of 16 cartwheels in a row
without stopping.</p>
<p>In Example 4, the population is the class of all freshmen at the coach’s university.
The sample is composed of the 8 volunteers. The sample is poorly chosen because
<strong>volunteers are more likely to be able to do cartwheels</strong> than the average freshman;
people who can’t do cartwheels probably did not volunteer! In the example, we are
also not told of the gender of the volunteers. Were they all women, for example?
That might affect the outcome, contributing to the non-representative nature of the
sample.</p>
<p><strong>Example 5:</strong>
Sometimes it is not feasible to build a sample using simple random sampling. To
see the problem, consider the fact that both Dallas and Houston are competing to
be hosts of the 2012 Olympics. Imagine that you are hired to assess whether most
Texans prefer Houston to Dallas as the host, or the reverse. Given the
impracticality of obtaining the opinion of every single Texan, you must construct a
sample of the Texas population. But now notice how difficult it would be to
proceed by simple random sampling. For example, how will you contact those
individuals who don’t vote and don’t have a phone? Even among people you find
in the telephone book, how can you identify those who have just relocated to
California (and had no reason to inform you of their move)? What do you do about
the fact that since the beginning of the study, an additional 4,212 people took up
residence in the state of Texas? As you can see, it is sometimes very difficult to
develop a truly random procedure.</p>
</div>
</div>
<div id="other-sampling-methods" class="section level4 hasAnchor" number="3.1.2.2">
<h4><span class="header-section-number">3.1.2.2</span> Other sampling methods<a href="statistics.html#other-sampling-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Systematic sampling</strong></p>
<p>Systematic sampling (a.k.a. interval sampling) relies on arranging the study population according to some ordering scheme and then selecting elements at regular intervals through that ordered list. Systematic sampling involves a random start and then proceeds with the selection of every k<span class="math inline">\(^{th}\)</span> element from then onwards.</p>
<p><strong>Accidental sampling / opportunity sampling / convenience sampling</strong></p>
<p>These sampling methods describe a type of nonprobability sampling which involves the sample being drawn from that part of the population which is close to hand. That is, a population is selected because it is readily available and convenient.</p>
<p><strong>Stratified sampling</strong></p>
<p>Since simple random sampling often does not ensure a representative sample, a
sampling method called stratified random sampling is sometimes used to make the
sample more representative of the population. This method can be used if the
population has a number of distinct groups. In stratified sampling, you
first identify members of your sample who belong to each group. Then you
randomly sample from each of those subgroups in such a way that the sizes of the
subgroups in the sample are proportional to their sizes in the population.
Let`s take an example: Suppose you were interested in views of capital
punishment at an urban university. You have the time and resources to interview
200 students. The student body is diverse with respect to age; many older people
work during the day and enroll in night courses (average age is 39), while younger
students generally enroll in day classes (average age of 19). It is possible that night
students have different views about capital punishment than day students. If 70%
of the students were day students, it makes sense to ensure that 70% of the sample
consisted of day students. Thus, your sample of 200 students would consist of 140
day students and 60 night students. The proportion of day students in the sample
and in the population (the entire university) would be the same. Inferences to the
entire population of students at the university would therefore be more secure.</p>
<p><strong>Cluster sampling</strong></p>
<p>Sometimes it is more cost-effective to select respondents in groups (clusters) of similar respondents. Sampling is often clustered by geography, or by time periods.</p>
</div>
<div id="random-assignment" class="section level4 hasAnchor" number="3.1.2.3">
<h4><span class="header-section-number">3.1.2.3</span> Random assignment<a href="statistics.html#random-assignment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In experimental research, populations are often hypothetical. For example, in an
experiment comparing the effectiveness of a new anti-depressant drug with a
placebo, there is no actual population of individuals taking the drug. In this case, a
specified population of people with some degree of depression is defined and a
random sample is taken from this population. The sample is then randomly divided
into two groups; one group is assigned to the treatment condition (drug) and the
other group is assigned to the control condition (placebo). This random division of
the sample into two groups is called random assignment. <strong>Random assignment is
critical for the validity of an experiment</strong>. For example, consider the bias that could
be introduced if the first 20 subjects to show up at the experiment were assigned to
the experimental group and the second 20 subjects were assigned to the control
group. It is possible that subjects who show up late tend to be more depressed than
those who show up early, thus making the experimental group less depressed than
the control group even before the treatment was administered.
In experimental research of this kind, failure to assign subjects randomly to
groups is generally more serious than having a non-random sample. Failure to
randomize (the former error) invalidates the experimental findings. A non-random
sample (the latter error) simply restricts the generalizability of the results.</p>
</div>
</div>
<div id="sample-size-matters" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Sample size matters<a href="statistics.html#sample-size-matters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is usually determined based on the cost, time, or convenience of collecting the data, and the need for it to offer sufficient statistical power.</p>
<p>Recall that the definition of a random sample is a sample in which every member of the population has an equal chance of being selected. This means that the <strong>sampling procedure</strong> rather than the <strong>results</strong> of the procedure define what it means for a sample to be random. Random samples, especially if the sample size is small, are not necessarily representative of the entire population.</p>
<p>Larger sample sizes generally lead to increased precision when estimating unknown parameters. For example, if we wish to know the proportion of a certain species of fish that is infected with a pathogen, we would generally have a more precise estimate of this proportion if we sampled and examined 200 rather than 100 fish. Several fundamental facts of mathematical statistics describe this phenomenon, including the <em>law of large numbers</em> and the <em>central limit theorem</em>.</p>
<p>A helpful slogan to keep in mind while scrutinizing statistical results is “garbage in, garbage out”. Regardless of how scientifically sound and visually appealing a statistic may appear, the formula used to derive it is oblivious to the quality of the data that underpins it. It is your responsibility to conduct a thorough examination. For example, if the data on which the statisitc is based emanates from a biased sample (one that favors certain individuals over others), a flawed design, unreliable data-collection protocols, or misleading questions, the margin of error becomes <em>bad</em>. If the bias is sufficiently severe, the outcomes become worthless.</p>
</div>
</div>
<div id="descriptive-statistics" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Descriptive statistics<a href="statistics.html#descriptive-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="univariate-data" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Univariate data<a href="statistics.html#univariate-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="arithmetic-mean" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Arithmetic mean<a href="statistics.html#arithmetic-mean" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The arithmetic mean (<span class="math inline">\(\bar{x}\)</span>) is calculated as the sum of all the values in a dataset divided by the total number of values:</p>
<p><span class="math display">\[
\bar{x} = \frac{{\sum_{i=1}^{n} x_i}}{n}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> represents the arithmetic mean, <span class="math inline">\(x_i\)</span> represents each individual value in the dataset, and <span class="math inline">\(n\)</span> represents the total number of values in the dataset.</p>
</div>
<div id="median" class="section level4 hasAnchor" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Median<a href="statistics.html#median" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The median is the middle value of a dataset when it is sorted in ascending or descending order. If the dataset has an odd number of values, the median is the middle value. If the dataset has an even number of values, the median is the average of the two middle values.</p>
</div>
<div id="mode" class="section level4 hasAnchor" number="3.2.1.3">
<h4><span class="header-section-number">3.2.1.3</span> Mode<a href="statistics.html#mode" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The mode is the value or values that appear most frequently in a dataset.</p>
</div>
<div id="range" class="section level4 hasAnchor" number="3.2.1.4">
<h4><span class="header-section-number">3.2.1.4</span> Range<a href="statistics.html#range" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The range is the difference between the maximum and minimum values in a dataset.</p>
<p><span class="math display">\[
\text{{Range}} = \max(x_i) - \min(x_i)
\]</span></p>
<p>where <span class="math inline">\(\text{{Range}}\)</span> represents the range value, and <span class="math inline">\(x_i\)</span> represents each individual value in the dataset.</p>
</div>
<div id="variance" class="section level4 hasAnchor" number="3.2.1.5">
<h4><span class="header-section-number">3.2.1.5</span> Variance<a href="statistics.html#variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The variance represents the average of the squared deviations of a random variable from its mean. It quantifies the extent to which a set of numbers deviates from their average value. Variance is commonly denoted as <span class="math inline">\(Var(X)\)</span>, <span class="math inline">\(\sigma^2\)</span>, or <span class="math inline">\(s^2\)</span>. The calculation of variance is as follows:
<span class="math display">\[
\sigma^2={\frac{1}{n}}\sum _{i=1}^{n}(x_{i}-\mu )^{2}
\]</span>
However, it is better to use
<span class="math display">\[
\sigma^2={\frac{1}{n-1}}\sum _{i=1}^{n}(x_{i}-\mu )^{2}.
\]</span></p>
<p>The use of <span class="math inline">\(n - 1\)</span> instead of <span class="math inline">\(n\)</span> in the formula for the sample variance is known as <em>Bessel’s correction</em>, which corrects the bias in the estimation of the population variance, and some, but not all of the bias in the estimation of the population standard deviation.
Consequently this way to calculate the variance and hence the standard deviation is called the <em>sample standard deviation</em> or the <em>unbiased estimation of standard deviation</em>.
In other words, when working with a sample instead of the full population the limited number of observations tend to be closer to the <em>sample mean</em> than to the <em>population mean</em>, see figure <a href="statistics.html#fig:bessel">3.3</a>. Bessels Correction takes that into account.</p>
<p>For a detailed explanation, you can watch the video by <a href="https://youtu.be/sHRBg6BhKjI">StatQuest with Josh Starmer: Why Dividing By N Underestimates the Variance</a></p>
<div class="figure"><span style="display:block;" id="fig:bessel"></span>
<img src="fig/bessel.png" style="width:75.0%" alt="" />
<p class="caption">Figure 3.3:  Bias when using the sample mean<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a></p>
</div>
</div>
<div id="standard-deviation" class="section level4 hasAnchor" number="3.2.1.6">
<h4><span class="header-section-number">3.2.1.6</span> Standard deviation<a href="statistics.html#standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As the variance is hard to interpret, the standard deviation is a more often used measure of dispersion.
A low standard deviation indicates that the values tend to be close to the mean.
It is often abbreviated with <span class="math inline">\(sd\)</span>, <span class="math inline">\(SD\)</span>, or most often with the Greek letter sigma, <span class="math inline">\(\sigma\)</span>.
The underlying idea is to measure the average deviation from the mean.
It is calculated as follows:
<span class="math display">\[
sd(x)=\sqrt{\sigma^2}={\sqrt {{\frac {1}{n-1}}\sum _{i=1}^{n}\left(x_{i}-{\mu}\right)^{2}}}=\sigma
\]</span></p>
</div>
<div id="standard-error" class="section level4 hasAnchor" number="3.2.1.7">
<h4><span class="header-section-number">3.2.1.7</span> Standard error<a href="statistics.html#standard-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. The SEM is always smaller than the SD. It matters because it helps you estimate how well your sample data represents the whole population.</p>
<p>The standard error of the mean (SEM) can be expressed as:
<span class="math display">\[
sd(\bar{x})=\sigma_{\bar {x}}\ = s = {\frac {\sigma }{\sqrt {n}}}
\]</span>
where <span class="math inline">\(\sigma\)</span> is the standard deviation of the population and <span class="math inline">\(n\)</span> is the size (number of observations) of the sample.</p>
<p>Also see the video by <a href="https://youtu.be/A82brFpdr9g">StatQuest with Josh Starmer: Standard Deviation vs Standard Error, Clearly Explained!!!</a></p>
<div id="why-divide-by-the-square-root-of-n" class="section level6 unlisted unnumbered hasAnchor">
<h6>Why divide by the square root of <span class="math inline">\(n\)</span>?<a href="statistics.html#why-divide-by-the-square-root-of-n" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Let <span class="math inline">\(X_{i}\)</span> be an independent draw from a distribution with mean <span class="math inline">\(\bar{x}\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>.
What is the variance of <span class="math inline">\(\bar{x}\)</span>?</p>
<p>By definition:
<span class="math display">\[
\operatorname{Var}(x)=E\left[\left(x_{i}-E\left[x_{i}\right]\right)^{2}\right]=\sigma^{2}
\]</span>
so
<span class="math display">\[\begin{align*}
\operatorname{Var}(\bar{x})&amp;=E\left[\left(\frac{\sum x_{i}}{n}-E \left[\frac{\sum x_{i}}{n}\right]\right)^{2}\right]\\
&amp;=E\left[\left(\frac{\sum x_{i}}{n}-\frac{1}{n} E\left[ \sum x_{i}\right]\right)^{2}\right]\\
&amp;=\frac{1}{n^{2}} E\left[\left(\sum x_{i}-E\left[\sum x_{i}\right]\right)^{2}\right]\\
&amp;=\frac{1}{n^{2}} E\left[\left(\sum x_{i}- \sum \bar{x}\right)^{2}\right]\\
&amp;=\frac{1}{n^{2}} E\left[(x_{1}+x_{2}+\cdots+x_{n}-\underbrace{\bar{x}-\bar{x}-\cdots -\bar{x}}_{n \text{ terms }})^{2}\right]\\
&amp;=\frac{1}{n^{2}} E\left[\sum\left(x_{i}-\bar{x}\right)^{2}\right]\\
&amp;=\frac{1}{n^{2}} \sum E\left(x_{i}-\bar{x}\right)^{2}\\
&amp;=\frac{1}{n^{2}} \underbrace{\sum \sigma^{2}}_{n\cdot \sigma^{2}}\\
&amp;=\frac{1}{n} \sigma^{2}
\end{align*}\]</span>
and hence
<span class="math display">\[
sd(\bar x)=\sqrt{\operatorname{Var}(\bar{x})}=s={\frac {\sigma }{\sqrt {n}}}
\]</span></p>
</div>
</div>
<div id="coefficient-of-variation" class="section level4 hasAnchor" number="3.2.1.8">
<h4><span class="header-section-number">3.2.1.8</span> Coefficient of variation<a href="statistics.html#coefficient-of-variation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The coefficient of variation (<span class="math inline">\(CoV\)</span>) is a relative measure of variability and is calculated as the ratio of the standard deviation to the mean, expressed as a percentage:</p>
<p><span class="math display">\[
CoV =  \frac{\sigma}{\bar{x}}
\]</span></p>
<p>where <span class="math inline">\(CoV\)</span> represents the coefficient of variation, <span class="math inline">\(\sigma\)</span> represents the standard deviation, and <span class="math inline">\(\bar{x}\)</span> represents the arithmetic mean.</p>
</div>
<div id="skewness" class="section level4 hasAnchor" number="3.2.1.9">
<h4><span class="header-section-number">3.2.1.9</span> Skewness<a href="statistics.html#skewness" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Skewness is a measure of the asymmetry of a distribution. There are different formulas to calculate skewness, but one common method is using the third standardized moment (<span class="math inline">\(\gamma_1\)</span>):</p>
<p><span class="math display">\[
\gamma_1 = \frac{{\sum_{i=1}^{n} \left(\frac{x_i - \bar{x}}{\sigma}\right)^3}}{n}
\]</span></p>
<p>where <span class="math inline">\(\gamma_1\)</span> represents the skewness, <span class="math inline">\(x_i\)</span> represents each individual value in the dataset, <span class="math inline">\(\bar{x}\)</span> represents the arithmetic mean, <span class="math inline">\(\sigma\)</span> represents the standard deviation, and <span class="math inline">\(n\)</span> represents the total number of values in the dataset.</p>
</div>
<div id="kurtosis" class="section level4 hasAnchor" number="3.2.1.10">
<h4><span class="header-section-number">3.2.1.10</span> Kurtosis<a href="statistics.html#kurtosis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Kurtosis measures the peakedness or flatness of a probability distribution. There are different formulations for kurtosis, and one of the common ones is the fourth standardized moment. The formula for kurtosis is given by:</p>
<p><span class="math display">\[
\text{{Kurtosis}} = \frac{{\frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^4}}{{\left(\frac{1}{n} \sum_{i=1}^{n}(x_i - \bar{x})^2\right)^2}}
\]</span></p>
<p>where <span class="math inline">\(\text{Kurtosis}\)</span> represents the kurtosis value, <span class="math inline">\(x_i\)</span> represents each individual value in the dataset, <span class="math inline">\(\bar{x}\)</span> represents the mean of the dataset, and <span class="math inline">\(n\)</span> represents the total number of values in the dataset.</p>
</div>
</div>
<div id="bivariate-data" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Bivariate data<a href="statistics.html#bivariate-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="covariance" class="section level4 hasAnchor" number="3.2.2.1">
<h4><span class="header-section-number">3.2.2.1</span> Covariance<a href="statistics.html#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Covariance <span class="math inline">\(Cov(X,Y)\)</span> (or <span class="math inline">\(\sigma_{XY}\)</span>) is a measure of the joint variability of two variables (<span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>) and their observations <span class="math inline">\(i\)</span>, respectively.
The covariance is positive when larger values of one variable tend to correspond with larger values of the other variable, or when smaller values of one variable tend to correspond with smaller values of the other variable. On the other hand, a negative covariance suggests an inverse relationship, where larger values of one variable tend to correspond with smaller values of the other variable.</p>
<p>It’s important to note that the magnitude of the covariance is influenced by the units of measurement, making it challenging to interpret directly. Additionally, the spread of the variables also affects the covariance.
The formula for calculating covariance is as follows:
<span class="math display">\[
\operatorname{Cov}(X,Y)=\sigma_{XY}={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})
\]</span>
where <span class="math inline">\(cov(X,Y)\)</span> represents the covariance, <span class="math inline">\(\sigma_{XY}\)</span> is an alternative notation, <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are the individual observations of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> are the means of variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(n\)</span> is the total number of observations.</p>
<p>To gain a better understanding of the concept and calculation of covariance, I highly recommend watching Josh Starmer’s informative and visually engaging video titled <a href="https://youtu.be/qtaqvPAeEJY">Covariance and Correlation Part 1: Covariance</a>.</p>
</div>
<div id="the-correlation-coefficient-bravais-pearson" class="section level4 hasAnchor" number="3.2.2.2">
<h4><span class="header-section-number">3.2.2.2</span> The correlation coefficient (Bravais-Pearson)<a href="statistics.html#the-correlation-coefficient-bravais-pearson" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Pearson correlation coefficient measures the linear relationship between two variables. It is calculated as the covariance of the variables divided by the product of their standard deviations.
<span class="math display">\[
\rho_{X,Y} = \frac{{\text{Cov}(X, Y)}}{{\sigma_X \sigma_Y}}
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> represents the Pearson correlation coefficient, <span class="math inline">\(\text{Cov}(X, Y)\)</span> denotes the covariance between variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(\sigma_X\)</span> denotes the standard deviation of variable <span class="math inline">\(X\)</span>, and <span class="math inline">\(\sigma_Y\)</span> denotes the standard deviation of variable <span class="math inline">\(Y\)</span>. It has a value between +1 and -1.</p>
<p>By dividing the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> by the multiplication of the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the correlation coefficient is normalized by having a minimum of -1 and a maximum of 1. Thus, it can fix the problem of the variance that the scale (unit of measurement) determines the size of the variance.</p>
<p>I highly recommend watching the video <a href="https://youtu.be/xZ_z8KWkhXE">Pearson’s Correlation, Clearly Explained!!!</a>
StatQuest with Josh Starmer. It provides a clear and engaging explanation of the meaning of correlation. The video features informative animations that help visualize the concept.</p>
<p>In interpreting correlations, it is important to remember that they…</p>
<ol style="list-style-type: decimal">
<li>… only reflect the strength and direction of linear relationships,</li>
<li>… do not provide information about the slope of the relationship, and</li>
<li>… fail to explain important aspects of nonlinear relationships.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corelateoverview"></span>
<img src="fig/corelate_overview.png" alt="Correlations are blind on some eye" width="75%" />
<p class="caption">
Figure 3.4: Correlations are blind on some eye
</p>
</div>
<p>Figure <a href="statistics.html#fig:corelateoverview">3.4</a> shows that correlation coefficients are limited in to explaining the relationship of two variables.
For example, when the slope of a relationship is zero, the correlation coefficient becomes undefined due to the variance of <span class="math inline">\(Y\)</span> being zero.
Furthermore, Pearson’s correlation coefficient is sensitive to outliers, and all correlation coefficients are prone to sample selection biases. It is crucial to be careful when attempting to correlate two variables, particularly when one represents a part and the other represents the total.
It is also worth noting that small correlation values do not necessarily indicate a lack of association between variables. For example, Pearson’s correlation coefficient can underestimates the association between variables exhibiting a quadratic relationship. Therefore, it is always advisable to examine scatterplots in conjunction with correlation analysis.</p>
<p>In figure <a href="statistics.html#fig:allcorsame">3.5</a> you see various graphs that all have the same correlation coefficient and share other statistical properties like is investigated in exercise <a href="statistics.html#exr:Datasaurus">3.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:allcorsame"></span>
<img src="fig/allcorsame.svg" alt="These diagrams all have the same statistical properties\protect\footnotemark"  />
<p class="caption">
Figure 3.5: These diagrams all have the same statistical properties
</p>
</div>
</div>
<div id="rank-correlation-coefficient-spearman" class="section level4 hasAnchor" number="3.2.2.3">
<h4><span class="header-section-number">3.2.2.3</span> Rank correlation coefficient (Spearman)<a href="statistics.html#rank-correlation-coefficient-spearman" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Spearman’s rank correlation coefficient is a measure of the strength and direction of the monotonic relationship between two variables.
It can be calculated for a sample of size <span class="math inline">\(n\)</span> by converting the <span class="math inline">\(n\)</span> raw scores <span class="math inline">\(X_i, Y_i\)</span> to ranks <span class="math inline">\(\text{R}(X_i), \text{R}(Y_i)\)</span>, then using the following formula:</p>
<p><span class="math display">\[
r_s = \rho_{\operatorname{R}(X),\operatorname{R}(Y)} = \frac{\text{cov}(\operatorname{R}(X), \operatorname{R}(Y))}{\sigma_{\operatorname{R}(X)} \sigma_{\operatorname{R}(Y)}},
\]</span></p>
<p>where
<span class="math inline">\(\rho\)</span> denotes the usual Pearson correlation coefficient, but applied to the rank variables,
<span class="math inline">\(\operatorname{cov}(\operatorname{R}(X), \operatorname{R}(Y))\)</span> is the covariance of the rank variables,
<span class="math inline">\(\sigma_{\operatorname{R}(X)}\)</span> and <span class="math inline">\(\sigma_{\operatorname{R}(Y)}\)</span> are the standard deviations of the rank variables.</p>
<p>If all <span class="math inline">\(n\)</span> ranks are distinct integers, you can use the handy formula:</p>
<p><span class="math display">\[
\rho = 1 - \frac{6\sum d_i^2}{n(n^2 - 1)}
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> denotes the correlation coefficient, <span class="math inline">\(\sum d_i^2\)</span> is the sum of squared differences between the ranks of corresponding pairs of variables, and <span class="math inline">\(n\)</span> represents the number of pairs of observations.</p>
<p>The coefficient ranges from -1 to 1. A value of 1 indicates a perfect increasing monotonic relationship, while a value of -1 indicates a perfect decreasing monotonic relationship. A value of 0 suggests no monotonic relationship between the variables.</p>
<p>Spearman’s rank correlation coefficient is a non-parametric measure and is often used when the relationship between variables is not linear or when the data is in the form of ranks or ordinal categories.</p>
<div class="exercise">
<p><span id="exr:Datasaurus" class="exercise"><strong>Exercise 3.3  </strong></span>Datasaurus</p>
<ol style="list-style-type: lower-alpha">
<li>Load the packages <code>datasauRus</code> and <code>tidyverse</code>. If necessary, install these packages.</li>
</ol>
</div>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>The package<code>datasauRus</code> comes with a dataset in two different formats: <code>datasaurus_dozen</code> and <code>datasaurus_dozen_wide</code>. Store them as <code>ds</code> and <code>ds_wide</code>.</p></li>
<li><p>Open and read the R vignette of the <code>datasauRus</code> package. Also open the R documentation of the dataset <code>datasaurus_dozen</code>.</p></li>
<li><p>Explore the dataset: What are the dimensions of this dataset? Look at the descriptive statistics.</p></li>
<li><p>How many unique values does the variable <code>dataset</code> of the tibble <code>ds</code> have? Hint: The function unique() return the unique values of a variable and the function length() returns the length of a vector, such as the unique elements.</p></li>
<li><p>Compute the mean values of the <code>x</code> and <code>y</code> variables for each entry in <code>dataset</code>. Hint: Use the group_by() function to group the data by the appropriate column and then the summarise() function to calculate the mean.</p></li>
<li><p>Compute the standard deviation, the correlation, and the median in the same way. Round the numbers.</p></li>
<li><p>What can you conclude?</p></li>
<li><p>Plot all datasets of <code>ds</code>. Hide the legend. Hint: Use the <code>facet_wrap()</code> and the <code>theme()</code> function.</p></li>
<li><p>Create a loop that generates separate scatter plots for each unique datatset of the tibble <code>ds</code>. Export each graph as a png file.</p></li>
<li><p>Watch the video <a href="https://youtu.be/T-kxUB29t0o">Animating the Datasaurus Dozen Dataset in R</a> from The Data Digest on YouTube.</p></li>
</ol>
<p>Please find the solutions <a href="https://htmlpreview.github.io/?https://raw.githubusercontent.com/hubchev/hubchev.github.io/main/various/datasaurus_solution.html">here</a>.</p>
<div class="exercise">
<p><span id="exr:summarystatistics" class="exercise"><strong>Exercise 3.4  </strong></span>Summary statistics</p>
<p>Calculate for the following datasets: the mode, the median, the 20% quantile, the range, the interquartile range, the variance, the arithmetic mean, the sample standard deviation, the coefficient of variation.</p>
<ol style="list-style-type: lower-alpha">
<li>For ten participants in a scientific conference the age
has been noted: <span class="math display">\[25, 21, 18, 37, 56, 89, 46, 23, 21, 34.\]</span></li>
<li>A random sample of 128 visitors of the Cupcake festival yielded the following frequencies regarding the cupcake consumption during their visit:</li>
</ol>
<table>
<thead>
<tr class="header">
<th>Cupcages consumed</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Abs. freq.</td>
<td align="right">2</td>
<td align="right">30</td>
<td align="right">37</td>
<td align="right">28</td>
<td align="right">23</td>
<td align="right">8</td>
</tr>
</tbody>
</table>
<p>Please find solution to the exercise <a href="appendix.html#sol:summarystatistics">here.</a></p>
</div>
<div class="exercise">
<p><span id="exr:destatexcel" class="exercise"><strong>Exercise 3.5  </strong></span>Summary statistics in spreadsheet software</p>
<p>Given is the following datset: <span class="math display">\[0, 0, 40, 50, 50, 60, 70, 90, 100, 100.\]</span>
Compute the following summary statistics of the data set using a spreadsheet software like or :
mean, median, mode, quartiles (Q1, Q2, Q3), range, interquartile range, variance, standard deviation, mean absolute deviation, coefficient of variation and skewness.</p>
<p>Please find solution to the exercise <a href="appendix.html#sol:destatexcel">here.</a></p>
</div>
<div class="exercise">
<p><span id="exr:guessstat" class="exercise"><strong>Exercise 3.6  </strong></span>Guess the summary statistics</p>
<p>Given are the following variables:</p>
<table>
<thead>
<tr class="header">
<th align="right">a</th>
<th align="right">b</th>
<th align="right">c</th>
<th align="right">d</th>
<th align="right">e</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">97</td>
<td align="right">70</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">970</td>
</tr>
<tr class="even">
<td align="right">98</td>
<td align="right">80</td>
<td align="right">50</td>
<td align="right">2</td>
<td align="right">980</td>
</tr>
<tr class="odd">
<td align="right">99</td>
<td align="right">90</td>
<td align="right">50</td>
<td align="right">3</td>
<td align="right">990</td>
</tr>
<tr class="even">
<td align="right">100</td>
<td align="right">100</td>
<td align="right">50</td>
<td align="right">4</td>
<td align="right">1000</td>
</tr>
<tr class="odd">
<td align="right">101</td>
<td align="right">110</td>
<td align="right">50</td>
<td align="right">5</td>
<td align="right">1010</td>
</tr>
<tr class="even">
<td align="right">102</td>
<td align="right">120</td>
<td align="right">50</td>
<td align="right">6</td>
<td align="right">1020</td>
</tr>
<tr class="odd">
<td align="right">103</td>
<td align="right">130</td>
<td align="right">99</td>
<td align="right">7</td>
<td align="right">1030</td>
</tr>
</tbody>
</table>
<p>Rank the variables without calculating concrete numbers accordingly to the values of the following descriptive statistics: mode, median, mean, range, variance, standard deviation, coefficient of variation.</p>
<p>Please find solution to the exercise <a href="appendix.html#sol:guessstat">here.</a></p>
</div>
</div>
</div>
<div id="simple-linear-regression" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Simple linear regression<a href="statistics.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The linear regression analysis is a widely used technique for predictive modeling. Its purpose is to establish a mathematical equation that relates a continuous response variable, denoted as <span class="math inline">\(y\)</span>, to one or more independent variables, represented by <span class="math inline">\(x\)</span>. The objective is to create a regression model that enables the prediction of the value of <span class="math inline">\(y\)</span> based on known values of <span class="math inline">\(x\)</span>.</p>
<p>To ensure meaningful predictions, it is important to have an adequate number of observations, denoted as <span class="math inline">\(i\)</span>, available for the variables of interest.</p>
<p>The linear regression model can be expressed as:
<span class="math display">\[
y_i = \beta_{0} + \beta_{1} x_i + \epsilon_i,
\]</span>
where the index <span class="math inline">\(i\)</span> denotes the individual observations, ranging from <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(n\)</span>. The variable <span class="math inline">\(y_i\)</span> represents the dependent variable, also known as the regressand. The variable <span class="math inline">\(x_i\)</span> represents the independent variable, also referred to as the regressor. <span class="math inline">\(\beta_0\)</span> denotes the intercept of the population regression line, a.k.a. the constant. <span class="math inline">\(\beta_1\)</span> denotes the slope of the population regression line. Lastly, <span class="math inline">\(\epsilon_i\)</span> refers to the error term or the residual, which accounts for the deviation between the predicted and observed values of <span class="math inline">\(y_i\)</span>.</p>
<p>By fitting a linear regression model, one aims to estimate the values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in order to obtain an equation that best captures the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p>While the correlation coefficient and the slope in simple linear regression are similar in many ways, it’s important to note that they are not identical. The correlation coefficient measures the strength and direction of the linear relationship between variables in a broader sense, while the slope in simple linear regression specifically quantifies the change in the dependent variable associated with a unit change in the independent variable.</p>
<div id="estimating-the-coefficients-of-the-linear-regression-model" class="section level4 hasAnchor" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> Estimating the coefficients of the linear regression model<a href="statistics.html#estimating-the-coefficients-of-the-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In practice, the intercept and slope of the regression are unknown. Therefore, we must employ data to estimate the unknown parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
The method we use is called the ordinary least squared (OLS) method. The idea is to minimize the sum of the squared differences of all <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_i^*\)</span> as sketched in figure <a href="statistics.html#fig:regressionols">3.6</a>.</p>
<div class="figure"><span style="display:block;" id="fig:regressionols"></span>
<img src="fig/regression_ols.png" style="width:50.0%" alt="" />
<p class="caption">Figure 3.6:  The fitted line and the residuals</p>
</div>
<p>Thus, we minimize the squared residuals by choosing the estimated coefficients <span class="math inline">\(\hat{\beta_{0}}\)</span> and <span class="math inline">\(\hat{\beta_{1}}\)</span>
<span class="math display">\[\begin{align*}
\min_{\hat{\beta_{0}}, \hat{\beta_{1}}}\sum_{i=1} \epsilon_i^2 &amp;= \sum_{i=1}    \left[y_i - \underbrace{(\hat{\beta_{0}} + \hat{\beta_{1}} x_i)}_{\text{predicted values}\equiv y_i^*}\right]^2\\
\Leftrightarrow  &amp;=  \sum_{i=1} (y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)^2
\end{align*}\]</span>
Minimizing the function requires to calculate the first order conditions with respect to alpha and beta and set them zero:
<span class="math display">\[\begin{align*}
\frac{\partial \sum_{i=1} \epsilon_i^2}{\partial \beta_{0}}=2 \sum_{i=1}    (y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)=0\\
\frac{\partial \sum_{i=1} \epsilon_i^2}{\partial \beta_{1}}=2 \sum_{i=1}    (y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)x_i=0
\end{align*}\]</span>
This is just a linear system of two equations with two unknowns <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>, which we can mathematically solve for <span class="math inline">\(\beta_0\)</span>:
<span class="math display">\[\begin{align*}
    &amp;\sum_{i=1} (y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)=0\\
    \Leftrightarrow \hat{\beta_{0}}&amp;=\frac{1}{n}\sum_{i=1}  (y_i  - \hat{\beta_{1}} x_i)\\
    \Leftrightarrow \hat{\beta_{0}}&amp;=\bar{y}-\hat{\beta_{1}}\bar{x}
\end{align*}\]</span>
and for <span class="math inline">\(\beta_{1}\)</span>:
<span class="math display">\[\begin{align*}
    &amp;\sum_{i=1} (y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)x_i=0\\
    \Leftrightarrow &amp; \sum_{i=1}    y_i x_i- \underbrace{\hat{\beta_{0}}}_{\bar{y}-\hat{\beta_{1}}\bar{x}}x_i - \hat{\beta_{1}} x_i^2=0\\
    \Leftrightarrow &amp; \sum_{i=1}    y_i x_i- (\bar{y}-\hat{\beta_{1}}\bar{x})x_i - \hat{\beta_{1}} x_i^2=0\\    
    \Leftrightarrow &amp; \sum_{i=1}    y_i x_i- \bar{y}x_i-\hat{\beta_{1}}\bar{x}x_i - \hat{\beta_{1}} x_i^2=0\\   
    \Leftrightarrow &amp; \sum_{i=1}    (y_i - \bar{y}-\hat{\beta_{1}}\bar{x} - \hat{\beta_{1}} x_i)x_i=0\\
%   \Leftrightarrow &amp; \sum_{i=1}    (y_i - \bar{y})-\beta_{1}\bar{x} - \hat{\beta_{1}} x_i=0\\
    \Leftrightarrow &amp; \sum_{i=1} (y_i - \bar{y}) x_i -\hat{\beta_{1}}(\bar{x} -  x_i)x_i =0\\
    \Leftrightarrow &amp; \sum_{i=1}    (y_i - \bar{y}) x_i  =  \hat{\beta_{1}} \sum_{i=1} (\bar{x} -  x_i) x_i \\
%   \Leftrightarrow &amp; \beta_{1} =\frac{\sum_{i=1}(y_i - \bar{y})x_i }{ \sum_{i=1} (\bar{x} -  x_i)x_i }\\
    \Leftrightarrow &amp; \hat{\beta_{1}} =\frac{\sum_{i=1}(y_i - \bar{y})x_i }{ \sum_{i=1} (\bar{x} -  x_i)x_i }\\
        \Leftrightarrow &amp; \hat{\beta_{1}} =\frac{\sum_{i=1}(y_i -\bar{y})(x_i-\bar{x})}{\sum_{i=1} (\bar{x} -  x_i)^2 }\\
    \Leftrightarrow &amp; \hat{\beta_{1}} ={\frac {\sigma_{x,y}}{\sigma^2_{x}}}
\end{align*}\]</span>
The estimated regression coefficient <span class="math inline">\(\hat{\beta_{1}}\)</span> equals the covariance between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> divided by the variance of <span class="math inline">\(x\)</span>.</p>
<p>The formulas presented above may not be very intuitive at first glance. The online version of the book <span class="citation">Hanck, Arnold, Gerber, &amp; Schmelzer (<a href="references.html#ref-Hanck2020Introduction" role="doc-biblioref">2020</a>)</span> offers a nice interactive application in the box <a href="https://www.econometrics-with-r.org/4-2-estimating-the-coefficients-of-the-linear-regression-model.html">The OLS Estimator, Predicted Values, and Residuals</a> that helps to understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. Once two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as you add further observations to the left panel. A double-click resets the application, i.e., all data are removed.</p>
</div>
<div id="the-least-squares-assumptions" class="section level4 hasAnchor" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> The least squares assumptions<a href="statistics.html#the-least-squares-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>OLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which need to be satisfied in order to ensure that the estimates are normally distributed in large samples.</p>
<p>The Least Squares Assumptions should fulfill the following assumptions:
<span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \text{, } i = 1,\dots,n\]</span>
- The error term <span class="math inline">\(\epsilon_i\)</span> has conditional mean zero given <span class="math inline">\(X_i: E(u_i|X_i)=0\)</span>.
- <span class="math inline">\((X_i,Y_i), i=1,\dots,n\)</span> are independent and identically distributed (i.i.d.) draws from their joint distribution.
- Large outliers are unlikely: <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> have nonzero finite fourth moments. That means, assumption 3 requires that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a finite kurtosis.</p>
</div>
<div id="measures-of-fit" class="section level4 hasAnchor" number="3.2.3.3">
<h4><span class="header-section-number">3.2.3.3</span> Measures of fit<a href="statistics.html#measures-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After fitting a linear regression model, a natural question is how well the model describes the data. Visually, this amounts to assessing whether the observations are tightly clustered around the regression line. Both the coefficient of determination and the standard error of the regression measure how well the OLS Regression line fits the data.</p>
<p><span class="math inline">\(R^2\)</span> is the fraction of the sample variance of <span class="math inline">\(Y_i\)</span> that is explained by <span class="math inline">\(X_i\)</span>. Mathematically, the <span class="math inline">\(R^2\)</span> can be written as the ratio of the explained sum of squares to the total sum of squares. The explained sum of squares (ESS) is the sum of squared deviations of the predicted values <span class="math inline">\(\hat{Y_i}\)</span>, from the average of the <span class="math inline">\(Y_i\)</span>. The total sum of squares (TSS) is the sum of squared deviations of the <span class="math inline">\(Y_i\)</span> from their average. Thus we have
<span class="math display">\[\begin{align}
ESS &amp; =  \sum_{i = 1}^n \left( \hat{Y_i} - \overline{Y} \right)^2,   \\
TSS &amp; =  \sum_{i = 1}^n \left( Y_i - \overline{Y} \right)^2,   \\
R^2 &amp; = \frac{ESS}{TSS}.
\end{align}\]</span>
Since <span class="math inline">\(TSS = ESS + SSR\)</span> we can also write
<span class="math display">\[R^2 = 1- \frac{\textcolor{blue}{SSR}}{\textcolor{red}{TSS}}\]</span>
with <span class="math inline">\(SSR= \sum_{i = 1}^n \epsilon^2\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:fitR"></span>
<img src="fig/fitR.png" style="width:75.0%" alt="" />
<p class="caption">Figure 3.7:  Total sum of squares and sum of squared residuals</p>
</div>
<p><span class="math inline">\(R^2\)</span> lies between 0 and 1. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies <span class="math inline">\(R2=1\)</span> since then we have <span class="math inline">\(SSR=0\)</span>. On the contrary, if our estimated regression line does not explain any variation in the <span class="math inline">\(Y_i\)</span>, we have <span class="math inline">\(ESS=0\)</span> and consequently <span class="math inline">\(R^2=0\)</span>. Figure <a href="statistics.html#fig:fitR">3.7</a> show the relationship of TTS and SSR.</p>
</div>
</div>
<div id="multiple-linear-regression" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Multiple linear regression<a href="statistics.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having understood the simple linear regression model, it is important to broaden our scope beyond the relationship between just two variables: the dependent variable and a single regressor. Our goal is to causally interpret the measured association of two variables, which requires certain conditions as explained in section <a href="identification.html#identification">2</a>.</p>
<p>To illustrate this concept, let’s revisit the phenomenon known as Simpson’s paradox. Simpson’s paradox occurs when the overall association between two categorical variables differs from the association observed when we consider the influence of one or more other variables, known as controlling variables. This paradox holds three key reasons for its significance.</p>
<p>Firstly, it challenges the assumption that statistical relationships are fixed and unchanging. In reality, the relationship between two variables can vary, either increasing, decreasing, or even changing direction depending on the set of variables being controlled.</p>
<p>Secondly, Simpson’s paradox is not an isolated phenomenon of interest solely to statisticians. It belongs to a larger class of association paradoxes, indicating that similar situations can arise in various contexts.</p>
<p>Lastly, Simpson’s paradox serves as a reminder to researchers about the potential pitfalls of making causal inferences, especially in nonexperimental studies. Uncontrolled or unobserved variables may exist that can eliminate or reverse the observed association between two variables.</p>
<p>Thus, it is important to consider confounding variables to ensure valid and reliable causal interpretations in research, particularly in nonexperimental settings.</p>
<div class="figure"><span style="display:block;" id="fig:foo13"></span>
<img src="fig/foo-13.png" style="width:75.0%" alt="" />
<p class="caption">Figure 3.8:  Simpsons paradox and the power of controlling variables (1)</p>
</div>
<div class="figure"><span style="display:block;" id="fig:foo32"></span>
<img src="fig/foo-32.png" style="width:75.0%" alt="" />
<p class="caption">Figure 3.9:  Simpsons paradox and the power of controlling variables (1)</p>
</div>
<p>The multiple regression model can be expressed as
<span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \dots + \beta_k X_{ki} + u_i, \ i=1,\dots,n.
\]</span></p>
<p>How can we estimate the coefficients of the multiple regression model? As in the simple model, we seek to minimize the sum of squared mistakes by choosing estimated
coefficients <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_k\)</span> such that
<span class="math display">\[\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - b_2 X_{2i} - \dots -  b_k X_{ki})^2 \]</span></p>
<p>This demands matrix notation. This goes beyond the scope of this introduction.</p>
<div id="gauss-markov-and-the-best-linear-unbiased-estimator" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Gauss-Markov and the best linear unbiased estimator<a href="statistics.html#gauss-markov-and-the-best-linear-unbiased-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Gauss-Markov assumptions, also known as the classical linear regression assumptions, are a set of assumptions that underlie the ordinary least squares (OLS) method for estimating the parameters in a linear regression model. These assumptions ensure that the OLS estimators are unbiased, efficient, and have desirable statistical properties.</p>
<p>The Gauss-Markov assumptions are as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Linearity: The relationship between the dependent variable and the independent variables is linear in the population model. This means that the true relationship between the variables can be represented by a linear equation.</p></li>
<li><p>Independence: The errors (residuals) in the regression model are independent of each other. This assumption ensures that the errors for one observation do not depend on or influence the errors for other observations.</p></li>
<li><p>Strict exogeneity: The errors have a mean of zero conditional on all the independent variables. In other words, the expected value of the errors is not systematically related to any of the independent variables.</p></li>
<li><p>No perfect multicollinearity: The independent variables are not perfectly correlated with each other. Perfect multicollinearity occurs when one independent variable is a perfect linear combination of other independent variables, leading to problems in estimating the regression coefficients.</p></li>
<li><p>Homoscedasticity: The errors have constant variance (homoscedasticity) across all levels of the independent variables. This assumption implies that the spread or dispersion of the errors is the same for all values of the independent variables.</p></li>
<li><p>No endogeneity: The errors are not correlated with any of the independent variables. Endogeneity occurs when there is a correlation between the errors and one or more of the independent variables, leading to biased and inefficient estimators.</p></li>
<li><p>No autocorrelation: The errors are not correlated with each other, meaning that there is no systematic pattern or relationship between the errors for different observations.</p></li>
</ol>
<p>These assumptions collectively ensure that the OLS estimators are unbiased, efficient, and have minimum variance among all linear unbiased estimators. Violations of these assumptions can lead to biased and inefficient estimators, invalid hypothesis tests, and unreliable predictions. Therefore, it is important to check these assumptions when using the OLS method and consider alternative estimation techniques if the assumptions are violated.</p>
</div>
<div id="confounding-and-control-variables" class="section level4 hasAnchor" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Confounding and control variables<a href="statistics.html#confounding-and-control-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A confounding variable is a factor that was not accounted for or controlled in a study but has the potential to influence the results. In other words, the true effects of the treatment or intervention can be obscured or muddled by the presence of this variable.</p>
<p>For instance, let’s consider a scenario where two groups of individuals are observed: one group took vitamin C daily, while the other group did not. Over the course of a year, the number of colds experienced by each group is recorded. It might be observed that the group taking vitamin C had fewer colds compared to the group that did not. However, it would be incorrect to conclude that vitamin C directly reduces the occurrence of colds. Since this study is observational and not a true experiment, numerous confounding variables are at play. One potential confounding variable could be the individuals’ level of health consciousness. Those who take vitamins regularly might also engage in other health-conscious behaviors, such as frequent handwashing, which could independently contribute to a lower risk of catching colds.</p>
<p>To address confounding variables, researchers employ control measures. The idea is to create conditions where confounding variables are minimized or eliminated. In the aforementioned example, researchers could pair individuals who have similar levels of health consciousness and randomly assign one person from each pair to take vitamin C daily (while the other person receives a placebo). Any differences observed in the number of colds between the groups would be more likely attributable to the vitamin C, compared to the original observational study. Well-designed experiments are crucial as they actively control for potential confounding variables.</p>
<p>Consider another scenario where a researcher claims that eating seaweed prolongs life. However, upon reading interviews with the study subjects, it becomes apparent that they were all over 100 years old, followed a very healthy diet, slept an average of 8 hours per day, drank ample water, and engaged in regular exercise. In this case, it is not possible to determine whether longevity was specifically caused by seaweed consumption due to the presence of numerous confounding variables. The healthy diet, sufficient sleep, hydration, and exercise could all independently contribute to longer life. These variables act as confounding factors.</p>
<p>A common error in research studies is to fail to control for confounding variables, leaving the results open to scrutiny. The best way to head off confounding variables is to do a well-designed experiment in a controlled setting. Observational studies are great for surveys and polls, but not for showing cause-and-effect relationships, because they don’t control for confounding variables.</p>
<p><strong>Control variables</strong> are usually variables that you are not particularly interested in, but that are related to the dependent variable. You want to remove their effects from the equation. A control variable enters a regression in the same way as an independent variable – the method is the same.</p>
<p>Nick Huntington-Klein offers <a href="https://www.nickchk.com/causalgraphs.html">Causal Inference Animated Plots</a> on his homepage. Read this page and consider the animated graphs.</p>
</div>
<div id="omitted-variable-bias-and-ceteris-paribus" class="section level4 hasAnchor" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Omitted variable bias and ceteris paribus<a href="statistics.html#omitted-variable-bias-and-ceteris-paribus" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>From the Gauss-Markov theorem we know that if the OLS assumptions are fullfiled, the OLS estimator is (in the sense of smallest variance) the best linear conditionally unbiased estimator (BLUE).
However, OLS estimates can suffer from omitted variable bias when any regressor, X, is correlated with any omitted variable that matters for variable Y.</p>
<p>For omitted variable bias to occur, two conditions must be fulfilled:</p>
<ol style="list-style-type: decimal">
<li>X is correlated with the omitted variable.</li>
<li>The omitted variable is a determinant of the dependent variable Y.</li>
</ol>
<p>In regression analysis, “ceteris paribus” is a Latin phrase that translates to “all other things being equal” or “holding everything else constant.” It is a concept used to examine the relationship between two variables while assuming that all other factors or variables remain unchanged.</p>
<p>When we say <em>ceteris paribus</em> in the context of regression analysis, we are isolating the effect of a specific independent variable on the dependent variable while assuming that the values of the other independent variables remain constant. By holding other variables constant, we can focus on understanding the direct relationship between the variables of interest.</p>
<p>For example, consider a regression analysis that examines the relationship between income (dependent variable) and education level (independent variable) while controlling for age, gender, and work experience. By stating <em>ceteris paribus</em>, we are assuming that age, gender, and work experience remain constant, and we are solely interested in understanding the impact of education level on income.</p>
<div class="exercise">
<p><span id="exr:lookatoutput" class="exercise"><strong>Exercise 3.7  </strong></span>Look at the Output</p>
<div class="figure"><span style="display:block;" id="fig:regstata2"></span>
<img src="fig/reg_stata2.png" style="width:85.0%" alt="" />
<p class="caption">Figure 3.10:  Regression output</p>
</div>
<p>Above you see an excerpt of a regression output taken from a statistical program. Some t-values and p-values are missing.</p>
<ol style="list-style-type: lower-alpha">
<li>Calculate the t-value of the coefficient <code>mpg</code>. Is the coefficient at a level of <span class="math inline">\(\alpha=0.05\)</span> statistically significant?<br />
</li>
<li>Is the coefficient foreign at a level of <span class="math inline">\(\alpha=0.05\)</span> statistically significant?<br />
</li>
<li>Is the constant at a level of <span class="math inline">\(\alpha=0.05\)</span> statistically significant?</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:exrstataoutput" class="exercise"><strong>Exercise 3.8  </strong></span>Look at Stata Output</p>
<p>Below you find two regression outputs from Stata. Try to interpret the p-values and the confidence intervals. How are the t-values calculated. Can you use the <em>magic number</em> 1.96 to say if a corresponding estimated coefficient is statistically significant, or not? Which estimated model is <em>better</em>?</p>
<div class="figure"><span style="display:block;" id="fig:regstata3"></span>
<img src="fig/reg_stata_class2.png" style="width:85.0%" alt="" />
<p class="caption">Figure 3.11:  Stata regression output (1)</p>
</div>
<div class="figure"><span style="display:block;" id="fig:regstata3b"></span>
<img src="fig/reg_stata_class.png" style="width:85.0%" alt="" />
<p class="caption">Figure 3.12:  Stata regression output (1)</p>
</div>
</div>
<div class="exercise">
<p><span id="exr:explainweight" class="exercise"><strong>Exercise 3.9  </strong></span>Explain the weight</p>
<p>Download the file <a href="https://github.com/hubchev/courses/blob/main/pdfs/exe_calories.pdf">exe_calories.pdf</a> from GitHub and answer the questions therein.</p>
<p>Solutions are provided <a href="https://raw.githubusercontent.com/hubchev/courses/main/scr/regress_lecture.R">here</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>Picture is taken from www.theparisreview.org/blog/2017/07/21/great-moments-literacy-hite-report<a href="statistics.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>Picture is taken from the video <a href="https://youtu.be/sHRBg6BhKjI" class="uri">https://youtu.be/sHRBg6BhKjI</a><a href="statistics.html#fnref20" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="identification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hands-on-experiments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
