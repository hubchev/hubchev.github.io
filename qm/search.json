[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methods",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-notes",
    "href": "index.html#about-the-notes",
    "title": "Quantitative Methods",
    "section": "About the notes",
    "text": "About the notes\n\n\n\n\n\n\nA PDF version of these notes is available here.\n\n\n\nPlease note that while the PDF contains the same content, it has not been optimized for PDF format. Therefore, some parts may not appear as intended.\n\n\n\n\n\n\n\n\nNotable changes\n\n\n\nMarch 26: I changed the structure moving some subsections into separate sections.\nMarch 27: I added 2.6 Feynman on scientific method and 2.7 How social scientists do research, reading lists for each section, and I moved most solutions closer to the exercises.\nApril 2: I added 6  Epistemic and moved some subsections of 3  Identification to it. I added a question to Exercise 6.2. \n\n\n\nThese notes aims to support my lecture at the HS Fresenius but are incomplete and no substitute for taking actively part in class.\nA pdf version of these notes is available here\nI appreciate you reading it, and I appreciate any comments.\nThis is work in progress so please check for updates regularly.\nDo not distribute without permission.\nFor making an appointment, you can use the online tool that you find on my private homepage: https://hubchev.github.io/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Quantitative Methods",
    "section": "About the author",
    "text": "About the author\n\n\n\nFigure 1: Prof. Dr. Stephan Huber\n\n\n\n\n\n\nI am a Professor of International Economics and Data Science at HS Fresenius, holding a Diploma in Economics from the University of Regensburg and a Doctoral Degree (summa cum laude) from the University of Trier. I completed postgraduate studies at the Interdisciplinary Graduate Center of Excellence at the Institute for Labor Law and Industrial Relations in the European Union (IAAEU) in Trier. Prior to my current position, I worked as a research assistant to Prof. Dr. Dr. h.c. Joachim Möller at the University of Regensburg, a post-doc at the Leibniz Institute for East and Southeast European Studies (IOS) in Regensburg, and a freelancer at Charles University in Prague.\nThroughout my career, I have also worked as a lecturer at various institutions, including the TU Munich, the University of Regensburg, Saarland University, and the Universities of Applied Sciences in Frankfurt and Augsburg. Additionally, I have had the opportunity to teach abroad for the University of Cordoba in Spain, the University of Perugia in Italy, and the Petra Christian University in Surabaya, Indonesia. My published work can be found in international journals such as the Canadian Journal of Economics and the Stata Journal. For more information on my work, please visit my private homepage at hubchev.github.io.\n\n\n\n\n\n\nContact:\n\n\n\n\n\nProf. Dr. Stephan Huber\nHochschule Fresenius für Wirtschaft & Medien GmbH\nIm MediaPark 4c\n50670 Cologne\nOffice: 4e OG-3\nTelefon: +49 221 973199-523\nMail: stephan.huber@hs-fresenius.de\nPrivate homepage: www.hubchev.github.io\nGithub: https://github.com/hubchev",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-course-quantitative-methods-for-business",
    "href": "index.html#about-the-course-quantitative-methods-for-business",
    "title": "Quantitative Methods",
    "section": "About the course Quantitative Methods for Business",
    "text": "About the course Quantitative Methods for Business\n\nWorkload of the course Quantitative & Qualitative Methods for Business\n125 h = 56 h (in-class) + 21 h (guided private study hours) - 48 h (private self-study).\n\n\nWorkload of Quantitative Methods\n62.5 h = 28 h (in-class) + 10.5 h (guided private study hours) - 24 h (private self-study).\n\n\nAssessment\nStudents complete this module with a written exam of 120 minutes where 50% of the points stem from M-IBS 8.1 Quantitative Methods and 50% from M-IBS 8.2 Qualitative Methods. A passing grade in this module is achieved when the overall grade is greater than or equal to 4.0.\n\n\nLearning outcomes:\nAfter successful completion of the module, students are able to:\n\nassess and discuss coherent research paradigms, based on quantitative, qualitative, and mixed-methods research approaches,\nexplain a broad set of quantitative and qualitative methods to collect, gather, illustrate, analyze, and interpret data,\ndistinguish and discuss empirical strategies to identify causal mechanisms, causes, and effects.\n\n\n\nLiterature:\nIn the lecture I frequently refer to Cunningham (2021), Huntington-Klein (2023), Illowsky & Dean (2018), Békés & Kézdi (2021), and Paldam (2021).\n\nCunningham, S. (2021). Causal inference: The mixtape. Accessed January 30, 2023; Yale University Press. https://mixtape.scunning.com/\n\nHuntington-Klein, N. (2023). The effect: An introduction to research design and causality. CRC Press. https://theeffectbook.net\n\nIllowsky, B., & Dean, S. (2018). Introductory statistics. Openstax. https://openstax.org/details/books/introductory-statistics\n\nBékés, G., & Kézdi, G. (2021). Data analysis for business, economics, and policy. Cambridge University Press.\n\nPaldam, M. (2021). Methods used in economic research: An empirical study of trends and levels. Economics, 15(1), 28–42.\n\nBergstrom, C. T., & West, J. D. (2021). Calling bullshit: The art of skepticism in a data-driven world. Penguin Books.\n\nChivers, T., & Chivers, D. (2021). How to read numbers: A guide to statistics in the news (and knowing when to trust them). Weidenfeld & Nicolson.\n\nHarford, T. (2020). How to make the world add up: Ten rules for thinking differently about numbers. The Bridge Street Press.\n\nHuff, D. (1954). How to lie with statistics. WW Norton & company.\n\nJones, B. (2020). Avoiding data pitfalls: How to steer clear of common blunders when working with data and presenting analysis and visualizations. John Wiley & Sons.\n\nSpiegelhalter, D. (2019). The art of statistics: Learning from data. Penguin UK.\nAbove that, there are tons of books around that are both insightful and entertaining and support the lecture. In Figure 2, I present a short list of books I recommend: Bergstrom & West (2021), Chivers & Chivers (2021), Cunningham (2021), Harford (2020), Huff (1954), Huntington-Klein (2023), Jones (2020), and Spiegelhalter (2019).\n\n\n\nFigure 2: Books for data literacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContent:\n\nResearch design\n\nHow to measure socio-economical reality\nHow to identify causes of effects\nHow to identify effects of causes\nThe selection problem and ways to solve it (matching, natural experiments, laboratory experiments)\n\nStatistical toolbox\n\nTypes of data (cross-section, panel, time-series, georeferenced)\nTypes of variables (continuous, count, ordinal, categorical, qualitative)\nData sampling methods\nDescriptive methods (data visualization, statistical moments, correlation)\nMethods of statistical inference (distribution, statistical tests)\nMathematical and statistical software packages (R, Stata, SPSS, Excel, WolframAlpha, etc.)\n\nMethods\n\nData mining (graphical visualizations, cluster analysis, factor analysis)\nRegression analysis (matching, instrument variables, difference in difference, fixed effects, regression discontinuity)\nOther methods (time series analysis, spatial analysis, simulations, qualitative comparative analysis, etc.)\n\n\n\n\nAbout how to learn (and prepare for the exam)\n\n\n\nFigure 3: Richard P. Feynman’s Los Alamos ID badge1\n\n1 Source: https://en.wikipedia.org/wiki/File:Richard_Feynman_Los_Alamos_ID_badge.jpg\n\n\n\n\nRichard P. Feynman (1918-1988) was a team leader at the Manhatten Project (see Figure 2.4) and won the Nobel Prize in 1965 in physics. He once said\n\n“I don’t know what’s the matter with people: they don’t learn by understanding; they learn by some other way – by rote, or something. Their knowledge is so fragile!”* (Feynman, 1985)\n\nFeynman, R. P. (1985). Surely you’re joking, Mr. Feynman! Adventures of a curious character. W.W. Norton.\n\nI agree with Feynman: The key to learning is understanding. However, I believe that there is no understanding without practice, that is, solving problems and exercises by yourself with a pencil and a blank sheet of paper without knowing the solution in advance.\n\nAttend lectures and and take the opportunity to ask questions and actively participate in class.\nStudy the lecture notes and work on the exercises.\nReview the material regularly each week. Learning in small increments is more effective than last-minute cramming.\nTest yourself with past exams that you find in Appendix A — Past exams.\nIf you have the opportunity to form a study group, make use of it. It is great to help each other, and it is very motivating to see that everyone has problems sometimes.\nIf you have difficulties with some exercises and the solutions shown do not solve your problem, ask a classmate or contact me.\n\nI am convinced that reading the lecture notes, preparing for class, taking actively part in class, and trying to solve the exercises without going straight to the solutions is the best method for students to\n\nmaximize leisure time and minimize the time needed to prepare for the exam, respectively,\ngetting long-term benefits out of the course,\nimprove grades, and\nhave more fun during lecture hours.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#personal-note",
    "href": "index.html#personal-note",
    "title": "Quantitative Methods",
    "section": "Personal note",
    "text": "Personal note\nDear students,\nIf the title of this course “Quantitative & Qualitative Methods for Business” seems uninteresting to you, I assure you that it is actually quite exciting because it focuses on how we can use information to understand how the world and business works and how to interpret facts. The course will enhance your data literacy, help you think critically, and improve your personal decision-making skills.\nOne way we can do this is by understanding the differences between quantitative and qualitative data and how they can be used to inform our choices.\nQuantitative data is information that can be measured, such as numbers and statistics, while qualitative data is information that cannot be measured and is often expressed in words or other non-numerical forms.\nBoth forms of information are crucial for making good decisions. Without sufficient information, it can be difficult to evaluate the options and potential outcomes of a decision, leading to poor or uninformed choices. In general, the more information a decision-maker has and the faster and better the information can be used, the better they will be to make a sound decision.\nThe methods we discuss in this course will help you systematically gather information and make sense of it.\nEnjoy the course!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "doing_research.html",
    "href": "doing_research.html",
    "title": "1  Doing research",
    "section": "",
    "text": "1.1 What is research\nResearch often involves exploring unknown territory and seeking out new information through methods such as attending conferences, conducting interviews and experiments, and reading related research. This process can lead to the discovery of valuable techniques or insights that address important issues in society or science. Zora Neale Hurston (2010) (see Figure 1.1) paraphrased it beautifully:\nEffective research is based on the principles of honesty, transparency and much more. A pithy yet profound quote from Scott Cunningham sums up this idea:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#what-is-research",
    "href": "doing_research.html#what-is-research",
    "title": "1  Doing research",
    "section": "",
    "text": "“Research is formalized curiosity. It is poking and prying with a purpose.” (Hurston, 2010)\n\nHurston, Z. N. (2010). Dust tracks on a road. HarperCollins.\n\n\n\n\nFigure 1.1: Zora Neale Hurston, 1891-19601\n\n1 Source: Photography is taken from Library of Congress: Prints & Photographs Division, Carl van Vechten Collection, Reproduction Number LC-USZ62-54231, see: https://www.loc.gov/pictures/item/2004663047/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“True scientists do not collect evidence in order to prove what they want to be true or what others want to believe. That is a form of deception and manipulation called propaganda, and propaganda is not science. Rather, scientific methodologies are devices for forming a particular kind of belief. Scientific methodologies allow us to accept unexpected, and sometimes undesirable, answers.” (Cunningham, 2021, p. 10)\n\nCunningham, S. (2021). Causal inference: The mixtape. Accessed January 30, 2023; Yale University Press. https://mixtape.scunning.com/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#everybody-can-do-research",
    "href": "doing_research.html#everybody-can-do-research",
    "title": "1  Doing research",
    "section": "1.2 Everybody can do research",
    "text": "1.2 Everybody can do research\n\n\n\nFigure 1.2: Children as little researcher2\n\n2 Source: Image by macrovector on Freepik, see: https://www.freepik.com/free-vector/kindergarten-set-isolated-icons-with-toys-characters-kids-practicing-with-teacher-playing-games-vector-illustration_26760074.htm\n\n\n\n\n\n\n\n\n\n\n\n\nBefore I go into how empirical research can and should be conducted, I would like to assert that each of us is a researcher in some sense and that you don’t need a degree or a higher education to be a (good) researcher. Each of my four children (ages 2, 5, 6, and 8 (at the time of writing this)), for example, explores the world and learns something new every day. Even though none of my children is yet able to verify the novelty of their acquired knowledge and write it down in scientific form, I will claim that mine, like practically all children, are already little scientists. Why? Well, they explore unknown territory and search for information to discover new techniques that will make their lives pleasant, see Figure 1.2. Of course, they don’t attend conferences or read journals to do this. They have never heard terms such as ontology, epistemology, axiology, or quantitative and qualitative methods. They are using methods that they have mastered for their age. They interview me, my wife and all other people around and they conduct experiments. For example, all my children liked to throw plates, cutlery, cups and alike from the table when they were about one year old. At first the throwing was just an accident, but they quickly found out that each throw was followed by a sound when the object touched the stone floor. My first son, in particular, took great delight in making these sounds. He threw everything within reach to the ground and giggled with joy at the clink he made when the object hit the ground. Perhaps he was also enjoying the attention he was getting from us parents through these actions. In any case, the behavior annoyed us. Wiping food scraps off the floor is not a nice thing to do. Unfortunately, at that time my son did not accept any argument to refrain from throwing. Neither a stern look nor a definite “no” helped to stop this behavior. Too great was the joy at the relationship he had figured out, which was, “I throw something off the table and it always clangs beautifully loud.” So I started to do some research to figure out what I could do to stop him. The short answer I found can be summed up pretty well as “nothing”. There is practically no good method to change the behavior without possibly negatively influencing his early childhood development. The reason is he did some research and we should not suppress that. Besides nature and material research he did social research: He found out that things fall to the ground (gravity), that things break and make different sounds (material research), and that other people notice him when he throws things (social research).\nOnce, when we were eating at a friend’s house, my son (once again) threw everything off the table one after the other in unobserved moments. This time, however, it made no noise. The carpet under the table muffled everything. My son was irritated and at some point became really angry. Why? Well, his surely believed reality and his law “I throw something from the table and then it always clangs beautifully loud.” was falsified. Soon he understood that his law only had to be adapted a little. It was then: “I throw something from the table and it clangs then beautifully loudly if a stone floor is under me.” He repeated his experiments for a few more weeks, to check its validity. In the meantime he does other experiments trying to contribute to his own knowledge.\nIn general, the purpose of research is to find new knowledge or discover new ways to use existing knowledge in a creative way so as to generate new concepts, methodologies, inventions and understandings that -now or later- may be of some value for the human mankind. In simple terms, we aim to find something out. We aim to find a new law, a new relationship, a new insight. Or, we aim to challenge and revise existing insights on how the world works. You don’t need a degree to do that. All you need is interest, open-mindedness, and a willingness to revise your ideas about how the world works. The latter is perhaps the most important skill you need to be a good researcher. Otherwise, one is a narrow-minded, and bigoted person who is too proud to follow up an insight with a change of mind.\nI myself have a quick and happy tendency to change my views because it is a statement of a fresh understanding. Here are two more quotes from Mr. Keynes (see Figure 1.3) and Mr. Adenauer (see Figure 1.4), two historically slightly more significant people than me that are along the same lines and should convince you that changing your mind is not a sign of weakness, but of strength. Especially in science, the willingness to change one’s mind is essential.\n\n\n\nFigure 1.3: John Maynard Keynes (1883-1946)3\n\n3 Source: Photography is public domain and stems from https://de.wikipedia.org/wiki/John_Maynard_Keynes#/media/Datei:Keynes_1933.jpg\n\n\n\n\n\n\n\n\n\n\n\n\n\n“When the facts change, I change my mind. What do you do, sir?”4\n4 This quote is often attributed to Keynes, but there is no clear evidence for it, see: https://quoteinvestigator.com/2011/07/22/keynes-change-mind/\n\n\n\nFigure 1.4: Konrad Adenauer (1876-1967)5\n\n5 Source: This photography from 1952 is public domain and stems from the Bundesarchiv, B 145 Bild-F078072-0004, Katherine Young, CC BY-SA 3.0 DE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n“What do I care about the rubbish I said yesterday? No one can stop me from getting smarter every day.” (“Was interessiert mich mein Geschwätz von gestern? … es kann mich doch niemand daran hindern, jeden Tag klüger zu werden.”)6\n6 Freely quoted (and translated) from Weymar (1955, p. 521)\nWeymar, P. (1955). Konrad Adenauer: Die autorisierte Biographie. Kindler.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#its-difficult-to-do-good-research",
    "href": "doing_research.html#its-difficult-to-do-good-research",
    "title": "1  Doing research",
    "section": "1.3 It’s difficult to do good research",
    "text": "1.3 It’s difficult to do good research\nSimply trying something and seeing what happens, like my children do, is a research method that relies on luck and chance. Before I go into more grown-up ways of doing research, I want to emphasize that the role of chance and serendipity in research is often downplayed and not acknowledged. The most well-known example of such research is the discovery of penicillin by Alexander Fleming (see Figure 1.5). In 1928, Fleming was studying the properties of staphylococcus bacteria when he noticed that a mold called Penicillium notatum had contaminated one of his bacterial cultures. He noticed that the mold seemed to be inhibiting the growth of the bacteria, and he began to investigate this further. Eventually, he was able to isolate and purify the active ingredient in the mold, which he named penicillin, and he discovered that it had powerful antibiotic properties. This discovery revolutionized the field of medicine and has saved countless lives.\n\n\n\nFigure 1.5: Sir Alexander Fleming (1881-1955)7\n\n7 Source: Photography is public domain and stems from https://en.wikipedia.org/wiki/File:Synthetic_Production_of_Penicillin_TR1468.jpg\n\n\n\n\n\n\n\n\n\n\n\n\nDoing something on purpose and observing how things respond to the action can be considered a research strategy. Acting like a child or just waiting for something to happen by chance can also be considered a research strategy, and of course this can contribute greatly to knowledge. However, it are a naïve and poorly targeted strategies to conduct research. There are more grown-up research methods that are targeting more precisely the gaps in our knowledge and speed up innovation in the field where progress is desperately needed.\nThe processes of research and observation of phenomena should aim to maximize the probability of discovering new and intriguing findings. They should also ensure a high degree of confidence in the validity of our findings and reduce the likelihood that they will be disproved shortly afterwards. Transparency, scientific collaboration and open competition are crucial for efficient progress in science.\nTake, for example, the scenario of a fatal disease. A naïve approach to finding a cure might be to try different things and observe who falls ill and who dies or is cured, hoping to stumble upon a cure through serendipitous observation. However, this method is unlikely to be effective or practical. A more promising strategy would be to systematically study the disease and openly communicate research plans before they are implemented. This avoids unnecessary efforts and costs and accelerates the achievement of results.\nFor example, a laboratory should first seek to isolate the causative virus or bacterium in order to be able to grow and study it outside the danger to humans. Once this is done, we need a precise plan on how we can use all the available knowledge to cure the disease, protect people from infection, or help them survive the disease. In short, we need a strategic way to conduct research, i.e., a research strategy or design.\nA research strategy is a general plan for conducting a study and a research design is a detailed plan for conducting the study. These words are frequently used interchangeably. A research strategy depends on many things including the question, the resources available, the current state of knowledge, the ambitions, whether quantitative or quantitative data are used, and what is considered to be the criteria of good research.\nBefore discussing some research strategies that can provide reasonable answers to certain types of questions, we should clarify how to ask a research question and what qualifies a research question.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#asking-questions-lika-a-good-researcher",
    "href": "doing_research.html#asking-questions-lika-a-good-researcher",
    "title": "1  Doing research",
    "section": "1.4 Asking questions lika a good researcher",
    "text": "1.4 Asking questions lika a good researcher\nUnfortunately, there is no one research strategy that is appropriate for all questions and, what is worse, there is still controversy about what constitutes good research and how to properly ask a research question. In particular, this controversy takes place between researchers who use quantitative data and statistical methods and researchers who use qualitative data and methods.\nQuantitative researchers are interested in both the causes of effects and the effects of causes. Experimental setups can allow to validate causes of effects and to measure the effects of causes. With observational data, however, it is often difficult to investigate the causes of effects. Thus, often quantitative research is more interested to quantify the effects of causes. Qualitative researchers also try to determine the causes of effects . However, their data analysis does rely less on statistical inference. A qualitative data set not necessarily requires (large) random samples or structured data (all the data that you can structure in a spreadsheet) in general, but allows to analyze selective and unstructured data (that is data in form of audio, video, text, images and alike). Qualitative research methods allow to classify these data into patterns or to interpret them in a meaningful way in order to arrive at results. Qualitative researchers are more concerned with the why and how of decision making and examine people’s behavior, beliefs, perceptions of events, experiences, attitudes, interactions, and more in great depth.\nIn empirical research, inductive and deductive are two different approaches to reasoning. Inductive reasoning is a process of collecting data from various sources, such as interviews, surveys or observations, and then use this data to identify patterns, themes, or relationships that can form the basis of a new hypothesis or theory. The goal of these exploratory studies, is to generate new ideas or insights about a topic, rather than testing a specific hypothesis. Deductive reasoning is a process in which the researcher starts with a general theory or hypothesis with the goal to test a specific hypothesis or theory. In most cases, a combination of both inductive and deductive reasoning may be used to formulate the research question and to design the empirical identification strategy.\nIn what follows, however, we focus on the criteria for good research that are more commonly used in evaluating the quality of quantitative research.\n\n\n\n\n\n\n\nExercise 1.1 The Effect ch.1+2\nRead chapter 1 and 2 of Huntington-Klein (2023) and answer the questions below. The book (see Figure 1.6) is freely available at https://theeffectbook.net and here is the link to chapter 1: https://theeffectbook.net/ch-TheDesignofResearch.html\n\n\n\nFigure 1.6: The Effect: An Introduction to Research Design and Causality\n\n\n\n\n\n\n\n\n\n\n\nSource: Huntington-Klein (2023) \n\n\n\n\nWhat is the main focus of the book the author is writing about?\n\nPhilosophy of science\nQualitative research methods\nEmpirical research and quantitative methods to identify and measure causal effects\nStatistics\n\nWhat is the main challenge faced by quantitative empirical research, according to the author?\n\nDifficulty in obtaining accurate measurements\nDifficulty in interpreting measurements\nDifficulty in obtaining data that allows to answers the research question\nDifficulty in designing a research that gets a lot of attention\n\nWhat is the author`s main point about research questions?\n\nThey should be well-defined, answerable, and understandable\nThey should be simple and easy to answer\nThey should be related to the world of traffic\nThey should be related to the field of quantum mechanics\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc), 2. c), 3. a)\n\n\n\n\n\n\n\n\n\nHuntington-Klein, N. (2023). The effect: An introduction to research design and causality. CRC Press. https://theeffectbook.net",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#sec-featuresofresearch",
    "href": "doing_research.html#sec-featuresofresearch",
    "title": "1  Doing research",
    "section": "1.5 Features of good research",
    "text": "1.5 Features of good research\nIn order to make you a competent researcher who does not have to wait for a lucky chance but has a clear strategy, let’s discuss the criteria of a good research. Before I do that, however, I must make a disclaimer: there is a lack of consensus on what constitutes high-quality research in social sciences. In my experience, the practical benefits of such a tedious discussion are quite small. All I like to put forward is that I believe that all social science disciplines such as sociology, anthropology, psychology, economics, business administration, and education using quantitative methods agree that good research should be replicable, reproducible, transparent, reliable, and valid.\n\n1.5.1 Reliability and validity\nA research design is a plan to examine information in a systematic and controlled way so that the results of the research are valid and reliable.\nValidity refers to the accuracy and truthfulness of research findings. In other words, if a study is valid, it should measure what it is intended to measure and produce results that are representative of the population being studied. Validity is important because it helps to ensure that the conclusions drawn from a study are supported by the data and are not based on flawed or biased methods.\nReliability refers to the consistency and stability of research findings. In other words, if a study is reliable, it should produce similar results if it is repeated using the same methods and conditions. Reliability is important because it helps to ensure that the results of a study are not simply due to chance or random error.\nBoth reliability and validity are important considerations in research, and researchers strive to maximize both in their studies. However, it is important to note that it is often difficult to achieve both at the same time, and trade-offs may need to be made between the two.\n\n\n\n\n\n\nTip 1.1\n\n\n\nA good research design should aim to minimize bias and maximize the reliability and validity of the research. It should also be appropriate for the research question being asked and the resources available to the researcher.\n\n\n\nHigh reliability and low validity\nAn example of a study that has high reliability but low validity is a study that measures the weight of a group of people using a digital scale. If the scale is consistently accurate and produces the same weight measurements each time it is used, then the study has high reliability. However, if the scale is not calibrated correctly and produces inaccurate weight measurements, then the study has low validity.\nAnother example of a research design that has high reliability but low validity is a study that uses a highly reliable measurement tool, such as a standardized test, to measure a concept that is not directly related to the research question being asked. For example, a study that uses a standardized math test to measure students’ critical thinking skills may have high reliability because the test is consistently accurate and produces similar scores each time it is administered. However, the study may have low validity because the math test is not an appropriate tool for measuring critical thinking skills. As a result, the results of the study may not be representative of the students’ true critical thinking abilities.\n\n\nHigh validity and low reliability\nAn example of a study that has high validity but low reliability is a study that asks people to self-report their eating habits. While the study may produce accurate and representative results about people’s eating habits, the self-reported data may vary from person to person and may not be consistent over time. As a result, the study has high validity but low reliability.\nAnother example of a study that has high validity but low reliability is a study that uses a highly valid measurement tool, such as a survey, to measure a concept that is directly related to the research question being asked. However, the study may have low reliability because the survey is not administered consistently or the responses are not accurately recorded. For example, a study that uses a survey to measure students’ attitudes towards school may have high validity because the survey is relevant to the research question and accurately measures the students’ attitudes. However, if the survey is not administered consistently or the responses are not accurately recorded, the study may have low reliability. As a result, the results of the study may not be representative of the students’ true attitudes towards school.\n\n\nTrade-offs between reliability and validity\nIn research design, trade-offs may need to be made between reliability and validity. For example, a study that uses a highly reliable measurement tool may not be valid if the tool is not appropriate for the research question being asked. Similarly, a study that uses a highly valid measurement tool may not be reliable if the tool is prone to producing inconsistent results. As a result, researchers must carefully consider both reliability and validity when designing a study and make trade-offs as necessary to maximize the overall quality of the research.\n\n\n\n1.5.2 Generalizability\nComing back to my little son who threw everything within reach to the ground and giggled with joy at the clink he made when the object hit the ground. He identified a cause-and-effect relationship through an experiment in an controlled environment. His law “I throw something off the table and it always clangs” worked in our home. To our regret, it was replicateable and he really tried hard to falsify it. Moreover, his study was reasonable valid as his study design, conduct, and analysis could answer his research questions without bias (at least ignoring the other noises that his sibling and parents make coincidentally during his experiment). Scientist call this internal validity. However, he also found out that when he leaves our home, things are sometimes a bit different, for example, if there is a carpet under the table. Thus, his insights from our home findings can’t be generalized to other contexts, at least not without further specifications. Scientist call this external validity.\n\n\n\n\n\n\nTip 1.2\n\n\n\nInternal validity examines whether the study design, conduct, and analysis answer the research questions without bias. External validity examines whether the study findings can be generalized to other contexts.\n\n\n\n\n1.5.3 Replicability, reproducibility, transparency, and other criteria\nIt must be possible to repeat the research conducted for several reasons. For example, if you can repeat a study with slightly changed parameters, you are able to improve its external validity and show that the conclusions drawn are reliable. To be able to repeat a study, everything that is important for drawing a conclusion from the research has to be mentioned. This is what we call transparency. Moreover, everything in the study must have been done in such a way that we can check the results for truth. In the best case, it is possible to reproduce the results in the same way they were obtained in the study. Sometimes this is not possible because, for example, we can never really ask the same people again in a survey, and even if we found the same people, they would have gotten older and not be the same people as before. In such a case, it should at least be possible to replicate the research. This means that we can basically do the same thing in a setting that differs only in those things that we cannot avoid to be different. For example, by interviewing a group of people who match the people in the study to replicate them on all the important characteristics like age.\nIn an empirical quantitative research study, for example, the data and the code written to process the data and analyze it should be accessible to everyone.\nIn a qualitative study, all sources of information should be stated, and the circumstances leading to a conclusion should be fully explained. For example, all transcripts of interviews conducted should be made available. The researcher should provide rich and detailed descriptions of the data and the context in which it was collected. Research should be provided with rich, nuanced, and multi-layered accounts of social phenomena by describing and interpreting the meanings, beliefs, and practices of the people being studied. That is known as thick description. Researchers typically employ a variety of methods such as participant observation, in-depth interviews, and document analysis, and they often use multiple sources of data to triangulate their findings. The goal is to provide a holistic and broad understanding of the phenomenon being studied, rather than a narrow view from the researcher’s perspective.\nThere are some other criteria of good research that are worth mentioning:\n\nCredibility\nThe research should be trustworthy and believable, and the researcher should provide detailed descriptions of the methods used to ensure transparency.\n\n\nReflective Practice\nThe researcher should engage in reflexive practice throughout the research process, which means to be critically aware of oneself, one’s own assumptions, and one’s own role in the research process.\n\n\nTriangulation\nThe researcher should use multiple methods, sources, and perspectives to increase the credibility of the findings (also see thick description above).\n\n\nTransferability\nThe conclusions drawn from looking at mostly unstructured data in qualitative research can hardly be generalized in a strict sense, as they depend crucially on the context of the object of study. For example, generalizability is essentially impossible in a qualitative case study, since everything depends on the specific situation of an individual, a company, or a group of people considered in the specific setting. This means that in a case study or interview, we may be looking at only a few or even a single observation that cannot be considered representative of the larger population, as generalizability does. Transferability, on the other hand, gives the reader the ability to transfer the findings into other contexts. The ability to transfer contextual findings to other cases is a goal of qualitative research, and the author of a study should attempt to offer the information in a way that allows the reader to transfer the findings to the setting or situation with which he or she is familiar.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#the-role-of-resources-data-and-ethics",
    "href": "doing_research.html#the-role-of-resources-data-and-ethics",
    "title": "1  Doing research",
    "section": "1.6 The role of resources, data and ethics",
    "text": "1.6 The role of resources, data and ethics\nThere are several types of research designs, including experimental designs, quasi-experimental designs, and observational designs. Each of these designs took advantage of various empirical methods and statistical procedures. We will discuss some of them later on. The choice of research design, of course, should depend on the research question being asked, the resources available, and the type of data that is being collected. The research design should also take into account any ethical considerations that may be relevant to the research. The research design should be chosen so that it is well suited to answer the research question. For example, if one is interested in the question “Why do some people get sick with a certain disease and others do not?” then an observational study design to determine possible causes of effect may be appropriate. These identified potential causes should then be verified followed by an experimental study. Relatively, a statistical analysis should be used which would allow the effects of causes to be evaluated. The aim should be to identify necessary and sufficient circumstances to develop a disease. Also circumstances should be described that favor a disease.\nIf the question is a “how” question, for example, “How do parents feel when their child throws everything off the table?” then interviews might be an appropriate study design. If available resources such as time, funding, and staff are limited, you might also consider conducting an (online) survey in which parents are asked standardized questions about their feelings. In any way, the chosen research design must be feasible given the resources available.\nIn answering a question, a researcher should know, state, and discuss all the assumptions and unexamined beliefs that led him to his conclusion. However, since resources for conducting and explaining research are limited, special attention should be paid to what are called critical assumptions. These are assumptions that must be true in reality, otherwise the research is meaningless. Therefore, researchers should make great efforts to identify and validate these assumptions.\nThe type of data that is being collected is another important factor to consider when choosing a research design. Different types of data, such as quantitative data, qualitative data, or a combination of both, may require different methods of collection and analysis. For example, quantitative data, such as numerical data, can be collected through methods such as surveys and analyzed using statistical techniques, whereas qualitative data, such as interview transcripts, may require more interpretive methods of analysis.\nFinally, the researcher should also take into account any ethical considerations that may be relevant to the research. For example, if the study involves human subjects, the researcher must ensure that the study is conducted in accordance with ethical principles such as informed consent and confidentiality. Additionally, the researcher should ensure that the potential benefits of the study outweigh any potential risks to the subjects.\n\n\n\n\n\n\n\nExercise 1.2 Features of research\n\nWhich of the following best defines reliability in research?\n\nThe extent to which a measurement tool produces consistent results\nThe extent to which a study’s results accurately reflect the concept being measured\nThe extent to which a study’s results can be generalized to other populations\nThe extent to which a study’s results are statistically significant\n\nWhich of the following best defines validity in research?\n\nThe extent to which a measurement tool produces consistent results\nThe extent to which a study’s results accurately reflect the concept being measured\nThe extent to which a study’s results can be generalized to other populations\nThe extent to which a study’s results are statistically significant\n\nWhich of the following is an example of a study with high reliability but low validity?\n\nA study that uses a highly reliable measurement tool to measure a concept that is directly related to the research question being asked\nA study that uses a highly valid measurement tool to measure a concept that is not directly related to the research question being asked\nA study that uses a highly reliable measurement tool to measure a concept that is not directly related to the research question being asked\nA study that uses a highly valid measurement tool to measure a concept that is directly related to the research question being asked\n\nWhich of the following is an example of a study with high validity but low reliability?\n\nA study that uses a highly reliable measurement tool to measure a concept that is directly related to the research question being asked\nA study that uses a highly valid measurement tool to measure a concept that is not directly related to the research question being asked\nA study that uses a highly reliable measurement tool to measure a concept that is not directly related to the research question being asked\nA study that uses a highly valid measurement tool to measure a concept that is directly related to the research question being asked\n\nWhat does internal validity examine in a study?\n\nThe ability to replicate the study\nThe generalizability of the study’s findings\nWhether the study design, conduct, and analysis answer the research questions without bias\nAll of the above\n\nWhat does external validity examine in a study?\n\nThe ability to replicate the study\nThe generalizability of the study’s findings\nWhether the study design, conduct, and analysis answer the research questions without bias\nNone of the above\n\nWhat is transparency in research?\n\nThe ability to replicate a study\nThe generalizability of the study’s findings\nThe availability and accessibility of the data and materials used in a study for others to review\nThe ethical considerations of the research\n\nWhat are the different types of research design discussed in the text?\n\nExperimental designs, quasi-experimental designs, and observational designs\nExperimental designs and descriptive designs\nQuasi-experimental designs and observational designs\nNone of the above\n\nWhy is replicability important in a study?\n\nTo be able to repeat a study with slightly changed parameters and thus improve the external validity\nTo be able to check the results of the study for truth.\nTo be able to reproduce the results in the same way they were obtained in the study\nAll of the above\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\na), 2. b), 3. c), 4. d), 5. c), 6. b), 7. c), 8. a), 9. d)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "doing_research.html#glossary",
    "href": "doing_research.html#glossary",
    "title": "1  Doing research",
    "section": "1.7 Glossary",
    "text": "1.7 Glossary\n\nGeneralizability: The extent to which the results of a study can be applied to other populations or contexts.\nInternal validity: The degree to which a study’s results can be attributed to the specific variables or factors being studied, and not to other extraneous factors.\nExternal validity: The degree to which a study’s results can be generalized to other populations or contexts outside of the specific sample or setting of the study.\nQuantitative data: Data that can be measured and quantified.\nQualitative data: Data that cannot be easily measured or quantified.\nQuantitative research: A research approach that uses statistical methods and experiments to determine the causes of effects, to quantify the effects of causes, or to describe data.\nQualitative research: A research approach that uses unstructured data and methods to examine, for example, people’s behavior, beliefs, and experiences in depth, rather than quantifying results.\nReflective Practice: A form of self-evaluation used to analyze one’s own thoughts and actions.\nReliability: The consistency of a study’s results to produce similar results when repeated.\nResearch design: A detailed plan for conducting a study, frequently used interchangeably with research strategy.\nResearch method: A procedure used to conduct a study or investigation to gain knowledge or understanding about a particular topic.\nResearch question: A question or problem that a study aims to answer or solve.\nResearch strategy: A general plan for conducting a study, frequently used interchangeably with research design.\nReplicability: The ability of a study to be repeated with new data.\nReproducibility: The ability of a study to be repeated and produce the same results, often used interchangeably with replicability.\nSerendipity: The role of luck and unexpected events in research.\nThick Description: A detailed narrative used to explain a situation and its context.\nCredibility: A quality criterion in qualitative research, which refers to confidence in the truth value of the data and interpretations of them.\nTransparency: The degree to which a study’s methods and data are easily accessible and understandable to others, allowing for the study to be independently evaluated and replicated.\nTriangulation: A method used in qualitative research to verify the accuracy of data by combining multiple sources of information.\nValidity: The degree to which a study measures what it is intended to measure, and the extent to which the results of the study can be considered accurate and meaningful.\nStructured data: Data that can be easily organized and analyzed in a structured format, such as a spreadsheet.\nUnstructured data: Data that cannot be easily organized and analyzed in a structured format, such as text, images, and audio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Doing research</span>"
    ]
  },
  {
    "objectID": "good_research.html",
    "href": "good_research.html",
    "title": "2  Good research",
    "section": "",
    "text": "2.1 From anecdote to insight\nData is everywhere in today’s world, and access to data and facts on almost any topic is more affordable than ever. Howver, the abundance of information often leads to confusion and risks drawing the wrong conclusions. Even as a professor of data science, I sometimes feel overwhelmed by the sheer amount of information available.\nWhile researchers, businesses, and individuals use facts to gain insights that are relevant to their interests, not everyone is trained to interpret data effectively, making it difficult to turn raw data into real insights.1 Many people (intentionally or unknowingly) misuse and misinterpret information and also scientific content. Many research papers exist that misapply empirical methods and therefore produce biased results or hide or disguise the potential weaknesses of the results. Numerous predatory journals are full of papers that have not undergone the rigorous peer review process typical of reputable publishers that strive to maintain qualitative standards.\nIn this chapter, I will continue the discussion started in Section 1.5 on what constitutes good research. I will examine the advantages and disadvantages of anecdotal evidence Section 2.1, discuss how to evaluate quantitative information from public sources Section 2.2, assess the quality of academic publications Section 2.4, and identify literature of questionable quality Section 2.5. This section aims to highlight key elements for identifying problematic or misleading literature, but it is not exhaustive.\nAnecdotes are great. They are true stories—often intriguing, relatable, and easy to understand. They provide vivid examples that make abstract ideas more concrete and memorable. Whether it’s a personal experience or a captivating story about a successful business leader, anecdotes resonate because they tap into our natural affinity for storytelling. Their simplicity and emotional impact can make them powerful teaching tools.\nAnd importantly, anecdotes are hard to contradict. Take, for example, the argument that smoking can’t be that harmful because your 88-year-old uncle has smoked his entire life and he is still in good health. It’s a tough claim to refute, as it’s a real-life example. However, the problem lies in extrapolating a single, isolated case to draw broader conclusions, which can be misleading.\nHowever, while anecdotes can be persuasive, their strength is also their weakness. They represent isolated instances, and while it’s hard to deny the truth of an individual story, the danger lies in overgeneralizing from it. Anecdotes lack the rigorous analysis and breadth of evidence necessary to draw reliable conclusions. They don’t account for the full complexity of most situations, especially in business, where decisions are influenced by many interconnected factors.\nIn business, relying too heavily on anecdotes can lead to misguided conclusions. For example, a company might base its strategy on the success story of a famous entrepreneur without considering the countless failed ventures that didn’t make the headlines. This is known as survivorship bias, where the successes are visible, but the failures are hidden.\nThe challenge, then, is to take anecdotes and go beyond them. Instead of drawing direct conclusions, use them as starting points for deeper investigation. They can provide valuable hypotheses but need to be supported by data, rigorous analysis, and an understanding of the underlying principles at play. Anecdotes can inspire curiosity and point us in interesting directions, but they should be tested against a larger body of evidence to ensure that the insights we draw are reliable and applicable in a broader context.\nDrawing insights from anecdotes is challenging, especially in business, for several reasons:\nThus, to make informed business decisions, it is critical to base insights on systematic data analysis rather than anecdotal evidence, as anecdotes are too narrow, subjective and unreliable to guide complex business strategies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-anecdoteinsi",
    "href": "good_research.html#sec-anecdoteinsi",
    "title": "2  Good research",
    "section": "",
    "text": "Exercise 2.1 Survivorship bias\nRead “How Successful Leaders Think” by Roger Martin (2007). Here is a summary of Martin (2007) taken from the Harvard Business Review Store:\n\nIn search of lessons to apply in our own careers, we often try to emulate what effective leaders do. Roger Martin says this focus is misplaced, because moves that work in one context may make little sense in another. A more productive, though more difficult, approach is to look at how such leaders think. After extensive interviews with more than 50 of them, the author discovered that most are integrative thinkers–that is, they can hold in their heads two opposing ideas at once and then come up with a new idea that contains elements of each but is superior to both. Martin argues that this process of consideration and synthesis (rather than superior strategy or faultless execution) is the hallmark of exceptional businesses and the people who run them. To support his point, he examines how integrative thinkers approach the four stages of decision making to craft superior solutions. First, when determining which features of a problem are salient, they go beyond those that are obviously relevant. Second, they consider multidirectional and nonlinear relationships, not just linear ones. Third, they see the whole problem and how the parts fit together. Fourth, they creatively resolve the tensions between opposing ideas and generate new alternatives. According to the author, integrative thinking is an ability everyone can hone. He points to several examples of business leaders who have done so, such as Bob Young, co-founder and former CEO of Red Hat, the dominant distributor of Linux open-source software. Young recognized from the beginning that he didn’t have to choose between the two prevailing software business models. Inspired by both, he forged an innovative third way, creating a service offering for corporate customers that placed Red Hat on a path to tremendous success.\n\n\nDiscuss the concepts introduced by Martin (2007) critically:\n\n\nDoes he provide evidence for his ideas to work?\nIs there a proof that his suggestions can yield success?\nIs there some evidence about whether his ideas are superior to alternative causes of action?\nWhat can we learn from the article?\nDoes his argumentation fulfill highest academic standards?\nWhat is his identification strategy with respect to the causes of effects and the effects of causes?\nMartin (2007, p. 81) speculates:\n\n\n“At some point, integrative thinking will no longer be just a tacit skill (cultivated knowingly or not) in the heads of a select few.”\n\n\nIf teachers in business schools would have followed his ideas of integrative thinkers being more successful, almost 20 years later, this should be the dominant way to think as a business leader. Is that the case? And if so, can you still gain some competitive advantage by thinking that way?\n\n\n\n\nFigure 2.1: Distribution of bullet holes in returned aircraft\n\n\n\nSource: Martin Grandjean (vector), McGeddon (picture), Cameron Moll (concept), CC BY-SA 4.0, Link\n\n\n\n\nFigure 2.1 visualizes the distribution of bullet holes in aircraft that returned from combat in World War II. Imagine you are an aircraft engineer. What does this picture teach you?\nInform yourself about the concept of survivorship bias explained in Wikipedia (2024).\nIn Martin (2007), the author provides an example of a successful company to support his management ideas. Discuss whether this article relates to survivorship bias.\n\n\n\n\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business Review, 85(6), 71–81. https://hbr.org/2007/06/how-successful-leaders-think\n\n\nLimited sample size: Anecdotes are usually individual cases that do not reflect the full extent of a situation. In business, decisions often require data from large, diverse populations to ensure reliability. Relying on a single story or experience can lead to conclusions that are not universally valid.\nBias and subjectivity: Anecdotes are often influenced by personal perspectives, emotions or particular circumstances. Moreover, anecdotes often highlight success stories while ignoring failures. This is an example for the so-called Survivorship Bias.\nLack of context and the inability to generalize: Anecdotes often lack the broader context necessary to understand the underlying factors of a situation. Business problems tend to be complex and influenced by numerous variables such as market trends, consumer behavior and external economic conditions. Many of these variables change significantly over time. Without this context, an anecdote can oversimplify the problem and lead to incorrect decisions. Anecdotes are usually specific to a particular time, place or set of circumstances. They may not apply to different markets, industries or economic environments, which limits their usefulness for general decision-making. For example, learning only from the tremendous success of figures like Steve Jobs while ignoring the countless people who failed is like learning how to live a long life by talking to a single 90-year-old person. If that person happens to be obese and a heavy smoker, it doesn’t mean those behaviors contributed to their longevity.\nLack of data rigor: Anecdotes lack the rigor and precision of data-driven analysis where the empirical model that allows to identify causality and to measure the effect of causes is formally described.\n\n\n\n\n\n\n\n\n\nExercise 2.2 Systematic analysis as an alternative to anecdotal analysis\n\nWhat defines a systematic analysis?\nWhen can we say that we have ‘found evidence’?\nWhen can we claim to have identified a causal effect?\nWhen can we trust the size of an effect that we have measured?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-datatoinsights",
    "href": "good_research.html#sec-datatoinsights",
    "title": "2  Good research",
    "section": "2.2 Information and insights",
    "text": "2.2 Information and insights\nThe information and the data you come across in private and professional contexts often comes from sources with specific interests that have reasons to manipulate your perception. These manipulators usually present their arguments and insights in a polemical manner and disregard counter-arguments that could weaken their position.\nWithout the ability to critically evaluate information and the insights derived from it, it’s easy to be manipulated by persuasive narratives. Manipulators work diligently to come up with convincing stories that feel intuitively right. For example, the tobacco industry has long successfully promoted the idea that smoking is healthy.\nI see two effective strategies for recognizing manipulators, becoming a more discerning consumer of information, and improving your ability to refute fallacious arguments that yield to false insights. Firstly, identify manipulators and, secondly, be educated enough to evaluate the arguments or to consult an objective expert that can evaluate the information for you.\nTo recognize manipulative arguments, you should familiarize yourself with the tactics manipulators use. Below, I explain some methods manipulators use to create narratives that resonate emotionally while using misleading information to lead readers to desired conclusions:\n\nEmotional appeals: Manipulators often evoke fear or pity to trigger emotional responses that can undermine critical thinking. Such emotional appeals should raise suspicions about the intentions of the information provider because it is difficult to be objective when you are charged with emotion.\nSelective data presentation: Manipulators frequently cherry-pick statistics that support their arguments while ignoring contrary evidence, skewing the overall picture. A one-sided presentation sometimes demonstrates the author’s unwillingness to challenge their views.\nManipulative language: This includes language designed to distort facts or exaggerate claims, creating an illusion of certainty or urgency that can easily mislead.\nPseudo-expertise: Manipulators may present themselves as authorities or experts, despite lacking the appropriate credentials to back up their claims.\nCiting research: By referencing and citing research findings, manipulators capitalize on the fact that most people would have a hard time effectively disputing these claims.\n\nRecognizing these tactics can create skepticism in the right place and motivate you to critically evaluate the claims that are being made.\nTo challenge manipulative arguments, you need to study the subject on your own. This often requires a solid understanding of both quantitative and qualitative research methods. I hope this course and the lecture notes can assist you in building that understanding. However, if you feel that you lack these skills, I highly recommend consulting an objective expert. The alternative is to risk becoming a potential victim of manipulation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-dogs",
    "href": "good_research.html#sec-dogs",
    "title": "2  Good research",
    "section": "2.3 Case study: Dogs and Votes",
    "text": "2.3 Case study: Dogs and Votes\nIn business, politics, and personal life, individuals often need to gather information in fields outside their expertise to make informed decisions. The information encountered in these situations is frequently complex and challenging to understand. Therefore, it is essential to find a reliable source that offers valid and trustworthy insights. This can be a significant challenge. Evaluating the reputation of the organization and the author of the information is one strategy to avoid low-quality data, but even this can be difficult to ascertain in certain fields.\nIn this section, I discuss the studies by Lippert & Sapy (2003) and Curtis et al. (2021). The first study examines the quality of dog food and the life expectancy of dogs, while the latter focuses on human life expectancy and voting behavior, specifically the tendency to vote for Republicans. Despite lacking expertise in veterinary or political science, our background in quantitative methods and scientific analysis allows us to identify Lippert & Sapy (2003) as a prime example of poor research design, where empirical methods are misapplied and results are misinterpreted. This study invites readers to draw misleading conclusions and misuse the findings presented. In contrast, the study by Curtis et al. (2021) demonstrates a much stronger research approach.\nBoth cases should illustrate how the knowledge you gain in this course can enhance your ability to inform yourself more effectively by enabling you to evaluate the quality of the information you encounter.\n\n2.3.1 Lippert & Sapy (2003): Life expectancy of dogs\nIf you love your dog, you naturally want him to live a long and happy life. However, uncovering the secrets to dog longevity can be challenging. Unlike humans, dogs don’t choose their food because they usually only eat what their owner feeds them. Therefore, owners have a responsibility to educate themselves on a proper diet for their dogs.\nA quick Google search can yield a wealth of information about dog longevity. For example, here is an excerpt from luckydogcuisine.com, a company that provides fresh cooked food for dogs:\n\n“Diet does help dogs live longer!\nHere at Lucky Dog Cuisine, we believe in feeding fresh foods to our dogs. We have been doing this for over 50 years and with good reason!\nIn a study out of Belgium, “Relation between the Domestic Dogs: Well-Being and Life Expectancy, a statistical essay”, used data gathered from more than 500 domestic dogs over a five year time frame (1998 to 2002).\nDrs. Lippert and Sapy, the authors showed statistically that dogs fed a homemade diet, consisting of high quality foods (not fatty table scraps) versus dogs fed an industrial commercial pet food diet had a life expectancy of 32 months longer – that’s almost 3 years!\nIt is the quality of the basic ingredients and the way they are processed that makes the difference. High heat cooking, extrusion and flaking as well as chemical treatment using preservatives and additives were found to be factors in destroying ingredient integrity.\nOur fresh foods are cooked the old fashioned way using steam and then frozen. No chemicals or preservatives ever!\nWe’re excited to be a part of helping dogs live longer and healthier lives.”\n\nAfter reading this blog post, you may feel compelled to buy high-quality food and prepare fresh meals for your dog. However, before you invest your money, you should review the information presented. After all, it comes from a company that wants to make a profit by selling fresh dog food. Therefore, you decide to do further research and read the article the blog references. As the provided link to the paper does not work, you search for the authors in literature databases such as PubMed and Google Scholar but without any results, likely because the paper has not been published in any journal or scholarly literature. After some effort with the unrestricted Google search engine, I found the paper by Lippert & Sapy (2003). It appears to be an essay that was once submitted for a prize from the Prince Laurent Foundation.\nThe authors write in their essay (Lippert & Sapy, 2003, p. 5):\n\n“Our objective is to study the influence of these various parameters on the life expectancy of the dog. The statistical use of more than 500 “DOGS’ LIFE SHEETS or DOGS’ FAMILY BOOK”, collected during five years, from 1998 until 2002, will be our basic working material and will serve to extract a causality connection between quality of life, animals’ well-being and life expectancy.\nWe took into consideration three categories of food 1. Home made: With products used from the owner’s meals 2. Mixture: A mix of home made and industrial food 3. Industrial: Retail sold dogs food\nThe difference between the 2 extremes amounts to more than 32 months. ( approximately 3 years)** This difference is important (F Value : 6.67 ; Pr&gt;F : 0017). Food is consequently of great importance for the life expectancy of the dog. We can consider that home made food is a protection factor for the domestic dog.\n\nIn Figure 2.2, I show a screenshot of the relevant pages, providing insight into the impact of food on life expectancy Lippert & Sapy (2003) claim to have found.\n\n\n\nFigure 2.2: Home made food as a protection factor\n\n\n\nSource: Lippert & Sapy (2003, pp. 12–13).\nLippert, G., & Sapy, B. (2003). Relation between the domestic dogs’ well-being and life expectancy statistical essay: Essay for the Prince Laurent Foundation Price. https://www.cavalierhealth.org/images/Lippert_Sapy_Domestic_Dogs_Life_Expectancy.pdf\n\n\n\n\nThere are several flaws and issues in this paper. Foremost, the interpretation of the presented results is false. Before going into details, let us clarify the meaning of the three numbers shown in the figure: They represent the average lifespan, in months, of dogs across three categories. Dogs fed with “products used from the owner’s meal” lived an average of about 157 months, those fed a “mix of homemade and industrial food” reached about 136 months, and dogs that consumed “retail-sold dog food” lived for about 124 months.\nHere is an incomplete list of the weaknesses identified in the study:\nPseude-expertise:\n\nThere are several issues, such as punctuation and spacing errors, as well as the figure mixing English and French terms while employing a three-dimensional format that is difficult to read. Additionally, the inconsistent use of bold and italics can be confusing. While these issues may seem minor, they indicate a lack of attention to detail on the part of the authors and raise doubts about the professionalism of the researchers. Although these factors may not directly affect the scientific quality of the investigation, they often correlate strongly with it and are relatively easy for readers to identify.\nThe authors claim to test the differences “between the two extremes.” While it seems clear they mean the averages of homemade-fed dogs versus industrially-fed dogs, they are looking at averages, not extremes. The language used is unclear.\nIn the acknowledgments, they thank several individuals for their “priceless assistance”, “erudition in statistics”, “translation into English”, and for their willingness “to read and correct the translation”. While it is commendable that they acknowledge this help, the overall impression of the article raises doubts about their academic proficiency and independence. For example, they do not relate their contribution to the current state of research as they do not have a reference list. Moreover, the dataset, with 522 observations and a dozen variables, does not appear particularly sophisticated, and the statistical tasks do not exceed the level of a Statistics 101 course. This raises the question of why two doctors of science require assistance with statistics.\n\nManipulative language:\n\nThe term ‘approximately’ implies closeness; however, 36 months (three years) is actually 12.5% greater than 32 months, indicating it is not a close estimate as the authors want us to believe. A closer approximation would be two and a half years.\nThe only statistical test referenced appears to be a mean difference test. The authors misinterpret this statistical significant difference by stating that “the difference is important.” Statistical significance does neither imply importance nor causality (see Section 3.1).\nComparing the averages of two groups of dogs that are statistically different, they conclude that\n\n“Food is consequently of great importance for the life expectancy of the dog”,\n“home made food is a protection factor for the domestic dog”, and\n“animals who receives varying home made food, will have the benefit of a longer life expectancy.”\n\nThe empirical results are straight forward and leave no room for doubt. This is very much in line with my definition of manipulative language, as it creates an illusion of certainty in order to exaggerate claims.\n\nSelective data representation:\n\nThe authors neither provide details on their statistical analyses nor on their data. We don’t know the sample sizes for each group, nor is the standard deviation provided for the statistics shown. The reproducibility of the paper is not given, and the transparency of their data work is low. I will discuss these issues in greater detail in Section 2.4.\n\nMethodological issues:\n\nAs we will learn in Section 3.1, differences in the averages of the treated (homemade food) and untreated (industrial food) groups can indicate a causal effect, assuming the conditions of ignorability and unconfoundedness hold. However, the authors provide no evidence that these assumptions are satisfied. In fact, the remainder of the article presents strong arguments suggesting that these assumptions do not hold.\nThe dogs were not randomly assigned to the groups. The authors do not rule out the possibility that group assignments were based on characteristics such as weight, breed, and gender, which they also find associated with life expectancy. For example, larger dogs may be less likely to be fed homemade food due to the greater cost and effort required. The authors noted that breeds like Pyrenean Mountain and Big Danish dogs lived significantly shorter lives on average than smaller breeds like Poodles and Yorkshire. Thus, differences in size and other factors alike could be responsible for the observed group differences. The authors do not discuss the composition of the two groups and do not appear to have made efforts to ensure comparability. For example, they could have compared homemade-fed Poodles with Poodles fed industrially.\nEven if the groups were similar, the observed averages may be influenced by confounding factors. For example, it is plausible that dogs receiving more expensive homemade food are treated differently across various aspects, including medical care-factors that may contribute to life expectancy. If this is the case, the dogs might not live longer because of the food but rather due to other factors correlated with the type of food they receive. This illustrates a classic case of bias due to omitted variables.\n\nOverall, the information provided in the study should not be used to make causality claims about the life expectancy of dogs on average. The two authors merely compared two different groups of dogs without controlling for confounding factors and without addressing potential sources of bias.\n\n\n2.3.2 Curtis et al. (2021): Life expectancy of humans\n\nCurtis, L. H., Hoffman, M. N., Califf, R. M., & Hammill, B. G. (2021). Life expectancy and voting patterns in the 2020 US presidential election. SSM-Population Health, 15, 100840.\nTo clarify this issue further and end with a better example, consider the example from Curtis et al. (2021), which found that life expectancy within regions is positively associated with the Democratic share of votes in 2016 and 2020. However, the authors do not assert a causal relationship using sensationalist language like “voting for Republicans kills”. Instead, they critically discuss the correlations while being aware of potential sources of bias that may influence the correlation, and they do not appear to have any obvious interests in misleading the reader or over-selling their results. For example, they emphasize that the\n\n“associations were moderated by demographic, social and economic factors that should drive health policy priorities over the coming years”\n\nand that their\n\n“study is limited by its ecological nature and the inability to associate individual voting behaviors with health outcomes. Additionally, the study is limited by available data sources and would benefit from more detailed social and economic data.”\n\nMoreover, the article was published in SSM - Population Health, a well-received peer-reviewed journal from Elsevier, which is managed by professionals from prestigious universities, including Harvard and RMIT University.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-repandtrans",
    "href": "good_research.html#sec-repandtrans",
    "title": "2  Good research",
    "section": "2.4 Reproduceability and transparency",
    "text": "2.4 Reproduceability and transparency\nWhile we have already addressed essential characteristics of good research such as validity and reliability see 1.5, we have not explicitly discussed how to recognize high-quality research in academic publications.\nThe quality of research depends largely on the honesty of the researcher and the transparency of the methods used. Although there is no absolute measure of honesty, certain indicators can provide some insight. For example, the reputation of the researcher, the institution with which he or she is associated and the credibility of the publisher, editor or journal can serve as helpful pointers. If a researcher is highly regarded in the academic world or in public life, he has much to lose by compromising his integrity. Unfortunately, this logical argument does not exclude the possibility that even respected individuals may occasionally engage in dishonest practices. History has shown that this can happen. Nevertheless, the more someone has to lose by being dishonest, the less likely they are to cheat, plagiarize, or falsify data and results, respectively.\nA clear empirical methodology forms the foundations of any empirical research work. However, to ensure the wide dissemination of a study, two aspects are crucial: accessibility and dissemination. Nowadays, public accessibility of a paper is relatively easy to achieve because anyone can upload their work online to ensure its availability. Dissemination, however, is more complex. While there are various strategies to increase the popularity of a paper, the traditional and arguably most effective method is to publish the paper in a high-impact journal or a popular journal with a large readership. The successful placement of a paper in such journals is almost an art form. The paper must fulfill several criteria as well as possible: a sound methodology, a clear contribution, thematic relevance and a compelling text that appeals to the journal’s readership are just some of the key benchmarks.\nHow can you learn to publish a paper successfully? I think that it is crucial to study papers that have gained recognition and make an effort to recognize the reasons for their success. Doing so sharpens your writing skills, improves your perceptiveness as a researcher, and helps you to select important research topics and to design research projects more efficiently.\nPrestigious journals like the American Economic Review and the Journal of Economic Literature, published by the American Economic Association (AEA), have refined and enhanced their guidelines over time. These rules are often regarded as a benchmark in academic publishing, not necessarily because they set the highest possible standards, but because they strike a balance that takes into account the practical realities faced by researchers, reviewers, and editors. Moreover, readers often prefer a swift review process, too, because they want papers that connect with current events. In that respect, it’s important to acknowledge the inherent trade-off journals must navigate: stringent standards can slow the pace of research and publication. This is particularly critical in fields like economics and business, where timely insights can significantly impact politics and society, addressing urgent needs.\nThis section looks at the AEA’s publishing standards. The path to publication with the AEA, particularly for empirical research, has changed considerably and is now governed by strict criteria. It is imperative that authors familiarize themselves with the key considerations and guidelines for submitting empirical research to an AEA journal. Although standards may vary across disciplines, the AEA’s exacting standards have influenced a wide range of social science journals. Engagement with these standards is invaluable as it provides guidance in designing and planning research that meets these rigorous requirements.\nWhile each AEA journal has its specific focus and requirements, all aim for integrity, clarity, and replicability of empirical research. Authors should start by carefully reviewing the author guidelines for their targeted journal, paying close attention to any specific mandates regarding empirical work. For Randomized Controlled Trials (RCTs), for example, a registration is required for all applicable submissions prior to submitting, see RCT Registry Policy.\n\n\n\n\n\n\nTip 2.1: The AEA’s registry for randomized controlled trials\n\n\n\nVisit www.socialscienceregistry.org and inform yourself about some registered RCTs.\n\n\n\n\n\nFigure 2.3: The spectrum of reproducibility\n\n\n\n\n\n\nThe AEA has placed a significant emphasis on the transparency and replicability of empirical research with the objective of ensuring that the empirical results can be fully replicated (see Figure 2.3). But why is replicability so crucial? Well, trust is good, control is better. Or, as Peng (2011, p. 1226) put it:\n\nPeng, R. D. (2011). Reproducible research in computational science. Science, 334(6060), 1226–1227.\n\n“Replication is the ultimate standard by which scientific claims are judged. With replication, independent investigators address a scientific hypothesis and build up evidence for or against it. The scientific community’s “culture of replication” has served to quickly weed out spurious claims and enforce on the community a disciplined approach to scientific discovery.” the Authors are required to ensure that their data and methodologies are openly available and clearly described, allowing other researchers to replicate their results. This commitment to transparency extends to the publication of data sets, code, and detailed methodological appendices, which must accompany the submitted manuscript.\n\nTable 2.1 is taken from Nosek et al. (2015) and it breaks down how scientific journals ask researchers to follow more strict rules, from Level 0 to Level 3, across eight different important categories. All these standards follow the objective to come closer to the gold standard shown in Figure 2.3.\n\nNosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G., et al. (2015). Promoting an open research culture. Science, 348(6242), 1422–1425.\n\n\n\nTable 2.1: Summary of the eight standards and three levels of the TOP guidelines\n\n\n\n\n\n\n\n\n\n\n\nLevel 0\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\nCitation Standards\n\n\n\n\n\nJournal encourages citation of data, code, and materials—or says nothing.\nJournal describes citation of data in guidelines to authors with clear rules and examples.\nArticle provides appropriate citation for data and materials used, consistent with journal’s author guidelines.\nArticle is not published until appropriate citation for data and materials is provided that follows journal’s author guidelines.\n\n\nData Transparency\n\n\n\n\n\nJournal encourages data sharing—or says nothing.\nArticle states whether data are available and, if so, where to access them.\nData must be posted to a trusted repository. Exceptions must be identified at article submission.\nData must be posted to a trusted repository, and reported analyses will be reproduced independently before publication.\n\n\nMethod Transparency (Code)\n\n\n\n\n\nJournal encourages code sharing—or says nothing.\nArticle states whether code is available and, if so, where to access them.\nCode must be posted to a trusted repository. Exceptions must be identified at article submission.\nCode must be posted to a trusted repository, and reported analyses will be reproduced independently before publication.\n\n\nMaterial Transparency\n\n\n\n\n\nJournal encourages materials sharing—or says nothing.\nArticle states whether materials are available and, if so, where to access them.\nMaterials must be posted to a trusted repository. Exceptions must be identified at article submission.\nMaterials must be posted to a trusted repository, and reported analyses will be reproduced independently before publication.\n\n\nDesign Transparency\n\n\n\n\n\nJournal encourages design and analysis transparency or says nothing.\nJournal articulates design transparency standards.\nJournal requires adherence to design transparency standards for review and publication.\nJournal requires and enforces adherence to design transparency standards for review and publication.\n\n\nPreregistration: Study\n\n\n\n\n\nJournal says nothing.\nJournal encourages preregistration of studies and provides link in article to preregistration if it exists.\nJournal encourages preregistration of studies and provides link in article and certification of meeting preregistration badge requirements.\nJournal requires preregistration of studies and provides link and badge in article to meeting requirements.\n\n\nPreregistration: Analysis Plan\n\n\n\n\n\nJournal says nothing.\nJournal encourages preanalysis plans and provides link in article to registered analysis plan if it exists.\nJournal encourages preanalysis plans and provides link in article and certification of meeting registered analysis plan badge requirements.\nJournal requires preregistration of studies with analysis plans and provides link and badge in article to meeting requirements.\n\n\nReplication\n\n\n\n\n\nJournal discourages submission of replication studies—or says nothing.\nJournal encourages submission of replication studies.\nJournal encourages submission of replication studies and conducts blind review of results.\nJournal uses Registered Reports as a submission option for replication studies with peer review before observing the study outcomes.\n\n\n\n\n\n\nBesides adhering to the guidelines summarized in Table 2.1, authors should also:\n\nJustify their choice of methodology: The paper should clearly explain why the chosen method is appropriate for the research question at hand.\nDetail the analytical process: From data collection to analysis, each step should be meticulously detailed, allowing for the study’s replication.\nAddress potential limitations: No empirical study is without its limitations. Authors should openly discuss these, including any biases, measurement errors, or external factors that may impact the results.\n\nWhile statistical significance is a key metric for empirical analysis, the AEA also places a strong emphasis on the economic relevance and implications of the findings. Authors should not only present statistically significant results but also explain their economic significance.\nThe AEA employs a rigorous double-blind peer review process that ensures that submissions are peer-reviewed without revealing the identity of the authors or reviewers. The editor of the respective journal is responsible to manage the process which involves identifying qualified reviewers and forwarding the paper to them for evaluation. This procedure serves to ensure the integrity and impartiality of the evaluation. Authors should be prepared to receive feedback and constructive criticism, which often requires a revision of the manuscript. A willingness to engage constructively with this feedback is crucial to refining the work and improving its chances of publication. Of course, this process has its disadvantages: It is slow and imposes a significant workload on both reviewers and editors. Considering the vast number of publications each year—a number that has significantly increased over the last few decades (see Paldam, 2021) and the fact that the guidelines of many journals have become stricter during the same period, it raises a question. Have researchers become miraculously more productive, or are they simply working more?\n\nPaldam, M. (2021). Methods used in economic research: An empirical study of trends and levels. Economics, 15(1), 28–42.\n\n\n\n\n\n\nTip 2.2: AEA Policies\n\n\n\nRead the AEA Data and Code Policies and Guidance.\nRead the Submission Guidelines of the American Economic Review.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-avoidpredatory",
    "href": "good_research.html#sec-avoidpredatory",
    "title": "2  Good research",
    "section": "2.5 Avoid low-quality sources",
    "text": "2.5 Avoid low-quality sources\nWriting a successful is an art without strict rules. However, you should adapt your writing to the reader’s expectations, using appropriate arguments, graphs, tables, and references. By studying credible sources, you can learn proper practices and conventions. Unfortunately, many authors rely on low-quality work, resulting in a poorly written papers. Thus, consider the quality of the resources you read and cite very carefully. Avoid predatory journals and be cautious of information from online blogs, magazines, and other media without a peer-review process. Aim to find more reliable sources from reputable institutes with a reliable peer-review process and non-profit motivations. Do not simply rely on the first Google search result or the selection provided by Google Scholar, as listing there does not guarantee reliability or quality. Don’t cite articles and books published by vanity presses. Identifying predatory journals can be challenging, but there are some clear indicators of dubious quality. Here is a checklist to help determine if a source is valid:\n\nListing in Beall’s List: Check if the journal or publisher is included in Beall’s List or on TUM’s website.\nOpen-Access status: Determine if the paper or journal is open-access. If it is, consider who is financing the journal. Predatory journals often require authors to pay for publication, and their peer-review process is often questionable.\nPublisher reputation: Some large publishers serve as hubs for conferences and organizations to publish journals and articles. While a few journals from publishers like IEEE, ACM, scirp.org, SciEP, Science of Europe, and MDPI are of acceptable quality due to rigorous editorial standards, most journals are of low quality or predatory. Avoid these publishers or at least approach them with caution and consult your supervisor if in doubt.\nSJR ranking: Is the journal listed in the SCImago Journal Rank and how is their ranking?\nQuestionable impact factor lists: Is the journal listed in dubious impact factor lists? The Scientific Jourrnal Impact Factor (sjifactor.com), for example, claims to determine the impact of a journal. However, the service provider remains anonymous, with no masthead, address or contact information other than an e-mail. This anonymity, which is against the law in many countries, including Europe, casts doubt on the quality and validity of the service, especially as the methodology is not transparent.\n\nBy following these guidelines, you can ensure that the sources you use and cite in your work are of high quality and reliability. However, note that there is no rule without an exception. For example, it can sometimes be appropriate to cite a post from an online blog. However, you should provide a clear reason for doing so. If you are uncertain, consult your supervisor.\n\n\n\n\n\n\nExercise\n\n\n\nFamiliarize yourself with the International Research Journal of Engineering and Technology (IRJET). What do you think? Do you think it is difficult to publish in this journal? Do you think it is a journal with a good reputation? What do you think about the peer review process? Ask your supervisor what they think about the journal. Reflect on these aspects to form an educated opinion about IRJET.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-feynmanon",
    "href": "good_research.html#sec-feynmanon",
    "title": "2  Good research",
    "section": "2.6 Feynman on scientific method",
    "text": "2.6 Feynman on scientific method\n\n\n\nFigure 2.4: Richard P. Feynman (1918 - 1988)\n\n\n\n\n\n\nID badge in Los Alamos\n\n\n\n\n\n\n\nFeynman’s best seller\n\n\n\n\n\n\n\n\n\nFeynman’s bus\n\n\n\n\n\n\n\nThe Big Bang Theory\n\n\n\n\n\n\n\n\nRichard P. Feynman was an American theoretical physicist. At the age of 25, he became a group leader of the Manhattan Project in Los Alamos, received the Nobel Prize in Physics in 1965, authored one of the most famous science books of our time (Surely You’re Joking, Mr. Feynman!) (Feynman, 1985), and remains a hero for many enthusiasts, educators, and nerds (see Figure 2.4). In 1964, more than half a century ago, he gave a good description of scientific method which is still worth considering.\n\nFeynman, R. P. (1985). Surely you’re joking, Mr. Feynman! Adventures of a curious character. W.W. Norton.\nWatch the video Feynman on scientific method:\n\n\n\nFigure 2.5: Feynman on scientific method\n\n\n\nSource: Youtube\n\n\n\nHere is a transcript of his lecture:\n\n“Now, I’m going to discuss how we would look for a new law. In general, we look for a new law by the following process. First, we guess it.\nThen we—well, don’t laugh. That’s really true. Then we compute the consequences of the guess to see if this law that we guessed is right. We check what it would imply and compare those computed results to nature. Or we compare it to experiments or experiences, comparing it directly with observations to see if it works.\nIf it disagrees with experiment, it’s wrong. And that simple statement is the key to science. It doesn’t matter how beautiful your guess is. It doesn’t matter how smart you are, who made the guess, or what his name is; if it disagrees with experiment, it’s wrong. That’s all there is to it.\nIt’s therefore not unscientific to take a guess, although many people outside of science think it is. For instance, I had a conversation about flying saucers a few years ago with laymen.\nBecause I’m a scientist, I said, ‘I don’t think there are flying saucers.’ Then my antagonist said, ‘Is it impossible that there are flying saucers? Can you prove that it’s impossible?’ I said, ‘No, I can’t prove it’s impossible. It’s just very unlikely.’\nThey replied, ‘You are very unscientific. If you can’t prove something is impossible, then how can you say it’s unlikely?’ Well, that’s how science works. It’s scientific to say what’s more likely or less likely, rather than attempting to prove every possibility and impossibility.\nTo clarify, I concluded by saying, ’From my understanding of the world around me, I believe it’s much more likely that the reports of flying saucers are the result of the known irrational characteristics of terrestrial intelligence rather than the unknown rational efforts of extraterrestrial intelligence. It’s just more likely, that’s all. And it’s a good guess. We always try to guess the most likely explanation, keeping in the back of our minds that if it doesn’t work, we must consider other possibilities.\n[…]\nNow, you see, of course, that with this method, we can disprove any specific theory. We can have a definite theory or a real guess from which we can compute consequences that could be compared to experiments, and in principle, we can discard any theory. You can always prove any definite theory wrong. Notice, however, we never prove it right.\nSuppose you invent a good guess, calculate the consequences, and find that every consequence matches the experiments. Does that mean your theory is right? No, it simply has not been proved wrong. Because in the future, there could be a wider range of experiments, and you may compute a broader range of consequences that could reveal that your theory is actually incorrect.\nThat’s why laws, like Newton’s laws of motion for planets, have lasted for such a long time. He guessed the law of gravitation, calculated various consequences for the solar system, and it took several hundred years before the slight error in the motion of Mercury was discovered.\n[…]\nI must also point out that you cannot prove a vague theory wrong. If the guess you make is poorly expressed and rather vague, and if the method you use to compute the consequences is also vague—you aren’t sure. You might say, ‘I think everything is due to [INAUDIBLE], and [INAUDIBLE] does this and that,’ more or less. Thus, you can explain how this works. However, that theory is considered ‘good’ because it cannot be proved wrong.\nIf the process for computing the consequences is indefinite, then with a little skill, any experimental result can be made to fit—at least in theory. You’re probably familiar with that in other fields. For example, A hates his mother. The reason is, of course, that she didn’t show him enough love or care when he was a child. However, upon investigation, you may find that she actually loved him very much and everything was fine. Then the explanation changes to say she was overindulgent when he was [INAUDIBLE]. With a vague theory, it’s possible to arrive at either conclusion.\n[APPLAUSE]\nNow, wait. The cure for this is the following: it would be possible to specify ahead of time how much love is insufficient and how much constitutes overindulgence precisely, enabling a legitimate theory against which you can conduct tests. It’s often said that when this is pointed out regarding how much love is involved, you’re dealing with psychological matters, and such things can’t be defined so precisely. Yes, but then you can’t claim to know anything about it. [APPLAUSE]\nNow, I want to concentrate for now on—because I’m a theoretical physicist and more fascinated with this end of the problem—how you make guesses. It is irrelevant where the guess originates. What matters is that it agrees with experiments and is as precise as possible.\nBut, you might say, that’s very simple. We just set up a machine—a great computing machine—with a random wheel that makes a succession of guesses. Each time it guesses hypotheses about how nature should work, it computes the consequences and compares them to a list of experimental results at the other end. In other words, guessing is a task for simpletons.\nActually, it’s quite the opposite, and I will try to explain why. […]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "good_research.html#sec-econway",
    "href": "good_research.html#sec-econway",
    "title": "2  Good research",
    "section": "2.7 How social scientists do research",
    "text": "2.7 How social scientists do research\nEconomics and business administration are a social sciences. When social scientists are trying to …\n\n…change the world, they act as policy advisors.\n…explain the world, they are scientists.\n\nSocial scientists distinguish between two types of statements:\n\nA positive statement attempts to describe the world as it is and can be tested by checking it against facts. In other words, a positive statement deals with assumptions about the state of the world and some conclusions. The validity of the statement is verifiable or testable in principle, no matter how difficult it might be.\nA normative statement claims how the world should be and cannot be tested. Normative statements often contain words such as have to, ought to, must, should, or non-quantifiable adjectives like important, which cannot be objectively measured. Consequently, normative statements cannot be verified or falsified by scientific methods.\n\n\n\n\n\n\n\n\nExercise 2.3 Positive or normative\n\nAn increase in the minimum wage will cause a decrease in employment among the least-skilled.\nHigher federal budget deficits will cause interest rates to increase.\nNobody should be paid less due to their gender, race, age, or religion.\n\n\n\n\n\nThe task of social science is to discover positive statements that align with our observations of the world and enable us to understand how the world operates. This task is substantial and can be broken into three steps:\n\nObservation and measurement: Social scientists observe and measure economic activity, tracking data such as quantities of resources, wages and work hours, prices and quantities of goods and services produced, taxes and government spending, sales and profits, and the volume and price of traded goods.\nModel building\nTesting models\n\nResearch methodologies in social science often blend the inductive and deductive approaches. Inductive reasoning builds theories, while deductive reasoning tests existing ones. It’s a methodical interplay between creation and scrutiny in the realm of research.\nWhen scientists want to act as political advisors, they are often asked to make normative statements. From an ethical point of view, however, it is difficult to make normative statements without including subjective judgments, which can make such statements unscientific. Therefore, scientists should rely on moral criteria to justify their judgments and normative statements.\nAmong these criteria, the Pareto criterion is one of the best known and most widely used. It states that only changes that benefit at least one individual without harming others can be considered improvements. This concept is often used in economics to discuss the distribution of resources, welfare economics and policy making.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Good research</span>"
    ]
  },
  {
    "objectID": "identification.html",
    "href": "identification.html",
    "title": "3  Identification",
    "section": "",
    "text": "3.1 Causal inference\nIn empirical research, identification refers to the process of establishing a clear and logical relationship between a cause and an effect. This involves demonstrating that the cause is responsible for the observed effect, and that there are no other factors that could potentially explain the effect. The goal of identification is to provide strong evidence that a particular factor is indeed the cause of a particular outcome, rather than simply coincidentally happen. In order to identify a cause-and-effect relationship, researchers can use experimental or non-experimental, that is, observational data, or both. Section 6.2 will explain some difficulties researchers must face when they aim to find empirical evidence on causal effects.\nAs Cunningham (2021) explains in his book (see Figure 3.1), establishing causality is very challenging. Causal inference can assist to some extent. It is the process of establishing causal relationships between variables, aiming to determine whether a change in one variable (the cause or independent variable) leads to a change in another variable (the effect or dependent variable). This process goes beyond mere association or correlation and seeks to establish that one event or factor directly influences another. Various methods of causal inference exist, and this section along with the upcoming chapters will discuss these methods. All methods share a common goal: identifying and measuring a relationship without any bias.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification</span>"
    ]
  },
  {
    "objectID": "identification.html#sec-causalinfer",
    "href": "identification.html#sec-causalinfer",
    "title": "3  Identification",
    "section": "",
    "text": "Figure 3.1: Causal Inference: The Mixtape1\n\n1 Source: Cunningham (2021)\nCunningham, S. (2021). Causal inference: The mixtape. Accessed January 30, 2023; Yale University Press. https://mixtape.scunning.com/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification</span>"
    ]
  },
  {
    "objectID": "identification.html#sec-fpci",
    "href": "identification.html#sec-fpci",
    "title": "3  Identification",
    "section": "3.2 The fundamental problem of causal inference",
    "text": "3.2 The fundamental problem of causal inference\nUnfortunately, claiming a causal relationship to be empirically true is often not straightforward. The main reason for this lies in the so-called fundamental problem of causal inference, which is the issue of observing only one of the potential outcomes for each unit in a study. This means we lack the counterfactual outcome, which is the hypothetical outcome that would have occurred if a subject or unit had experienced a different condition or treatment than what actually happened. Thus, the fundamental problem of causal inference is actually a missing data problem.\nFor example, consider my son, who enjoyed throwing plates from the table. He must decide between throwing a plate or not, but he cannot do both simultaneously – an ability only possible in fictional movies like “Everything Everywhere All at Once”. Of course, my son can conduct an experiment by throwing a plate now and later deciding not to throw a plate. After observing both actions, he may claim to have found evidence that throwing a plate causes noise. However, he can never be 100% certain that the noise he heard after throwing the plate was solely caused by his action. It could be a coincidence that something else caused the noise at precisely the same time, like one of his siblings throwing a fork. He merely assumes it was due to his action. To be more certain, he might repeat the experiment hundreds of times. Even then, he can never be 100% sure. It is still not proof in a logical sense because an external factor could theoretically cause the noise. However, this is where statistics come into play: nowing the environment and the setup of his actions, it becomes extremely unlikely that the noise was not caused by his action. Knowing the setup means, we now that hasn’t been any external factor that may have caused a false a causal fallacy. As Scott Cunningham emphasizes, “prior knowledge is required in order to justify any claim about a causal finding”:\n\nCunningham (2021, ch. 1.3): “It is my firm belief, which I will emphasize over and over in this book, that without prior knowledge, estimated causal effects are rarely, if ever, believable. Prior knowledge is required in order to justify any claim of a causal finding. And economic theory also highlights why causal inference is necessarily a thorny task.”\n\nTo illustrate that the fundamental problem of causal inference is actually a missing data problem, let’s consider the fictitious example data presented in Table 3.1. For different individuals, dentoted as \\(i\\), we know whether they were received treatment \\((T=1)\\) or did not receive treatment \\((T=0)\\), as well as whether the outcome was positive \\((Y=1)\\) or negative \\((Y=0)\\). Since we do not observe the counterfactual outcomes, we are unable to determine the individual treatment effect (ITE), which is expressed as \\(Y_i(1)-Y_i(0)\\).\n\n\n\nTable 3.1: Example data to illustrate that the fundamental problem of causal inference\n\n\n\n\n\n\\(i\\)\n\\(T\\)\n\\(Y\\)\n\\(Y_i(1)\\)\n\\(Y_i(0)\\)\n\\(Y_i(1)-Y_i(0)\\)\n\n\n\n\n1\n0\n0\n?\n0\n?\n\n\n2\n1\n1\n1\n?\n?\n\n\n3\n1\n0\n0\n?\n?\n\n\n4\n0\n0\n?\n0\n?\n\n\n5\n0\n1\n?\n1\n?\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.1 Causal inference ch.1\nPlease read chapter 1 (Introduction) of Cunningham (2021) and answer the following questions. The book is freely available here and here you find chapter 1.\n\nWhat are some common misconceptions about causality that the author addresses in chapter 1?\nWhat is the role of randomization in causal inference, as described in the book?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSome common misconceptions about causality that the author addresses in chapter 1 include confusion between correlation and causality, and the belief that observational studies cannot (hardly) establish causality without prior knowledge. He says that human beings “engaging in optimal behavior are the main reason correlations almost never reveal causal relationships, because rarely are human beings acting randomly” which is crucial for identifying causal effects.\nThe role of randomization in causal inference, as described in the book, is that it helps to control for confounding variables and allows for the estimation of causal effects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification</span>"
    ]
  },
  {
    "objectID": "identification.html#rubin-causal-model",
    "href": "identification.html#rubin-causal-model",
    "title": "3  Identification",
    "section": "3.3 Rubin causal model",
    "text": "3.3 Rubin causal model\nIf we are interested in the causal effect of a certain treatment on an outcome, we need to compare the outcome, \\(Y\\), of an individuals, \\(i\\), who received the treatment, \\(1\\), to the outcome, \\(Y\\), of the same individual, \\(i\\), who did not receive the treatment, \\(0\\):\n\\[\nITE_i=Y_i(1)-Y_i(0).\n\\]\nUnfortunately, as discussed in Section 3.2, this individual treatment effect (ITE) does not exist as person \\(i\\) can either be treated or not, but not both simultaneously. Since the counterfactual outcome is missing for each individual, we cannot observe the actual causal effect.\nThe Rubin model, also known as the potential outcomes framework, provides a theoretical framework for identifying causality in the context of missing data-existing.\nIn the model, each subject, denoted with \\(i\\) (for example, a person, a school), has two potential outcomes: one outcome if the subject receives the treatment (treatment condition denoted with \\(T=1\\)) and another outcome if the does not receive the treatment (control condition denoted with \\(T=0\\)). In short, the model specifies that you can use the difference of the average of a group that received the treatment and the average of the group that did not received the treatment and use it as a substitute for the ITE: \\[\n\\mathbb{E} [\\underbrace{Y_i(1)-Y_i(0)}_{ITE}] = \\underbrace{\\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]}_{ATE}.\n\\tag{3.1}\\]\nHowever, the ATE is only equal to the expected ITE if certain assumptions are fulfilled. The upcoming sections will discuss these assumptions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification</span>"
    ]
  },
  {
    "objectID": "identification.html#sec-overfunda",
    "href": "identification.html#sec-overfunda",
    "title": "3  Identification",
    "section": "3.4 Its difficult to overcome the fundamental problem",
    "text": "3.4 Its difficult to overcome the fundamental problem\n\nKeele (2015, p. 314): “An identification analysis identifies the assumptions needed for statistical estimates to be given a causal interpretation.”\n\nKeele, L. (2015). The statistics of causal inference: A view from political methodology. Political Analysis, 23(3), 313–335.\n\nIn the following we will discuss conditions that need to hold in order to empirically draw causal conclusions from the ATE without bias. This is important because equation Equation 3.1 does not necessarily hold when using observational data without a more elaborated identification strategy.\n\n3.4.1 Example\nSuppose we want to measure the effect of a vaccine on survival rates. We observed the residents of a small city with 2,000 inhabitants over the course of 30 days. On day 1, we arrived in town and injected the vaccine to 200 individuals. By day 30, we counted the deceased in both groups: four died in the vaccinated group, while eighteen died in the group of 1,800 unvaccinated individuals. With a survival rate of 98% in the vaccinated group and 99% in the unvaccinated group, it may appear that the vaccine lowers the survival rate. Imagine that study is real, would you claim that the vaccine kills because according to Equation 3.1 we could use the ATE to indicate the ITE?\nThe answer is yes, but only if the assumptions of ignorability (Section 3.4.2) and unconfoundedness (Section 3.4.3) are satisfied.\nIn brief, ignorability means that the 200 treated individuals are not systematically different from the other 1,800 individuals regarding characteristics that have an impact of the chances to survive. Considering the fact that we cannot randomly select 200 individuals from the 2,000 inhabitants due to legal constraints (as everyone has the right to choose whether or not to receive the vaccine), we must consider who is willing to get vaccinated. This selection bias may pose issues, as vulnerable populations often have a higher willingness to accept the vaccine compared to younger and healthier individuals who may fear the disease less. For example, if we vaccinated individuals with preexisting conditions that make them more vulnerable, such as the elderly or those with chronic illnesses, we cannot assume that the ATE is equal to the ITE. This is because the overall mortality risk is higher among those who received the vaccine.\nUnconfoundedness means that there are no other factors that could explain both the likelihood of receiving the vaccine and the likelihood of death. For example, if vaccinated individuals were not required to stay at home during these 30 days, their likelihood of dying may increase due to greater exposure to risky situations and other people, which in turn raises their chances of contracting a disease.\n\n\n\n\n\n\nTip 3.1\n\n\n\n\n\n\nFigure 3.2: Average treatment effect (ATE)\n\n\n\n\n\n\nWatch the video of Brady Neal’s lecture What Does Imply Causation? Randomized Control Trials (see Figure 3.2). Alternatively, you can read Neal (2020, ch. 2) of his lecture notes, see here.\n\n\n\n\n3.4.2 Ignorability\nReferring to table Table 3.1, Brady Neal (2020) wrote:\n\n“what makes it valid to calculate the ATE by taking the average of the Y(0) column, ignoring the question marks, and subtracting that from the average of the Y(1) column, ignoring the question marks?” This ignoring of the question marks (missing data) is known as ignorability. Assuming ignorability is like ignoring how people ended up selecting the treatment they selected and just assuming they were randomly assigned their treatment” (Neal, 2020, p. 9)\n\nIgnorability means that the way individuals are assigned to treatment and control groups is irrelevant for the data analysis. Thus, when we aim to explain a certain outcome, we can ignore how an individual made it into the treated or control group. It has also been called unconfoundedness or no omitted variable bias. We will come back to these two terms in Section 7.4 and in Chapter 7.\nRandomized controlled trials (RCTs) are characterized by randomly assigning individuals to different treatment groups and comparing the outcomes of those groups. Thus, RCTs are essentially build on the assumption of ignorability which can be written formally like \\[\n(Y(1), Y(0)) \\perp T.\n\\]\nThis notation indicates that the potential outcomes of an individual, \\(Y\\), are independent of whether they have actually received the treatment. The symbol “\\(\\perp\\)” denotes independence, suggesting that the outcomes \\(Y(1)\\) and \\(Y(0)\\) are orthogonal to the treatment \\(T\\).\nThe assumption of ignorability allows to write the ATE as follows: \\[\\begin{align}\n\\mathbb{E}[Y(1)]-\\mathbb{E}[Y(0)] & =\\mathbb{E}[Y(1) \\mid T=1]-\\mathbb{E}[Y(0) \\mid T=0] \\\\\n& =\\mathbb{E}[Y \\mid T=1]-\\mathbb{E}[Y \\mid T=0].\n\\end{align}\\]\nAnother perspective on this assumption is the concept of exchangeability. Exchangeability refers to the idea that the treatment groups can be interchanged such that if they were switched, the new treatment group would have the same outcomes as the old treatment group, and the new control group would have the same outcomes as the old control group.\n\n\n3.4.3 Unconfoundedness\nWhile randomized controlled trials (RCTs) assume the concept of ignoreability, most observational data present challenges in drawing causal conclusions due to the presence of confounding factors that affect both (1) the likelihood of individuals being part of the treatment group and (2) the observed outcome. For example, regional factors can affect both the number of storks and the number of babies born in a region. These factors are typically referred to as confounders, which we discussed in Section 6.2 as having the potential to create the illusion of a causal impact where none exists. However, empirical methods are available to control for these confounders and prevent the violation of the ignoreability assumption. Formally, the assumption can be written as \\[\n(Y(1), Y(0)) \\perp T \\mid X.\n\\] This allows to write the ATE as follows: \\[\\begin{align}\n\\mathbb{E}[Y(1)\\mid X]-\\mathbb{E}[Y(0)\\mid X] & =\\mathbb{E}[Y(1) \\mid T=1, X]-\\mathbb{E}[Y(0) \\mid T=0, X] \\\\\n& =\\mathbb{E}[Y \\mid T=1, X]-\\mathbb{E}[Y \\mid T=0, X].\n\\end{align}\\]\nThis means that we need to control for all factors (X) that influence both groups. We will revisit this topic in Section 7.4, where we will discuss the various functional impacts that must be considered to avoid causal bias.\n\n\n\n\n\n\n\nExercise 3.2 Treatment effects\nRead sections 2.1 and 2.3 of Neal (2020).\n\nWhat is the individual treatment effect (ITE)?\nWhat is the average treatment effect (ATE)?\nHow is the ATE calculated?\nCan the ATE be used to determine the effect of a treatment on an individual level?\nWhat are some potential sources of bias when estimating the ATE?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe individual treatment effect (ITE) is a measure of the effect of a treatment or intervention on an individual level. It represents the difference in the outcome for an individual who receives the treatment versus the outcome for that same individual if they had not received the treatment.\nThe average treatment effect (ATE) is a measure of the difference in the expected outcomes between a treatment group and a control group. It represents the overall effect of a treatment on the population as a whole.\nThe ATE is calculated by taking the difference between the average outcome for the treatment group and the average outcome for the control group.\nNo, the ATE is a population-level measure and cannot be used to determine the effect of a treatment on an individual level. To determine the effect of a treatment on an individual level, you would need to use techniques such as propensity score matching or instrumental variables.\nSome potential sources of bias when estimating the ATE include selection bias, measurement bias, and unobserved confounding variables. To mitigate these biases, researchers may use randomization or other advanced statistical techniques such as propensity score matching or instrumental variables to control for these potential sources of bias.\n\n\n\n\n\n\n\n\n\n\n\nNeal, B. (2020). Introduction to causal inference from a machine learning perspective: Course lecture notes. Accessed January 30, 2023. https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Identification</span>"
    ]
  },
  {
    "objectID": "getdata.html",
    "href": "getdata.html",
    "title": "4  Data acquisition",
    "section": "",
    "text": "4.1 Interviews\nThere are several ways to get data which allows you to (hopefully) identify a cause-and-effect relationship:\nAn interview is normally a one-on-one verbal conversation. Interviews are conducted to learn about the participants’ experiences, perceptions, opinions, or motivations. The relationship between the interviewer and interviewee must be taken into account and other circumstances (place, time, face to face, email, etc.) should be taken into account. There are three types of interviews structured, semi-structured, and unstructured. Structured interviews use a set list of questions and hence are like a verbal surveys. In unstructured interviews the interviewer doesn’t use predetermined questions but only a list of topics to address. Semi-structured interviews are the middle ground. Semi-structured interviews require the interviewer to have a list of questions and topics pre-prepared, which can be asked in different ways with different interviewee/s. Semi-structured interviews increase the flexibility and the responsiveness of the interview while keeping the interview on track, increasing the reliability and credibility of the data. Semi-structured interviews are one of the most common interview techniques.\nStructured interviews use a predetermined list of questions that must be asked in a specific order, improving the validity and trustworthiness of the data but lowering respondent response. Structured interviews resemble verbal questionnaires. In unstructured interviews, the interviewer has a planned list of subjects to cover but no predetermined interview questions. In exchange for less reliable data, this makes the interview more adaptable. Long-term field observation studies may employ unstructured interviews. The middle ground are interviews that are semi-structured. In semi-structured interviews, the interviewer must prepare a list of questions and themes that can be brought up in various ways with various interviewees.\nInterviews allow you to address a cause-and-effect relationship fairly directly, and it can be a good idea to interview experts and ask some why and how questions to gather initial knowledge about a particular topic before further elaborating your research strategy. For example, I interviewed kindergarten teachers with many years of experience working with children, as well as other parents, to get information on how to solve the problem of my children throwing plates around the dining room. However, findings based on interviews are not very valid or reliable because the personal perceptions of both the interviewer and the interviewee can have an impact on the conclusions drawn. For example, I received very different tips and explanations because of the personal experiences of the people I interviewed. Unfortunately, I could not really ask my son why he was misbehaving. His vocabulary was too limited at the time, and even if he could speak, he would probably refuse to tell me the truth.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "getdata.html#surveys",
    "href": "getdata.html#surveys",
    "title": "4  Data acquisition",
    "section": "4.2 Surveys",
    "text": "4.2 Surveys\nIn contrast to an interview a survey can be sent out to many different people. Surveys can be used to identify a cause-and-effect relationship by asking questions about both the cause and the effect and examining the responses. For example, if a researcher wanted to determine whether there is a relationship between a person’s level of education and their income, they could conduct a survey asking participants about their education level and their income. If the data shows that participants with higher levels of education tend to have higher incomes, it suggests that education may be a cause of higher income. However, it is important to note that surveys can only establish a correlation between variables, but it is difficult to claim that correlations that where found through the survey imply a causal relationship. To establish a causal relationship, a researcher would need to use other methods, such as an experiment, to control for other potential factors that might influence the relationship that the respondent does not see.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "getdata.html#case-studies",
    "href": "getdata.html#case-studies",
    "title": "4  Data acquisition",
    "section": "4.3 Case studies",
    "text": "4.3 Case studies\nCase studies involve in-depth examination of a single case or a small number of cases in order to understand a particular phenomenon. Case studies can be conducted using both quantitative and qualitative methods, depending on the research question and the data being analyzed. While it is reasonable to find causal effects in the particular case, it is problematic to generalize the causal relationship. To establish a general causal relationship, a researcher would need to use other methods, such as an experiment, to control for other potential factors that might influence the relationship that the respondent does not see.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "getdata.html#sec-experimentsrct",
    "href": "getdata.html#sec-experimentsrct",
    "title": "4  Data acquisition",
    "section": "4.4 Experiments",
    "text": "4.4 Experiments\nOne way to clearly identify a cause-and-effect relationship is through experiments, which involve manipulating the cause (the independent variable) and measuring the effect (the dependent variable) under controlled conditions (we will later on define precisely what is meant here). Experiments can be conducted using both quantitative and qualitative methods. Here are some examples:\n\nA medical study in which a new drug is tested on a group of patients, while a control group receives a placebo.\nAn educational study in which a group of students is taught a new method of learning, while a control group is taught using the traditional method.\nAn agricultural study in which a group of crops is treated with a new fertilization method, while a control group is not treated.\nA study to determine the effect of a new training program on employee productivity might involve randomly assigning employees to either a control group that does not receive the training, or an experimental group that does receive the training. By comparing the productivity of the two groups, the researchers can determine if the new training program had a causal effect on employee productivity.\nA study to determine the effect of a new advertising campaign on sales might involve randomly assigning different groups of customers to be exposed to different versions of the campaign. By comparing the sales of the different groups, the researchers can determine if the advertising campaign had a causal effect on sales.\nIn experimental economics, experimental methods are used to study economic questions. In a lab-like environment data are collected to investigate the size of certain effects, to test the validity of economic theories, to illuminate market mechanisms, or to examine the decision making of people. Economic experiments usually motivates and rewards subjects with money. The overall goal is to mimic real-world incentives and investigate things that cannot be captured or identified in the field.\nIn behavioral economics, laboratory experiments are also used to study decisions of individuals or institutions and to test economic theory. However, it is done with a focus on cognitive, psychological, emotional, cultural, and social factors.\n\n\n\n\nFigure 4.1: Daniel Kahneman and his best selling book1\n\n1 Source: https://commons.wikimedia.org/wiki/File:Daniel_Kahneman_(3283955327)_(cropped).jpg\n\n\n\n\nDaniel Kahneman\n\n\n\n\n\n\n\nThinking, Fast and Slow\n\n\n\n\n\n\n\n\nIn 2002 the Nobel Prize of Economics was awarded to Vernon L. Smith, I quote The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel (2002), “for having established laboratory experiments as a tool in empirical economic analysis, especially in the study of alternative market mechanisms” and Daniel Kahneman “for having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty”.\n\nThe Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel. (2002). Nobel prize outreach AB 2024. Fri. 15 nov 2024. https://www.nobelprize.org/prizes/economic-sciences/2002/summary/\nThe strength of evidence from a controlled experiment is generally considered to be strong. However, the external validity, i.e., the generalizability, should be considered as well. External validity is sometimes low because effects that you can identify and measure in a lab are sometimes only of minor importance in the field.\nThere are different types of experiments:\nRandomized controlled trials (RCTs) are a specific type of an experiment that involve randomly assigning participants to different treatment groups and comparing the outcomes of those groups. RCTs are often considered the gold standard of experimental research because they provide a high degree of control over extraneous variables and are less prone to bias.\nFor a better explanation and some great insights into what an RCT actually is, please watch the video produced by UNICEFInnocenti and published on the YouTube channel of UNICEF’s dedicated research center, see https://youtu.be/Wy7qpJeozec and Figure 4.2.\n\n\n\nFigure 4.2: Randomized Controlled Trials (RCTs)2\n\n2 Source: https://youtu.be/Wy7qpJeozec\n\n\n\n\nQuasi-experiments involve the manipulation of an independent variable, but do not involve random assignment of participants to treatment groups. Quasi-experiments are less controlled than RCTs, but can still provide valuable insights into cause-and-effect relationships.\nNatural experiments involve the observation of naturally occurring events or situations that provide an opportunity to study cause-and-effect relationships. Natural experiments are often used when it is not possible or ethical to manipulate variables experimentally.\nIn a laboratory experiment, researchers manipulate an independent variable and measure the effect on a dependent variable in a controlled laboratory setting. This allows for greater control over extraneous variables, but the results may not generalize to real-world situations.\nIn a field experiment, researchers manipulate an independent variable and measure the effect on a dependent variable in a natural setting, rather than in a laboratory. This allows researchers to study real-world phenomena, but it can be more difficult to control for extraneous variables.\nChapter 8 will elaborate more on experiments that take place outside of a lab.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "getdata.html#observational-data",
    "href": "getdata.html#observational-data",
    "title": "4  Data acquisition",
    "section": "4.5 Observational data",
    "text": "4.5 Observational data\n\n\n\nFigure 4.3: Observational data3\n\n3 Source: https://pixabay.com/images/id-5029286/\n\n\n\n\nObservational data are data that had been observed before the research question was asked or being collected independently from the study. To understand how observational data can be used to constitute a causal relationship is a bit tricky because there is only one world and only one reality at a time. In other words, we usually miss a counterfactual which we can use for a comparison. Take, for example, the past COVID-19 pandemic, where you chose to be vaccinated or not. Regardless of what you chose, we will never find out what would have happened to you if you had chosen differently. Maybe you would have died, maybe you would have gotten more or less sick, or maybe you wouldn’t have gotten sick at all. We don’t know, and it’s impossible to find out because it’s impossible to observe the counterfactual outcomes. This makes it difficult to establish causality from observational data. However, ingenious minds have found reasonable procedures and methods to extract some level of knowledge from observational data that allows us to infer causal relationships from observational data where we cannot directly observe the counterfactual outcome. We will come back to these methods later on.\nIn the upcoming sections, however, we will discuss experimental research designs including randomized controlled trials (RCTs) which are considered to be the “gold standard for measuring the effect of an action” (Taddy, 2019, p. 128). RCTs can be used, for example, to study the effectiveness of drugs by observing people randomly assigned to three groups, one taking the pill (or treatment), a second receiving a placebo, and a third taking nothing. If the first group responds in any way differently than the other groups, the drug has an effect. Before explaining an RCT in more detail, we need to be clear about the fundamental problem of causal inference. This will be discussed in the following.\n\nTaddy, M. (2019). Business data science: Combining machine learning and economics to optimize, automate, and accelerate business decisions (1st ed.). McGraw Hill Education.\n\n\n\n\n\n\n\nExercise 4.1 Methods used in economic research\nRead Paldam (2021) which is freely available here and answer the following questions:\n\nList the eight types of research methods described in the paper and provide the description found in the paper.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nEconomic theory: Papers are where the main content is the development of a theoretical model. The ideal theory paper presents a (simple) new model that recasts the way we look at something important.\nStatistical technique, incl. forecasting Papers reporting new estimators and tests are published in a handful of specialized journals in econometrics and mathematical statistics. Some papers compare estimators on actual data sets. If the demonstration of a methodological improvement is the main feature of the paper, it belongs to this subgroup, but if the economic interpretation is the main point of the paper, it belongs to the classical empirical studies or newer techniques group.\nSurveys, incl. meta-studies When the literature in a certain field becomes substantial, it normally presents a motley picture with an amazing variation, especially when different schools exist in the field. They are of two types, where the second type is still rare:\n\nAssessed surveys where the author reads the papers and assesses what the most reliable results are. Such assessments require judgment that is often quite difficult to distinguish from priors, even for the author of the survey.\nMeta-studies which are quantitative surveys of estimates of parameters claimed to be the same. These types of studies have two levels: The basic level collects and codes the estimates and studies their distribution. This is a rather objective exercise where results seem to replicate rather well. The second level analyzes the variation between the results. This is less objective.\n\nExperiments in laboratories Most of these experiments take place in a laboratory, where the subjects communicate with a computer, giving a controlled, but artificial, environment. A number of subjects are told a (more or less abstract) story and paid to react in either of a number of possible ways. A great deal of ingenuity has gone into the construction of such experiments and in the methods used to analyze the results. Lab experiments do allow studies of behavior that are hard to analyze in any other way, and they frequently show sides of human behavior that are difficult to rationalize by economic theory. However, everything is artificial – even the payment while participants usually receive real money for participation and their performance. In some cases, the stories told are so elaborate and abstract that framing must be a substantial risk. In addition, experiments cost money, which limits the number of subjects. It is also worth pointing to the difference between expressive and real behavior. It is typically much cheaper for the subject to `express’ nice behavior in a lab than to be nice in the real world.\nEvent studies (field experiments and natural experiments) Event studies are studies of real world experiments. They are of two types:\n\nField experiments analyze cases where some people get a certain treatment and others do not. The `gold standard’ for such experiments is double blind random sampling, where everything (but the result!) is announced in advance. Experiments with humans require permission from the relevant authorities, and the experiment takes time too. In the process, things may happen that compromise the strict rules of the standard. Controlled experiments are expensive, as they require a team of researchers.\nNatural experiments take advantage of a discontinuity in the environment, i.e., the period before and after an (unpredicted) change of a law, an earth-quake, etc. Methods have been developed to find the effect of the discontinuity. Often, such studies look like classical empirical studies with many controls that may that may or may not belong. Thus, the problems discussed under the classic empirical studies also apply here.\n\nDescriptive, deductions from data In a descriptive study, researcher use an existing sample and hence, they have no control over the data generating process as it is usually the case with experiments. Descriptive studies are deductive. The researcher describes the data aiming at finding structures that tell a story, which can be interpreted. The findings may call for a formal test. If one clean test follows from the description, the paper can still be classified as a descriptive study. If more elaborate regression analysis is used, however, it can also be classified as a classical empirical study. Descriptive studies often contain a great deal of theory. Some descriptive studies present a new data set developed by the author to analyze a debated issue. In these cases, it is often possible to make a clean test, so to the extent that biases sneak in, they are hidden in the details of the assessments made when the data are compiled.\nClassical empirical studies Typically have three steps: It starts by a theory, which is developed into an operational model. Then it presents the data set, and finally it runs regressions. The significance levels of the t-ratios on the coefficient estimated assume that the regression is the first meeting of the estimation model and the data. In practice, we all know that this is rarely the case. The classical method is often just a presentation technique. The great virtue of the method is that it can be applied to real problems outside academia. The relevance comes with a price: The method is quite flexible as many choices have to be made, and they often give different results. Preferences and interests, may affect these choices.\nNewer techniques Partly as a reaction to the problems of classical empirical methods, the last 3–4 decades have seen a whole set of newer empirical techniques. They include different types of vector autoregression (VAR) (A VAR is a statistical time series model used to capture the relationship between multiple quantities as they change over time.), Bayesian techniques, causality and co-integration tests, Kalman filters, hazard functions, etc. The main reason for the lack of success for the new empirics is that it is quite bulky to report a careful set of co-integration tests or VARs, for example, and they often show results that are far from useful in the sense that they are unclear and difficult to interpret.\n\n\n\n\n\nRead the following statements and discuss whether they are true or not, and if the latter, correct them:\n\nThe annual production of research papers in economics in the year 2017 has reached about 100 papers in top journals, and about 1,400 papers in the group of good journals. The production has grown with 3.3% per year, and thus it has doubled the last twenty years.\nThe upward trend in publication must be due to the large increase in the importance of publications for the careers of researchers, which has greatly increased the production of papers. There has also been a large increase in the number of researches, but as citations are increasingly skewed toward the top journals it has not increased demand for papers correspondingly.\nFour trends are significant: The fall in theoretical papers and the rise in classical papers. There is also a rise in the share of statistical method and event studies. It is surprising that there is no trend in the number of experimental studies.\nBook reviews have dropped to less than 1/3. Perhaps, it also indicates that economists read fewer books than they used to. Journals have increasingly come to use smaller fonts and larger pages, allowing more words per page. The journals from North-Holland Elsevier have managed to cram almost two old pages into one new one. This makes it easier to publish papers, while they become harder to read.\nAbout 50% of papers in the sample considered in belong to the economic theory class, about 6% are experimental studies, and about 43% are empirical studies based on data inference.\nThe papers in economic theory have increased from 33.6% to 59.5% – this is the largest change for any of the eight subgroups. It is highly significant in the trend test.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStatements i) and vi) are false, all others are correct.\n\nThe numbers are wrong: The annual production of research papers in economics in the year 2017 has now reached about 1,000 papers in top journals, and about 14,000 papers in the group of good journals. The production has grown with 3.3% per year, and thus it has doubled the last twenty years.\nStatement is correct: The upward trend in publication must be due to the large increase in the importance of publications for the careers of researchers, which has greatly increased the production of papers. There has also been a large increase in the number of researches, but as citations are increasingly skewed toward the top journals it has not increased demand for papers correspondingly.\nStatement is correct: Four trends are significant: The fall in theoretical papers and the rise in classical papers. There is also a rise in the share of statistical method and event studies. It is surprising that there is no trend in the number of experimental studies.\nStatement is correct: Book reviews have dropped to less than 1/3. Perhaps, it also indicates that economists read fewer books than they used to. Journals have increasingly come to use smaller fonts and larger pages, allowing more words per page. The journals from North-Holland Elsevier have managed to cram almost two old pages into one new one. This makes it easier to publish papers, while they become harder to read.\nStatement is correct: About 50% of papers in the sample considered in belong to the economic theory class, about 6% are experimental studies, and about 43% are empirical studies based on data inference.\nEconomic theory is not on the rise: The papers in economic theory have dropped from 59.5% to 33.6% – this is the largest change for any of the eight subgroups. It is highly significant in the trend test.\n\n\n\n\n\nExplain what is meant with theory fatigue and discuss the reasons that lead to that fatigue.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n“Theory fatigue” is a term used to describe the decreasing attractiveness of theoretical research among journals, researchers and political decision-makers. This trend goes hand in hand with the increasing importance of empirical research. Policy makers are finding it increasingly difficult to engage with variations of existing theoretical models, and researchers often struggle to systematically summarize the findings of theoretical work, making it difficult to draw definitive conclusions on specific topics. In addition, theoretical work can be unconvincing to a wider audience that must rely on the reasonableness of complex and sometimes unrealistic assumptions. The credibility of theoretical research often depends on how realistic the initial assumptions are and how plausible the conclusions are. If neither aspect is grounded in reality, there is a danger that the research becomes an abstract exercise that provides new insights into the real world, but which are difficult to communicate to the layperson.\n\n\n\n\nAccording to Paldam (2021): What factors contribute to the immediate relevance of research papers for policymakers?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA research paper that policymakers find appealing typically offers estimates of a crucial effect that decision-makers outside of academia are keen to understand. Papers that target policymakers should put an emphasis on distilling the core findings into a short executive summary tailored for decision-makers, facilitating their understanding and application of the research insights.\n\n\n\n\n\n\n\n\nPaldam, M. (2021). Methods used in economic research: An empirical study of trends and levels. Economics, 15(1), 28–42.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "getdata.html#sampling",
    "href": "getdata.html#sampling",
    "title": "4  Data acquisition",
    "section": "4.6 Sampling",
    "text": "4.6 Sampling\n\n4.6.1 The Hite Report\nIn 1976, when the The Hite Report (see Hite, 1976) was published it instantly became a best seller. Hite used an individualistic research method. Thousands of responses from anonymous questionnaires were used as a framework to develop a discourse on human responses to gender and sexuality. The following comic concludes the main results.\n\n\n\nFigure 4.4: The Hite (1976) Report\n\n\n\n\n\n\n\n\n\nFigure 4.5: Comic on the Hite Report4\n\n4 Picture is taken from https://www.theparisreview.org/blog/2017/07/21/great-moments-literacy-hite-report.\n\n\n\n\nThe picture of womens’ sexuality in Hite (1976) was probably a bit biased as the sample can hardly be considered to be a random and unbiased one:\n\nHite, S. (1976). The hite report. A nationwide study of female sexuality. New York: Dell.\n\nLess than 5% of all questionnaires which were sent out were filled out and returned (response bias).\nThe questions were only sent out to women’s organizations (an opportunity sample).\n\nThus, the results were based on a sample of women who were highly motivated to answer survey’s questions, for whatever reason.\n\n\n4.6.2 Sample design\nIn statistics and quantitative research methodology, a sample is a group of individuals or objects that are collected or selected from a statistical population using a defined procedure. The elements of a sample are called sample points, sampling units, or observations.\nUsually, the population is very large, and therefore, conducting a census or complete enumeration of all individuals in the population is either impractical or impossible. Therefore, a sample is taken to represent a manageable subset of the population. Data is collected from the sample, and statistics are calculated to make inferences or extrapolations from the sample to the population.\nIn statistics, we often rely on a sample, that is, a small subset of a larger set of data, to draw inferences about the larger set. The larger set is known as the population from which the sample is drawn.\nResearchers adopt a variety of sampling strategies. The most straightforward is simple random sampling. Such sampling requires every member of the population to have an equal chance of being selected into the sample. In addition, the selection of one member must be independent of the selection of every other member. That is, picking one member from the population must not increase or decrease the probability of picking any other member (relative to the others). In this sense, we can say that simple random sampling chooses a sample by pure chance. To check your understanding of simple random sampling, consider the following example. What is the population? What is the sample? Was the sample picked by simple random sampling? Is it biased?\n\n4.6.2.1 Random sampling\nRandom sampling is a sampling procedure by which each member of a population has an equal chance of being included in the sample. Random sampling ensures a representative sample. There are several types of random sampling. In simple random sampling, not only each item in the population but each sample has an equal probability of being picked. In systematic sampling, items are selected from the population at uniform intervals of time, order, or space (as in picking every one-hundredth name from a telephone directory). Systematic sampling can be biased easily, such as, for example, when the amount of household garbage is measured on Mondays (which includes the weekend garbage). In stratified and cluster sampling, the population is divided into strata (such as age groups) and clusters (such as blocks of a city) and then a proportionate number of elements is picked at random from each stratum and cluster. Stratified sampling is used when the variations within each stratum are small in relation to the variations between strata. Cluster sampling is used when the opposite is the case. In what follows, we assume simple random sampling. Sampling can be from a finite population (as in picking cards from a deck without replacement) or from an infinite population (as in picking parts produced by a continuous process or cards from a deck with replacement).\nIn statistics, a simple random sample is a subset of individuals (a sample) chosen from a larger set (a population). Each individual is chosen randomly and entirely by chance, such that each individual has the same probability of being chosen at any stage during the sampling process, and each subset of k individuals has the same probability of being chosen for the sample as any other subset of k individuals.\nThe simple random sample has two important properties:\n\nUNBIASED: Each unit has the same chance of being chosen.\nINDEPENDENCE: Selection of one unit has no influence on the selection of other units.\n\n\n\n\n\n\n\n\nExercise 4.2 Random sampling\n\nWhat is meant by random sampling (simple random sample)?\nWhat is its importance?\nWhy is having a large sample always better than having a small(er) one?\n\n\n\n\n\n\n\n\n\n\n\nSolution: Random sampling\n\n\n\n\n\nRandom sampling is a sampling procedure by which each member of a population has an equal chance of being included in the sample. Random sampling ensures a representative sample. There are several types of random sampling. In simple random sampling, not only each item in the population but each sample has an equal probability of being picked. In systematic sampling, items are selected from the population at uniform intervals of time, order, or space (as in picking every one-hundredth name from a telephone directory). Systematic sampling can be biased easily, such as, for example, when the amount of household garbage is measured on Mondays (which includes the weekend garbage). In stratified and cluster sampling, the population is divided into strata (such as age groups) and clusters (such as blocks of a city) and then a proportionate number of elements is picked at random from each stratum and cluster. Stratified sampling is used when the variations within each stratum are small in relation to the variations between strata. Cluster sampling is used when the opposite is the case. In what follows, we assume simple random sampling. Sampling can be from a finite population (as in picking cards from a deck without replacement) or from an infinite population (as in picking parts produced by a continuous process or cards from a deck with replacement). The larger the sample gets, the closer we get to the population and hence, we reduce the bias of having a non-randomly selected sample.\n\n\n\n\n\n4.6.2.2 Other sampling methods\nSystematic sampling\nSystematic sampling (a.k.a. interval sampling) relies on arranging the study population according to some ordering scheme and then selecting elements at regular intervals through that ordered list. Systematic sampling involves a random start and then proceeds with the selection of every k\\(^{th}\\) element from then onwards.\nAccidental sampling / opportunity sampling / convenience sampling\nThese sampling methods describe a type of nonprobability sampling which involves the sample being drawn from that part of the population which is close to hand. That is, a population is selected because it is readily available and convenient.\nStratified sampling\nSince simple random sampling often does not ensure a representative sample, a sampling method called stratified random sampling is sometimes used to make the sample more representative of the population. This method can be used if the population has a number of distinct groups. In stratified sampling, you first identify members of your sample who belong to each group. Then you randomly sample from each of those subgroups in such a way that the sizes of the subgroups in the sample are proportional to their sizes in the population. Let`s take an example: Suppose you were interested in views of capital punishment at an urban university. You have the time and resources to interview 200 students. The student body is diverse with respect to age; many older people work during the day and enroll in night courses (average age is 39), while younger students generally enroll in day classes (average age of 19). It is possible that night students have different views about capital punishment than day students. If 70% of the students were day students, it makes sense to ensure that 70% of the sample consisted of day students. Thus, your sample of 200 students would consist of 140 day students and 60 night students. The proportion of day students in the sample and in the population (the entire university) would be the same. Inferences to the entire population of students at the university would therefore be more secure.\nCluster sampling\nSometimes it is more cost-effective to select respondents in groups (clusters) of similar respondents. Sampling is often clustered by geography, or by time periods.\n\n\n4.6.2.3 Random assignment\nIn experimental research, populations are often hypothetical. For example, in an experiment comparing the effectiveness of a new anti-depressant drug with a placebo, there is no actual population of individuals taking the drug. In this case, a specified population of people with some degree of depression is defined and a random sample is taken from this population. The sample is then randomly divided into two groups; one group is assigned to the treatment condition (drug) and the other group is assigned to the control condition (placebo). This random division of the sample into two groups is called random assignment. Random assignment is critical for the validity of an experiment. For example, consider the bias that could be introduced if the first 20 subjects to show up at the experiment were assigned to the experimental group and the second 20 subjects were assigned to the control group. It is possible that subjects who show up late tend to be more depressed than those who show up early, thus making the experimental group less depressed than the control group even before the treatment was administered. In experimental research of this kind, failure to assign subjects randomly to groups is generally more serious than having a non-random sample. Failure to randomize (the former error) invalidates the experimental findings. A non-random sample (the latter error) simply restricts the generalizability of the results.\n\n\n\n4.6.3 Sample size\nThe sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is usually determined based on the cost, time, or convenience of collecting the data, and the need for it to offer sufficient statistical power.\nRecall that the definition of a random sample is a sample in which every member of the population has an equal chance of being selected. This means that the sampling procedure rather than the results of the procedure define what it means for a sample to be random. Random samples, especially if the sample size is small, are not necessarily representative of the entire population.\nLarger sample sizes generally lead to increased precision when estimating unknown parameters. For example, if we wish to know the proportion of a certain species of fish that is infected with a pathogen, we would generally have a more precise estimate of this proportion if we sampled and examined 200 rather than 100 fish. Several fundamental facts of mathematical statistics describe this phenomenon, including the law of large numbers and the central limit theorem.\n\n\n\n\n\n\nTip 4.1\n\n\n\nThe quality of data matters\nA helpful slogan to keep in mind while scrutinizing statistical results is garbage in, garbage out. Regardless of how scientifically sound and visually appealing a statistic may appear, the formula used to derive it is oblivious to the quality of the data that underpins it. It is your responsibility to conduct a thorough examination. For example, if the data on which the statistic is based emanates from a biased sample (one that favors certain individuals over others), a flawed design, unreliable data-collection protocols, or misleading questions, the margin of error becomes bad. If the bias is sufficiently severe, the outcomes become worthless.\n\n\n\n\n4.6.4 Sample errors\nRead the following examples5:\n5 The examples are taken from Lane (2023) and can be accessed here.\nLane, D. M. (2023). Introduction to statistics: Online statistics education: A multimedia course of study. Accessed January 30, 2023; Online Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com\nExample 1: You have been hired by the National Election Commission to examine how the American people feel about the fairness of the voting procedures in the U.S. Who will you ask?\nIt is not practical to ask every single American how he or she feels about the fairness of the voting procedures. Instead, we query a relatively small number of Americans, and draw inferences about the entire country from their responses. The Americans actually queried constitute our sample of the larger population of all Americans. The mathematical procedures whereby we convert information about the sample into intelligent guesses about the population fall under the rubric of inferential statistics. A sample is typically a small subset of the population. In the case of voting attitudes, we would sample a few thousand Americans drawn from the hundreds of millions that make up the country. In choosing a sample, it is therefore crucial that it not over-represent one kind of citizen at the expense of others. For example, something would be wrong with our sample if it happened to be made up entirely of Florida residents. If the sample held only Floridians, it could not be used to infer the attitudes of other Americans. The same problem would arise if the sample were comprised only of Republicans.\nInferential statistics is based on the assumption that sampling is random. We trust a random sample to represent different segments of society in close to the appropriate proportions (provided the sample is large enough; see below).\nExample 2: We are interested in examining how many math classes have been taken on average by current graduating seniors at American colleges and universities during their four years in school. Whereas our population in the last example included all US citizens, now it involves just the graduating seniors throughout the country. This is still a large set since there are thousands of colleges and universities, each enrolling many students. It would be prohibitively costly to examine the transcript of every college senior. We therefore take a sample of college seniors and then make inferences to the ntire population based on what we find. To make the sample, we might first choose some public and private colleges and universities across the United States. Then we might sample 50 students from each of these institutions. Suppose that the average number of math classes taken by the people in our sample were 3.2. Then we might speculate that 3.2 approximates the number we would find if we had the resources to examine every senior in the entire population. But we must be careful about the possibility that our sample is non-representative of the population. Perhaps we chose an overabundance of math majors, or chose too many technical institutions that have heavy math requirements. Such bad sampling makes our sample unrepresentative of the population of all seniors. To solidify your understanding of sampling bias, consider the following example. Try to identify the population and the sample, and then reflect on whether the sample is likely to yield the information desired.\nExample 3: A substitute teacher wants to know how students in the class did on their last test. The teacher asks the 10 students sitting in the front row to state their latest test score. He concludes from their report that the class did extremely well. What is the sample? What is the population? Can you identify any problems with choosing the sample in the way that the teacher did?\nIn Example 3, the population consists of all students in the class. The sample is made up of just the 10 students sitting in the front row. The sample is not likely to be representative of the population. Those who sit in the front row tend to be more interested in the class and tend to perform higher on tests. Hence, the sample may perform at a higher level than the population.\nExample 4: A coach is interested in how many cartwheels the average college freshmen at his university can do. Eight volunteers from the freshman class step forward. After observing their performance, the coach concludes that college freshmen can do an average of 16 cartwheels in a row without stopping.\nIn Example 4, the population is the class of all freshmen at the coach’s university. The sample is composed of the 8 volunteers. The sample is poorly chosen because volunteers are more likely to be able to do cartwheels than the average freshman; people who can’t do cartwheels probably did not volunteer! In the example, we are also not told of the gender of the volunteers. Were they all women, for example? That might affect the outcome, contributing to the non-representative nature of the sample.\nExample 5: Sometimes it is not feasible to build a sample using simple random sampling. To see the problem, consider the fact that both Dallas and Houston are competing to be hosts of the 2012 Olympics. Imagine that you are hired to assess whether most Texans prefer Houston to Dallas as the host, or the reverse. Given the impracticality of obtaining the opinion of every single Texan, you must construct a sample of the Texas population. But now notice how difficult it would be to proceed by simple random sampling. For example, how will you contact those individuals who don’t vote and don’t have a phone? Even among people you find in the telephone book, how can you identify those who have just relocated to California (and had no reason to inform you of their move)? What do you do about the fact that since the beginning of the study, an additional 4,212 people took up residence in the state of Texas? As you can see, it is sometimes very difficult to develop a truly random procedure.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data acquisition</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "5  Descriptive statistics",
    "section": "",
    "text": "5.1 Univariate data\nLearning objectives:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#univariate-data",
    "href": "statistics.html#univariate-data",
    "title": "5  Descriptive statistics",
    "section": "",
    "text": "5.1.1 Arithmetic mean\nThe arithmetic mean (\\(\\bar{x}\\)) is calculated as the sum of all the values in a dataset divided by the total number of values:\n\\[\\bar{x} = \\frac{{\\sum_{i=1}^{n} x_i}}{n}\\]\nwhere \\(\\bar{x}\\) represents the arithmetic mean, \\(x_i\\) represents each individual value in the dataset, and \\(n\\) represents the total number of values in the dataset.\n\n\n5.1.2 Median\nThe median is the middle value of a dataset when it is sorted in ascending or descending order. If the dataset has an odd number of values, the median is the middle value. If the dataset has an even number of values, the median is the average of the two middle values.\n\n\n5.1.3 Mode\nThe mode is the value or values that appear most frequently in a dataset.\n\n\n5.1.4 Range\nThe range is the difference between the maximum and minimum values in a dataset.\n\\[\n\\text{{Range}} = \\max(x_i) - \\min(x_i)\n\\]\nwhere \\(\\text{{Range}}\\) represents the range value, and \\(x_i\\) represents each individual value in the dataset.\n\n\n5.1.5 Variance\nThe variance represents the average of the squared deviations of a random variable from its mean. It quantifies the extent to which a set of numbers deviates from their average value. Variance is commonly denoted as \\(Var(X)\\), \\(\\sigma^2\\), or \\(s^2\\). The calculation of variance is as follows: \\[\n    \\sigma^2={\\frac{1}{n}}\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}\n\\] However, it is better to use \\[\n  \\sigma^2={\\frac{1}{n-1}}\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}.\n\\]\nThe use of \\(n - 1\\) instead of \\(n\\) in the formula for the sample variance is known as Bessel’s correction, which corrects the bias in the estimation of the population variance, and some, but not all of the bias in the estimation of the population standard deviation. Consequently this way to calculate the variance and hence the standard deviation is called the sample standard deviation or the unbiased estimation of standard deviation. In other words, when working with a sample instead of the full population the limited number of observations tend to be closer to the sample mean than to the population mean, see Figure 5.1. Bessels Correction takes that into account.\nFor a detailed explanation, you can watch the video by StatQuest with Josh Starmer: Why Dividing By N Underestimates the Variance:\n\n\n\n\n\nFigure 5.1: Bias when using the sample mean1\n\n1 Picture is taken from the video https://youtu.be/sHRBg6BhKjI\n\n\n\n\n\n\n5.1.6 Standard deviation\nAs the variance is hard to interpret, the standard deviation is a more often used measure of dispersion. A low standard deviation indicates that the values tend to be close to the mean. It is often abbreviated with \\(sd\\), \\(SD\\), or most often with the Greek letter sigma, \\(\\sigma\\). The underlying idea is to measure the average deviation from the mean. It is calculated as follows: \\[\nsd(x)=\\sqrt{\\sigma^2}={\\sqrt {{\\frac {1}{n-1}}\\sum _{i=1}^{n}\\left(x_{i}-{\\mu}\\right)^{2}}}=\\sigma\n\\]\n\n\n5.1.7 Standard error\nThe standard deviation (SD) measures the amount of variability, or dispersion, for a subject set of data from the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. The SEM is always smaller than the SD. It matters because it helps you estimate how well your sample data represents the whole population.\nThe standard error of the mean (SEM) can be expressed as: \\[\nsd(\\bar{x})=\\sigma_{\\bar {x}}\\ = s = {\\frac {\\sigma }{\\sqrt {n}}}\n\\] where \\(\\sigma\\) is the standard deviation of the population and \\(n\\) is the size (number of observations) of the sample.\nAlso see the video by StatQuest with Josh Starmer: Standard Deviation vs Standard Error, Clearly Explained!!!:\n\n\n\n\n\n\n\n\nTip 5.1\n\n\n\nWhy divide by the square root of \\(n\\)?\nLet \\(X_{i}\\) be an independent draw from a distribution with mean \\(\\bar{x}\\) and variance \\(\\sigma^{2}\\). What is the variance of \\(\\bar{x}\\)?\nBy definition: \\[\n\\operatorname{Var}(x)=E\\left[\\left(x_{i}-E\\left[x_{i}\\right]\\right)^{2}\\right]=\\sigma^{2}\n\\] so \\[\\begin{align*}\n         \\operatorname{Var}(\\bar{x})&=E\\left[\\left(\\frac{\\sum x_{i}}{n}-E \\left[\\frac{\\sum x_{i}}{n}\\right]\\right)^{2}\\right]\\\\\n         &=E\\left[\\left(\\frac{\\sum x_{i}}{n}-\\frac{1}{n} E\\left[ \\sum x_{i}\\right]\\right)^{2}\\right]\\\\\n         &=\\frac{1}{n^{2}} E\\left[\\left(\\sum x_{i}-E\\left[\\sum x_{i}\\right]\\right)^{2}\\right]\\\\\n         &=\\frac{1}{n^{2}} E\\left[\\left(\\sum x_{i}- \\sum \\bar{x}\\right)^{2}\\right]\\\\\n         &=\\frac{1}{n^{2}} E\\left[(x_{1}+x_{2}+\\cdots+x_{n}-\\underbrace{\\bar{x}-\\bar{x}-\\cdots -\\bar{x}}_{n \\text{ terms }})^{2}\\right]\\\\\n         &=\\frac{1}{n^{2}} E\\left[\\sum\\left(x_{i}-\\bar{x}\\right)^{2}\\right]\\\\\n         &=\\frac{1}{n^{2}} \\sum E\\left(x_{i}-\\bar{x}\\right)^{2}\\\\\n         &=\\frac{1}{n^{2}} \\underbrace{\\sum \\sigma^{2}}_{n\\cdot \\sigma^{2}}\\\\\n         &=\\frac{1}{n} \\sigma^{2}\n\\end{align*}\\] and hence \\[\nsd(\\bar x)=\\sqrt{\\operatorname{Var}(\\bar{x})}=s={\\frac {\\sigma }{\\sqrt {n}}}\n\\]\n\n\n\n\n5.1.8 Coefficient of variation\nThe coefficient of variation (\\(CoV\\)) is a relative measure of variability and is calculated as the ratio of the standard deviation to the mean, expressed as a percentage:\n\\[\nCoV =  \\frac{\\sigma}{\\bar{x}}\n\\]\nwhere \\(CoV\\) represents the coefficient of variation, \\(\\sigma\\) represents the standard deviation, and \\(\\bar{x}\\) represents the arithmetic mean.\n\n\n5.1.9 Skewness\nSkewness is a measure of the asymmetry of a distribution. There are different formulas to calculate skewness, but one common method is using the third standardized moment (\\(\\gamma_1\\)):\n\\[\n\\gamma_1 = \\frac{{\\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{\\sigma}\\right)^3}}{n}\n\\] where \\(\\gamma_1\\) represents the skewness, \\(x_i\\) represents each individual value in the dataset, \\(\\bar{x}\\) represents the arithmetic mean, \\(\\sigma\\) represents the standard deviation, and \\(n\\) represents the total number of values in the dataset.\n\n\n5.1.10 Kurtosis\nKurtosis measures the peakedness or flatness of a probability distribution. There are different formulations for kurtosis, and one of the common ones is the fourth standardized moment. The formula for kurtosis is given by:\n\\[\n\\text{Kurtosis} = \\frac{{\\frac{1}{n} \\sum_{i=1}^{n}(x_i - \\bar{x})^4}}{{\\left(\\frac{1}{n} \\sum_{i=1}^{n}(x_i - \\bar{x})^2\\right)^2}}\n\\] where \\(\\text{Kurtosis}\\) represents the kurtosis value, \\(x_i\\) represents each individual value in the dataset, \\(\\bar{x}\\) represents the mean of the dataset, and \\(n\\) represents the total number of values in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#bivariate-data",
    "href": "statistics.html#bivariate-data",
    "title": "5  Descriptive statistics",
    "section": "5.2 Bivariate data",
    "text": "5.2 Bivariate data\n\n5.2.1 Covariance\nCovariance \\(Cov(X,Y)\\) (or \\(\\sigma_{XY}\\)) is a measure of the joint variability of two variables (\\(x\\) and \\(y\\)) and their observations \\(i\\), respectively. The covariance is positive when larger values of one variable tend to correspond with larger values of the other variable, or when smaller values of one variable tend to correspond with smaller values of the other variable. On the other hand, a negative covariance suggests an inverse relationship, where larger values of one variable tend to correspond with smaller values of the other variable.\nIt’s important to note that the magnitude of the covariance is influenced by the units of measurement, making it challenging to interpret directly. Additionally, the spread of the variables also affects the covariance. The formula for calculating covariance is as follows: \\[\n\\operatorname{Cov}(X,Y)=\\sigma_{XY}={\\frac {1}{n-1}}\\sum _{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})\n\\] where \\(cov(X,Y)\\) represents the covariance, \\(\\sigma_{XY}\\) is an alternative notation, \\(x_i\\) and \\(y_i\\) are the individual observations of variables \\(X\\) and \\(Y\\), \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of variables \\(X\\) and \\(Y\\), and \\(n\\) is the total number of observations.\n\n\n\n\n\n\nTip 5.2\n\n\n\nTo gain a better understanding of the concept and calculation of covariance, I highly recommend watching Josh Starmer’s informative and visually engaging video titled Covariance and Correlation Part 1: Covariance:\n\n\n\n\n\n\n5.2.2 The correlation coefficient (Bravais-Pearson)\nThe Pearson correlation coefficient measures the linear relationship between two variables. It is calculated as the covariance of the variables divided by the product of their standard deviations. \\[\n\\rho_{X,Y} = \\frac{{\\text{Cov}(X, Y)}}{{\\sigma_X \\sigma_Y}}\n\\] where \\(\\rho\\) represents the Pearson correlation coefficient, \\(\\text{Cov}(X, Y)\\) denotes the covariance between variables \\(X\\) and \\(Y\\), \\(\\sigma_X\\) denotes the standard deviation of variable \\(X\\), and \\(\\sigma_Y\\) denotes the standard deviation of variable \\(Y\\). It has a value between +1 and -1.\nBy dividing the covariance of \\(X\\) and \\(Y\\) by the multiplication of the standard deviations of \\(X\\) and \\(Y\\), the correlation coefficient is normalized by having a minimum of -1 and a maximum of 1. Thus, it can fix the problem of the variance that the scale (unit of measurement) determines the size of the variance.\n\n\n\n\n\n\nTip 5.3\n\n\n\nI highly recommend watching the video Pearson’s Correlation, Clearly Explained!!! StatQuest with Josh Starmer. It provides a clear and engaging explanation of the meaning of correlation. The video features informative animations that help visualize the concept:\n\n\n\n\nIn interpreting correlations, it is important to remember that they…\n\n… only reflect the strength and direction of linear relationships,\n… do not provide information about the slope of the relationship, and\n… fail to explain important aspects of nonlinear relationships.\n\n\n\n\nFigure 5.2: Correlations are blind on some eye\n\n\n\n\n\n\nFigure 5.2 shows that correlation coefficients are limited in to explaining the relationship of two variables. For example, when the slope of a relationship is zero, the correlation coefficient becomes undefined due to the variance of \\(Y\\) being zero. Furthermore, Pearson’s correlation coefficient is sensitive to outliers, and all correlation coefficients are prone to sample selection biases. It is crucial to be careful when attempting to correlate two variables, particularly when one represents a part and the other represents the total. It is also worth noting that small correlation values do not necessarily indicate a lack of association between variables. For example, Pearson’s correlation coefficient can underestimates the association between variables exhibiting a quadratic relationship. Therefore, it is always advisable to examine scatterplots in conjunction with correlation analysis.\nIn Figure 5.3 you see various graphs that all have the same correlation coefficient and share other statistical properties like is investigated in Exercise 5.1.\n\n\n\nFigure 5.3: These diagrams all have the same statistical properties2\n\n2 This graph was produced employing the datasauRus R package.\n\n\n\n\n\n\n5.2.3 Rank correlation coefficient (Spearman)\nSpearman’s rank correlation coefficient is a measure of the strength and direction of the monotonic relationship between two variables. It can be calculated for a sample of size \\(n\\) by converting the \\(n\\) raw scores \\(X_i, Y_i\\) to ranks \\(\\text{R}(X_i), \\text{R}(Y_i)\\), then using the following formula:\n\\[\nr_s = \\rho_{\\operatorname{R}(X),\\operatorname{R}(Y)} = \\frac{\\text{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))}{\\sigma_{\\operatorname{R}(X)} \\sigma_{\\operatorname{R}(Y)}},\n\\] where \\(\\rho\\) denotes the usual Pearson correlation coefficient, but applied to the rank variables, \\(\\operatorname{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))\\) is the covariance of the rank variables, \\(\\sigma_{\\operatorname{R}(X)}\\) and \\(\\sigma_{\\operatorname{R}(Y)}\\) are the standard deviations of the rank variables.\nIf all \\(n\\) ranks are distinct integers, you can use the handy formula: \\[\n\\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2 - 1)}\n\\] where \\(\\rho\\) denotes the correlation coefficient, \\(\\sum d_i^2\\) is the sum of squared differences between the ranks of corresponding pairs of variables, and \\(n\\) represents the number of pairs of observations.\nThe coefficient ranges from -1 to 1. A value of 1 indicates a perfect increasing monotonic relationship, while a value of -1 indicates a perfect decreasing monotonic relationship. A value of 0 suggests no monotonic relationship between the variables.\nSpearman’s rank correlation coefficient is a non-parametric measure and is often used when the relationship between variables is not linear or when the data is in the form of ranks or ordinal categories.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "statistics.html#exercises",
    "href": "statistics.html#exercises",
    "title": "5  Descriptive statistics",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\n\n\n\n\n\n\n\nExercise 5.1 DatasauRus (Solution online.)\nThe following exercise shows how to create Figure 5.3 using the programming language R.\n\n\n\nFigure 5.4: The logo of the DatasauRus package\n\n\n\nSource: https://github.com/jumpingrivers/datasauRus.\n\n\n\n\nLoad the packages datasauRus and tidyverse. If necessary, install these packages.\nThe package datasauRus comes with a dataset in two different formats: datasaurus_dozen and datasaurus_dozen_wide. Store them as ds and ds_wide.\nOpen and read the R vignette of the datasauRus package. Also open the R documentation of the dataset datasaurus_dozen.\nExplore the dataset: What are the dimensions of this dataset? Look at the descriptive statistics.\nHow many unique values does the variable dataset of the tibble ds have? Hint: The function unique() return the unique values of a variable and the function length() returns the length of a vector, such as the unique elements.\nCompute the mean values of the x and y variables for each entry in dataset. Hint: Use the group_by() function to group the data by the appropriate column and then the summarise() function to calculate the mean.\nCompute the standard deviation, the correlation, and the median in the same way. Round the numbers.\nWhat can you conclude?\nPlot all datasets of ds. Hide the legend. Hint: Use the facet_wrap() and the theme() function.\nCreate a loop that generates separate scatter plots for each unique datatset of the tibble ds. Export each graph as a png file.\nWatch the video Animating the Datasaurus Dozen Dataset in R from The Data Digest on YouTube:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.2 Summary statistics (Solution 5.1)\nCalculate for the following datasets: the mode, the median, the 20% quantile, the range, the interquartile range, the variance, the arithmetic mean, the sample standard deviation, the coefficient of variation.\n\nFor ten participants in a scientific conference the age has been noted: [25, 21, 18, 37, 56, 89, 46, 23, 21, 34.]\nA random sample of 128 visitors of the Cupcake festival yielded the following frequencies regarding the cupcake consumption during their visit:\n\n\n\n\nTable 5.1: Random sample of 128 visitors\n\n\n\n\n\nCupcages consumed\n1\n2\n3\n4\n5\n6\n\n\n\n\nAbs. freq.\n2\n30\n37\n28\n23\n8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 5.2\n\n\n\n\n\n\nSolution 5.1. \n\n\n\nMetric\n1\n2\n\n\n\n\nMode\n21\n3\n\n\nMedian\n29.5\n3\n\n\nP20\n21\n2\n\n\nRange\n71\n5\n\n\nIQR\n25\n1.5\n\n\nArithmetic mean\n37\n3.5\n\n\n\\(\\sigma^2\\)\n485.33\n1.5591\n\n\n\\(\\sigma\\)\n22.030\n1.248621\n\n\nCOV\n0.5954\n2.4027\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.3 Summary statistics in spreadsheet software (Solution 5.2)\nGiven is the following datset: [0, 0, 40, 50, 50, 60, 70, 90, 100, 100.] Compute the following summary statistics of the data set using a spreadsheet software like or : mean, median, mode, quartiles (Q1, Q2, Q3), range, interquartile range, variance, standard deviation, mean absolute deviation, coefficient of variation and skewness.\n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 5.3\n\n\n\n\n\n\nSolution 5.2. \n\n\n\nMittelwert\n56\n\n\n\n\nStandardfehler\n11,4697670227235\n\n\nModus\n0\n\n\nMedian\n55\n\n\nErstes Quartil\n42,5\n\n\nDrittes Quartil\n85\n\n\nVarianz\n1315,55555555556\n\n\nStandardabweichung\n36,2705880232945\n\n\nKurtosis\n-0,731538352420953\n\n\nSchräge\n-0,417051115341008\n\n\nBereich\n100\n\n\nMinimum\n0\n\n\nMaximum\n100\n\n\nSumme\n560\n\n\nAnzahl\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.4 Guess the summary statistics (Solution 5.3)\nGiven are the following variables:\n\n\n\nTable 5.2: Some variables with observations\n\n\n\n\n\na\nb\nc\nd\ne\n\n\n\n\n97\n70\n1\n1\n970\n\n\n98\n80\n50\n2\n980\n\n\n99\n90\n50\n3\n990\n\n\n100\n100\n50\n4\n1000\n\n\n101\n110\n50\n5\n1010\n\n\n102\n120\n50\n6\n1020\n\n\n103\n130\n99\n7\n1030\n\n\n\n\n\n\nRank the variables without calculating concrete numbers accordingly to the values of the following descriptive statistics: mode, median, mean, range, variance, standard deviation, coefficient of variation.\n\n\n\n\n\n\n\n\n\n\nSolution to Exercise 5.4\n\n\n\n\n\n\nSolution 5.3. \n\n\n\n\n\n\n\n\n\n\n\nvarlabel\na\nb\nc\nd\ne\n\n\n\n\nVariance\n2 (4.6666)\n4 (466.66)\n5 (800.33)\n1 (4.6666)\n3 (466.66)\n\n\nCOV\n1 (.02160)\n3 (.21602)\n5 (.56580)\n4 (.54006)\n2 (.02160)\n\n\nMean\n3 (100)\n4 (100)\n2 (50)\n1 (4)\n5 (1000)\n\n\nMedian\n3 (100)\n4 (100)\n2 (50)\n1 (4)\n5 (1000)\n\n\nRange\n1 (6)\n4 (60)\n5 (98)\n2 (6)\n3 (60)\n\n\nSD\n1 (2.1602)\n4 (21.602)\n5 (28.290)\n2 (2.1602)\n3 (21.602)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "epidem.html",
    "href": "epidem.html",
    "title": "6  Epistemic",
    "section": "",
    "text": "6.1 Pitfalls\nThis section highlights some common pitfalls when dealing with data. Being aware of these pitfalls helps build best practices to maintain the integrity of analyses and visualizations. More pitfalls and explanations can be found in the excellent book by Jones (2020). I recommend reading this book.\nEpistemic errors occur when there are mistakes in our understanding and conceptualization of data. These errors arise from cognitive biases, misunderstandings, and incorrect assumptions about the nature of data and the reality it represents. Recognizing and addressing these errors is crucial for accurate data analysis and effective decision-making.\nOne significant type of epistemic error is the data-reality gap, which refers to the difference between the data we collect and the reality it is supposed to represent. For example, a survey on customer satisfaction that only includes responses from a self-selected group of highly engaged customers may not accurately reflect the overall customer base. To avoid this specific pitfall, it is essential to ensure that your data collection methods are representative and unbiased, and to validate your data against external benchmarks or additional data sources.\nTo avoid epistemic errors, critically assess your assumptions, methodologies, and interpretations. Engage in critical thinking by regularly questioning your assumptions and seeking alternative explanations for your findings. Employ methodological rigor by using standardized and validated methods for data collection and analysis. Engage with peers to review and critique your work, providing a fresh perspective and identifying potential biases. Finally, stay updated with the latest research and best practices in your field to avoid outdated or incorrect methodologies.\nAnother common epistemic error involves the influence of human biases during data collection and interpretation. Known as the all too human data error, this occurs when personal biases or inaccuracies affect the data. An example would be a researcher’s personal bias influencing the design of a study or the interpretation of its results. To mitigate this, implement rigorous protocols for data collection and analysis, and consider using double-blind studies and peer reviews to minimize bias.\nInconsistent ratings can also lead to epistemic errors. This happens when there is variability in data collection methods, resulting in inconsistent or unreliable data. For example, different evaluators might rate the same product using different criteria or standards. To avoid this issue, standardize data collection processes and provide training for all individuals involved in data collection to ensure consistency.\nUnderstanding and addressing epistemic errors can significantly improve the reliability and accuracy of your data analyses, leading to better decision-making and more trustworthy insights.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Epistemic</span>"
    ]
  },
  {
    "objectID": "epidem.html#pitfalls",
    "href": "epidem.html#pitfalls",
    "title": "6  Epistemic",
    "section": "",
    "text": "The Data-Reality Gap\n\n\n\nThe difference between the data we collect and the reality it is supposed to represent.\n\n\n\n\n\n\n\n\n\n\nAll Too Human Data\n\n\n\nErrors introduced by human biases or inaccuracies during data collection and interpretation.\n\n\n\n\n\n\n\n\n\nInconsistent Ratings\n\n\n\nVariability in data collection methods that leads to inconsistent or unreliable data.\n\n\n\n\n\n\n\n\n\n\nExercise 6.1  \n\n\n\nFigure 6.1: Bananas in various stages of ripeness\n\n\n\nSource: Jones (2020, p. 33)\n\n\n\n\nRate the ripeness level of the bananas pictured by Figure 6.1. Compare your assessment to that of a colleague and discuss any differences in your ratings. What might account for the variance in perception of the bananas’ ripeness between you and your colleague?\nSpecify how you rated the second and the last bananas on the ripeness scale?\nUpon reevaluation, it appears that the second and the last bananas are identical in ripeness. How would you justify your initial decision now? This scenario underscores an important lesson for interpreting polls and surveys: it illustrates how subjective assessments can lead to variance in results. It highlights the necessity of ensuring clarity and consistency in the criteria used for evaluations to minimize subjective discrepancies.\n\n\n\n\n\n\nJones, B. (2020). Avoiding data pitfalls: How to steer clear of common blunders when working with data and presenting analysis and visualizations. John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Epistemic</span>"
    ]
  },
  {
    "objectID": "epidem.html#sec-cornotcaus",
    "href": "epidem.html#sec-cornotcaus",
    "title": "6  Epistemic",
    "section": "6.2 Correlation does not imply causation",
    "text": "6.2 Correlation does not imply causation\nCorrelation refers to a statistical relationship between two variables, where one variable tends to increase or decrease as the other variable also increases or decreases. However, just because two variables are correlated does not necessarily mean that one variable causes the other. This is known as the correlation does not imply causation principle.\nFor example, across many areas the number of storks is correlated with the birth rate of babies (see Matthews, 2000). However, this does not mean that the presence of storks causes an increase in the birth rate. It is possible that both the number of storks and the number of babies born are influenced by other factors, such as the overall population density or economic conditions in the area.\n\nMatthews, R. (2000). Storks deliver babies (p= 0.008). Teaching Statistics, 22(2), 36–38.\nTherefore, it is important to carefully consider all possible explanations (confounders) for a correlation and to use data to disentangle the true cause-and-effect relationship between variables.\n\n\n\nFigure 6.2: Correlation does not imply causation1\n\n1 Source: https://youtu.be/DFPm_a-_uJM\n\n\n\n\n\n\n\n\n\n\nTip 6.1\n\n\n\nWatch the video of Brady Neal’s lecture Correlation Does Not Imply Causation and Why. Alternatively, you can read chapter 1.3 of his lecture notes (Neal, 2020) which you find here.\n\n\n\nNeal, B. (2020). Introduction to causal inference from a machine learning perspective: Course lecture notes. Accessed January 30, 2023. https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Epistemic</span>"
    ]
  },
  {
    "objectID": "epidem.html#simpsons-paradox",
    "href": "epidem.html#simpsons-paradox",
    "title": "6  Epistemic",
    "section": "6.3 Simpsons Paradox",
    "text": "6.3 Simpsons Paradox\n\n\n\nFigure 6.3: Discrimination2\n\n2 Source: The photography is public domain and stems from the Library of Congress Prints and Photographs Division Washington, see: http://hdl.loc.gov/loc.pnp/pp.print.\n\n\n\n\nDiscrimination is bad. Whenever we see it, we should try to find ways to overcome it. De jure segregation mandated the separation of races by law is clearly discriminatory. Other forms of discrimination, however, are often more difficult to spot and as long we don’t have good evidence for discrimination, we should not judge prematurely. That means, we should be sure that we see an act of making unjustified distinctions between individuals based on some categories to which they belong or perceived to belong. For example, if men and women are treated differently without an acceptable reason, we consider it discriminative.\nHowever, as the following example discussed in Bickel et al. (1975) will show, it is often challenging to identify discrimination. In 1973, UC Berkeley was accused of discrimination because it admitted only 35% of female applicants but 44% of male applicants overall. The difference was statistical significant and based on that many people protested claiming justice and equality. However, it turned out that the selection of students was not discriminative against women but against men. Accordingly to Bickel et al. (1975) the different overall admission rates can be largely explained by a “tendency of women to apply to graduate departments that are more difficult for applicants of either sex to enter” (Bickel et al., 1975, p. 403). Figure Figure 6.4 taken from Bickel et al. (1975, p. 403) visualizes this fact. Looking on the decisions within the departments seperately, there is even a “statistically significant bias in favor of women” (Bickel et al., 1975, p. 403).\n\n\n\nFigure 6.4: Proportion of applicants that are women plotted against proportion of applicants admitted3\n\n3 Source: Bickel et al. (1975, p. 403)\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley: Measuring bias is harder than is usually assumed, and the evidence is sometimes contrary to expectation. Science, 187(4175), 398–404.\n\n\n\n\n\nHere is a summary of Bickel et al. (1975, p. 403):\n\n“Examination of aggregate data on graduate admissions to the University of California, Berkeley, for fall 1973 shows a clear but misleading pattern of bias against female applicants. Examination of the disaggregated data reveals few decision-making units that show statistically significant departures from expected frequencies of female admissions, and about as many units appear to favor women as to favor men. If the data are properly pooled, taking into account the autonomy of departmental decision making, thus correcting for the tendency of women to apply to graduate departments that are more difficult for applicants of either sex to enter, there is a small but statistically significant bias in favor of women. The graduate departments that are easier to enter tend to be those that require more mathematics in the undergraduate preparatory curriculum. The bias in the aggregated data stems not from any pattern of discrimination on the part of admissions committees, which seem quite fair on the whole, but apparently from prior screening at earlier levels of the educational system. Women are shunted by their socialization and education toward fields of graduate study that are generally more crowded, less productive of completed degrees, and less well funded, and that frequently offer poorer professional employment prospects.”\n\n\n\n\n\n\n\n\nExercise 6.2 Graduate admissions\nRead the first three pages of Bickel et al. (1975), i.e., pages 398-400, and answer the following questions. The article can be found here.\n\nDescribe the two assumptions that must be true in order to prove that UC Berkeley discriminates against women or men overall.\nTable 1, shows that 277 fewer women and 277 more men were admitted than we would have expected under the two assumptions. Show how this number was calculated.\nTable 2 of the paper which you find also in Table 6.1 shows admissions data by sex of applicant for two hypothetical departments. Explain the discrimation that is explained in that example.\n\n\n\n\nTable 6.1: Admissions data by sex of applicant for two hypothetical departments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplicants\nAdmit (observed)\nDeny (observed)\nAdmit (expected)\nDeny (expected)\nAdmit (difference)\nDeny (difference)\n\n\n\n\nDepartment of Machismatics\n\n\n\n\n\n\n\n\nMen\n200\n200\n200\n200\n0\n0\n\n\nWomen\n100\n100\n100\n100\n0\n0\n\n\nDepartment of Social Warfare\n\n\n\n\n\n\n\n\nMen\n50\n100\n50\n100\n0\n0\n\n\nWomen\n150\n300\n150\n300\n0\n0\n\n\nTotals\n\n\n\n\n\n\n\n\nMen\n250\n300\n229.2\n320.8\n20.8\n-20.8\n\n\nWomen\n250\n400\n270.8\n379.2\n-20.8\n20.8\n\n\n\nSource: Bickel et al. (1975, p. 400)\n\n\n\n\nExplain the analogy with fish that illustrates the danger of pooling data.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAssumption 1 is that in any given discipline male and female applicants do not differ in respect of their intelligence, skill, qualifications, promise, or other attribute deemed legitimately pertinent to their acceptance as students. It is precisely this assumption that makes the study of “sex bias” meaningful, for if we did not hold it any differences in acceptance of applicants by sex could be attributed to differences in their qualifications, promise as scholars, and so on. (…) Assumption 2 is that the sex ratios of applicants to the various fields of graduate study are not importantly associated with any other factors in admission. (Bickel et al., 1975, p. 398)\nExpectations were taken based on the overall acceptance rate of about 0.416 and multiplied by the total observed numbers of applicants admitted and rejected. For example: \\((3738+4704) \\cdot 0.41666 \\approx 3460\\) and \\((3738+4704) \\cdot (1-0.41666) \\approx 4981\\). Taking the difference of these two measures gives the number to be explained.\nIn the department of machismatics no gender discrimination can be observed as 50% of women and men were accepted. Also, in the department of social warfare no gender discrimination can be spotted as 1/3 of women and men were accepted. However, overall only about 38% (250 of 650) of women have been accepted while about 45% (250 of 550) of men were accepted. The latter can be explained by more female applicants in the department with the higher rejection rate.\nThe analogy is explained on page 400:\n\n\n“Picture a fishnet with two different mesh sizes. A school of fish, all of identical size (assumption 1), swim toward the net and seek to pass. The female fish all try to get through the small mesh, while the male fish all try to get through the large mesh. On the other side of the net all the fish are male. Assumption 2 said that the sex of the fish had no relation to the size of the mesh they tried to get through. It is false.”\n\nThe UC Berkley case is just one of many examples to illustrate that uniformity of group assignment of individuals is a necessary condition to ensure that pooling of data does not lead to misleading conclusions when using statistics. The phenomenon of obtaining different results depending on whether one considers the data pooled or unpooled is often referred to as the Simpson Paradox.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.3 Simpson’s Paradox\n\nWhat is Simpson’s Paradox?\n\nA phenomenon in which the direction of a relationship between two variables changes when a third variable is introduced\nA phenomenon in which the strength of a relationship between two variables changes when a third variable is introduced\nThe phenomenon where correlation appears to be present in different groups of data, but disappears or reverses when the groups are combined\n\nWhat is a potential cause of Simpson’s Paradox?\n\nDifferences in the variance of the two variables\nDifferences in the correlation of the two variables\nConfounding variables\nDifferences in the sample size of the two variables\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\na), 2. c) and d)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Epistemic</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "7  Regression analysis",
    "section": "",
    "text": "7.1 Simple linear regression\nThe linear regression analysis is a widely used technique for predictive modeling. Its purpose is to establish a mathematical equation that relates a continuous response variable, denoted as \\(y\\), to one or more independent variables, represented by \\(x\\). The objective is to create a regression model that enables the prediction of the value of \\(y\\) based on known values of \\(x\\).\nTo ensure meaningful predictions, it is important to have an adequate number of observations, denoted as \\(i\\), available for the variables of interest.\nThe linear regression model can be expressed as: \\[\ny_i = \\beta_{0} + \\beta_{1} x_i + \\epsilon_i,\n\\] where the index \\(i\\) denotes the individual observations, ranging from \\(i = 1\\) to \\(n\\). The variable \\(y_i\\) represents the dependent variable, also known as the regressand. The variable \\(x_i\\) represents the independent variable, also referred to as the regressor. \\(\\beta_0\\) denotes the intercept of the population regression line, a.k.a. the constant. \\(\\beta_1\\) denotes the slope of the population regression line. Lastly, \\(\\epsilon_i\\) refers to the error term or the residual, which accounts for the deviation between the predicted and observed values of \\(y_i\\).\nBy fitting a linear regression model, one aims to estimate the values of \\(\\beta_0\\) and \\(\\beta_1\\) in order to obtain an equation that best captures the relationship between \\(y\\) and \\(x\\).\nWhile the correlation coefficient and the slope in simple linear regression are similar in many ways, it’s important to note that they are not identical. The correlation coefficient measures the strength and direction of the linear relationship between variables in a broader sense, while the slope in simple linear regression specifically quantifies the change in the dependent variable associated with a unit change in the independent variable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#simple-linear-regression",
    "href": "regression.html#simple-linear-regression",
    "title": "7  Regression analysis",
    "section": "",
    "text": "7.1.1 Estimating the coefficients of the linear regression model\nIn practice, the intercept and slope of the regression are unknown. Therefore, we must employ data to estimate the unknown parameters, \\(\\beta_0\\) and \\(\\beta_1\\). The method we use is called the ordinary least squared (OLS) method. The idea is to minimize the sum of the squared differences of all \\(y_i\\) and \\(y_i^*\\) as sketched in figure Figure 7.1.\n\n\n\nFigure 7.1: The fitted line and the residuals\n\n\n\n\n\n\nThus, we minimize the squared residuals by choosing the estimated coefficients \\(\\hat{\\beta_{0}}\\) and \\(\\hat{\\beta_{1}}\\) \\[\\begin{align*}\n   \\min_{\\hat{\\beta_{0}}, \\hat{\\beta_{1}}}\\sum_{i=1} \\epsilon_i^2 &= \\sum_{i=1} \\left[y_i -   \\underbrace{(\\hat{\\beta_{0}} + \\hat{\\beta_{1}} x_i)}_{\\textnormal{predicted values}\\equiv y_i^*}\\right]^2\\\\\n   \\Leftrightarrow  &=  \\sum_{i=1}  (y_i - \\hat{\\beta_{0}} - \\hat{\\beta_{1}} x_i)^2\n\\end{align*}\\] Minimizing the function requires to calculate the first order conditions with respect to alpha and beta and set them zero: \\[\\begin{align*}\n   \\frac{\\partial \\sum_{i=1} \\epsilon_i^2}{\\partial \\beta_{0}}=2 \\sum_{i=1} (y_i - \\hat{\\beta_{0}} - \\hat{\\beta_{1}} x_i)=0\\\\\n   \\frac{\\partial \\sum_{i=1} \\epsilon_i^2}{\\partial \\beta_{1}}=2 \\sum_{i=1} (y_i - \\hat{\\beta_{0}} - \\hat{\\beta_{1}} x_i)x_i=0\n\\end{align*}\\] This is just a linear system of two equations with two unknowns \\(\\beta_{0}\\) and \\(\\beta_{1}\\), which we can mathematically solve for \\(\\beta_0\\): \\[\\begin{align*}\n   &\\sum_{i=1}  (y_i - \\hat{\\beta_{0}} - \\hat{\\beta_{1}} x_i)=0\\\\\n   \\Leftrightarrow \\hat{\\beta_{0}}&=\\frac{1}{n}\\sum_{i=1}   (y_i  - \\hat{\\beta_{1}} x_i)\\\\\n   \\Leftrightarrow \\hat{\\beta_{0}}&=\\bar{y}-\\hat{\\beta_{1}}\\bar{x}\n\\end{align*}\\] and for \\(\\beta_{1}\\): \\[\\begin{align*}\n   &\\sum_{i=1}  (y_i - \\hat{\\beta_{0}} - \\hat{\\beta_{1}} x_i)x_i=0\\\\\n   \\Leftrightarrow & \\sum_{i=1} y_i x_i- \\underbrace{\\hat{\\beta_{0}}}_{\\bar{y}-\\hat{\\beta_{1}}\\bar{x}}x_i - \\hat{\\beta_{1}} x_i^2=0\\\\\n   \\Leftrightarrow & \\sum_{i=1} y_i x_i- (\\bar{y}-\\hat{\\beta_{1}}\\bar{x})x_i - \\hat{\\beta_{1}} x_i^2=0\\\\    \n   \\Leftrightarrow & \\sum_{i=1} y_i x_i- \\bar{y}x_i-\\hat{\\beta_{1}}\\bar{x}x_i - \\hat{\\beta_{1}} x_i^2=0\\\\   \n   \\Leftrightarrow & \\sum_{i=1} (y_i - \\bar{y}-\\hat{\\beta_{1}}\\bar{x} - \\hat{\\beta_{1}} x_i)x_i=0\\\\\n   %    \\Leftrightarrow & \\sum_{i=1}    (y_i - \\bar{y})-\\beta_{1}\\bar{x} - \\hat{\\beta_{1}} x_i=0\\\\\n   \\Leftrightarrow & \\sum_{i=1} (y_i - \\bar{y}) x_i -\\hat{\\beta_{1}}(\\bar{x} -  x_i)x_i =0\\\\\n   \\Leftrightarrow & \\sum_{i=1} (y_i - \\bar{y}) x_i  =  \\hat{\\beta_{1}} \\sum_{i=1} (\\bar{x} -  x_i) x_i \\\\\n   %    \\Leftrightarrow & \\beta_{1} =\\frac{\\sum_{i=1}(y_i - \\bar{y})x_i }{ \\sum_{i=1} (\\bar{x} -  x_i)x_i }\\\\\n   \\Leftrightarrow & \\hat{\\beta_{1}} =\\frac{\\sum_{i=1}(y_i - \\bar{y})x_i }{ \\sum_{i=1} (\\bar{x} -  x_i)x_i }\\\\\n   \\Leftrightarrow & \\hat{\\beta_{1}} =\\frac{\\sum_{i=1}(y_i -\\bar{y})(x_i-\\bar{x})}{\\sum_{i=1} (\\bar{x} -  x_i)^2 }\\\\\n   \\Leftrightarrow & \\hat{\\beta_{1}} ={\\frac {\\sigma_{x,y}}{\\sigma^2_{x}}}\n\\end{align*}\\] The estimated regression coefficient \\(\\hat{\\beta_{1}}\\) equals the covariance between \\(y\\) and \\(x\\) divided by the variance of \\(x\\).\nThe formulas presented above may not be very intuitive at first glance. The online version of the book Hanck et al. (2020) offers a nice interactive application in the box The OLS Estimator, Predicted Values, and Residuals that helps to understand the mechanics of OLS. You can add observations by clicking into the coordinate system where the data are represented by points. Once two or more observations are available, the application computes a regression line using OLS and some statistics which are displayed in the right panel. The results are updated as you add further observations to the left panel. A double-click resets the application, that means, all data are removed.\n\nHanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (2020). Introduction to econometrics with R. University of Duisburg-Essen. www.econometrics-with-r.org\n\n\n7.1.2 The least squares assumptions\nOLS performs well under a quite broad variety of different circumstances. However, there are some assumptions which need to be satisfied in order to ensure that the estimates are normally distributed in large samples.\nThe Least Squares Assumptions should fulfill the following assumptions: \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\text{, } i = 1,\\dots,n\n\\]\n\nThe error term \\(\\epsilon_i\\) has conditional mean zero given \\(X_i: E(u_i|X_i)=0\\).\n\\((X_i,Y_i), i=1,\\dots,n\\) are independent and identically distributed (i.i.d.) draws from their joint distribution.\nLarge outliers are unlikely: \\(X_i\\) and \\(Y_i\\) have nonzero finite fourth moments. That means, assumption 3 requires that \\(X\\) and \\(Y\\) have a finite kurtosis.\n\n\n\n7.1.3 Measures of fit\nAfter fitting a linear regression model, a natural question is how well the model describes the data. Visually, this amounts to assessing whether the observations are tightly clustered around the regression line. Both the coefficient of determination and the standard error of the regression measure how well the OLS Regression line fits the data.\n\\(R^2\\) is the fraction of the sample variance of \\(Y_i\\) that is explained by \\(X_i\\). Mathematically, the \\(R^2\\) can be written as the ratio of the explained sum of squares to the total sum of squares. The explained sum of squares (ESS) is the sum of squared deviations of the predicted values \\(\\hat{Y_i}\\), from the average of the \\(Y_i\\). The total sum of squares (TSS) is the sum of squared deviations of the \\(Y_i\\) from their average. Thus we have \\[\\begin{align}\n   ESS & =  \\sum_{i = 1}^n \\left( \\hat{Y_i} - \\overline{Y} \\right)^2,   \\\\\n   TSS & =  \\sum_{i = 1}^n \\left( Y_i - \\overline{Y} \\right)^2,   \\\\\n   R^2 & = \\frac{ESS}{TSS}.\n\\end{align}\\] Since \\(TSS = ESS + SSR\\) we can also write \\[\nR^2 = 1- \\frac{\\textcolor{blue}{SSR}}{\\textcolor{red}{TSS}}\n\\] with \\[\nSSR= \\sum_{i = 1}^n \\epsilon^2.\n\\]\n\n\n\nFigure 7.2: Total sum of squares and sum of squared residuals\n\n\n\n\n\n\n\\(R^2\\) lies between 0 and 1. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies \\(R2=1\\) since then we have \\(SSR=0\\). On the contrary, if our estimated regression line does not explain any variation in the \\(Y_i\\), we have \\(ESS=0\\) and consequently \\(R^2=0\\). Figure 7.2 represents the relationship of TTS and SSR.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#multiple-linear-regression",
    "href": "regression.html#multiple-linear-regression",
    "title": "7  Regression analysis",
    "section": "7.2 Multiple linear regression",
    "text": "7.2 Multiple linear regression\nHaving understood the simple linear regression model, it is important to broaden our scope beyond the relationship between just two variables: the dependent variable and a single regressor. Our goal is to causally interpret the measured association of two variables, which requires certain conditions as explained in Chapter 3.\n\n7.2.1 Simpson’s paradox and regressions\nTo illustrate this concept, let’s revisit the phenomenon known as Simpson’s paradox. Simpson’s paradox occurs when the overall association between two categorical variables differs from the association observed when we consider the influence of one or more other variables, known as controlling variables. This paradox highlights three key points:\n\nIt challenges the assumption that statistical relationships are fixed and unchanging, showing that the relationship between two variables can vary depending on the set of variables being controlled.\nSimpson’s paradox is part of a larger class of association paradoxes, indicating that similar situations can arise in various contexts.\nIt serves as a reminder of the potential pitfalls of making causal inferences in nonexperimental studies, emphasizing the importance of considering confounding variables.\n\nThus, it is important to consider confounding variables to ensure valid and reliable causal interpretations in research, particularly in nonexperimental settings.\n\n\n\nFigure 7.3: Simpsons paradox and the power of controlling variables (1)\n\n\n\n\n\n\n\n\n\nFigure 7.4: Simpsons paradox and the power of controlling variables (2)\n\n\n\n\n\n\nThe multiple regression model can be expressed as:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\dots + \\beta_k X_{ki} + u_i, \\ i=1,\\dots,n.\n\\]\nTo estimate the coefficients of the multiple regression model, we seek to minimize the sum of squared mistakes by choosing estimated coefficients \\(\\beta_0,\\beta_1,\\dots,\\beta_k\\) such that:\n\\[\n\\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - b_2 X_{2i} - \\dots -  b_k X_{ki})^2\n\\]\nThis demands matrix notation which goes beyond the scope of this introduction.\n\n\n7.2.2 Gauss-Markov and the best linear unbiased estimator\nThe Gauss-Markov assumptions, also known as the classical linear regression assumptions, are a set of assumptions that underlie the ordinary least squares (OLS) method for estimating the parameters in a linear regression model. These assumptions ensure that the OLS estimators are unbiased, efficient, and have desirable statistical properties.\nThe Gauss-Markov assumptions are as follows:\n\nLinearity: The relationship between the dependent variable and the independent variables is linear in the population model. This means that the true relationship between the variables can be represented by a linear equation.\nIndependence: The errors (residuals) in the regression model are independent of each other. This assumption ensures that the errors for one observation do not depend on or influence the errors for other observations.\nStrict exogeneity: The errors have a mean of zero conditional on all the independent variables. In other words, the expected value of the errors is not systematically related to any of the independent variables.\nNo perfect multicollinearity: The independent variables are not perfectly correlated with each other. Perfect multicollinearity occurs when one independent variable is a perfect linear combination of other independent variables, leading to problems in estimating the regression coefficients.\nHomoscedasticity: The errors have constant variance (homoscedasticity) across all levels of the independent variables. This assumption implies that the spread or dispersion of the errors is the same for all values of the independent variables.\nNo endogeneity: The errors are not correlated with any of the independent variables. Endogeneity occurs when there is a correlation between the errors and one or more of the independent variables, leading to biased and inefficient estimators.\nNo autocorrelation: The errors are not correlated with each other, meaning that there is no systematic pattern or relationship between the errors for different observations.\n\nThese assumptions collectively ensure that the OLS estimators are unbiased, efficient, and have minimum variance among all linear unbiased estimators. Violations of these assumptions can lead to biased and inefficient estimators, invalid hypothesis tests, and unreliable predictions. Therefore, it is important to check these assumptions when using the OLS method and consider alternative estimation techniques if the assumptions are violated.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#how-to-identify-statistically-significant-estimated-coefficients-in-a-regression-analysis",
    "href": "regression.html#how-to-identify-statistically-significant-estimated-coefficients-in-a-regression-analysis",
    "title": "7  Regression analysis",
    "section": "7.3 How to identify statistically significant estimated coefficients in a regression analysis",
    "text": "7.3 How to identify statistically significant estimated coefficients in a regression analysis\n\n7.3.1 P-values\nWhen you perform regression analysis, the results are often presented in a table that includes p-values. Understanding these p-values is essential to determine whether the relationships between your variables are statistically significant.\nThe p-value tells us the probability that the coefficient (effect) we observe is due to random chance rather than a real relationship. Thus, it ranges theoretically from 0 to 1.\n\nThreshold for Significance\nCommonly used thresholds (also called significance levels) are:\n\n0.05 (5% significance level)\n0.01 (1% significance level)\n0.001 (0.1% significance level)\n\nDecision Rule for a 5% significance level:\n\nP-Value &lt; 0.05: The result is considered statistically significant. There is strong evidence that the coefficient (relationship) is not due to random chance.\nP-Value ≥ 0.05: The result is not considered statistically significant. There is not enough evidence to say that the coefficient is different from zero (no relationship).\n\n\nHere is an example of a stylized regression table:\n\n\n\nVariable\nCoefficient\nP-Value\n\n\n\n\nIntercept\n2.5\n0.0001\n\n\nStudy Hours\n0.8\n0.0005\n\n\nSleep Hours\n0.1\n0.0450\n\n\nTV Watching\n-0.3\n0.0600\n\n\n\nHow can we interpret the p-values:\n\nIntercept (P-Value: 0.0001): The p-value is much less than 0.05, indicating the intercept is statistically significant. This means the starting value (when all other variables are zero) is not due to chance.\nStudy Hours (P-Value: 0.0005): The p-value is less than 0.05, so the coefficient for Study Hours is statistically significant. This means there is strong evidence that more study hours are associated with higher scores, and this relationship is unlikely to be due to chance.\nSleep Hours (P-Value: 0.0450): The p-value is slightly less than 0.05, indicating that the coefficient for Sleep Hours is statistically significant. There is some evidence that more sleep hours are associated with higher scores.\nTV Watching (P-Value: 0.0600): The p-value is greater than 0.05, meaning the coefficient for TV Watching is not statistically significant. There isn’t strong enough evidence to conclude that TV Watching affects scores.\n\nSummary:\n\nP-values help you determine the significance of your results.\nA p-value less than 0.05 typically indicates a significant result, meaning the variable likely has an effect.\nA p-value greater than 0.05 suggests the variable’s effect is not statistically significant, and any observed relationship might be due to chance.\n\n\n\n7.3.2 t-values\nUnderstanding t-values helps to determine whether the relationships between your variables are statistically significant.\nThe t-value measures how many standard deviations the estimated coefficient is away from zero. It helps us understand if the coefficient is significantly different from zero (no effect).\n\nThreshold for Significance:\n\nThe significance of a t-value depends on the chosen significance level (e.g., 0.05) and the degrees of freedom in the regression model.\nIn regression analysis, “degrees of freedom” refer to the number of independent values that can vary in the calculation of a statistic, typically calculated as the number of observations minus the number of estimated parameters (including the intercept). This concept helps adjust the precision of the estimates and the validity of the statistical tests used in the analysis.\nA common rule of thumb is that a t-value greater than approximately 2 (or less than -2) indicates statistical significance at the 0.05 level.\nA little bit more precise is the value 1.96. It is often called the “magic number” as it is crucial in statistics because it marks the cutoff for a 95% confidence interval in a standard normal distribution, meaning 95% of data lies within 1.96 standard deviations of the mean. This makes it a key threshold for determining statistical significance in hypothesis testing at the 5% significance level.\n\nDecision Rule:\n\n|t-Value| &gt; 1.96: The result is considered statistically significant. There is strong evidence that the coefficient (relationship) is not zero.\n|t-Value| ≤ 1.96: The result is not considered statistically significant. There is not enough evidence to say that the coefficient is different from zero (no relationship).\n\n\nHere is an example of a stylized regression table:\n\n\n\nVariable\nCoefficient\nT-Value\n\n\n\n\nIntercept\n2.5\n5.0\n\n\nStudy Hours\n0.8\n4.0\n\n\nSleep Hours\n0.1\n1.89\n\n\nTV Watching\n-0.3\n-2.1\n\n\n\nHow can we interpret the t-values:\n\nIntercept (T-Value: 5.0): The t-value is much greater than 2, indicating the intercept is statistically significant. This means the starting value (when all other variables are zero) is not due to chance.\nStudy Hours (T-Value: 4.0): The t-value is greater than 2, so the coefficient for Study Hours is statistically significant. This means there is strong evidence that more study hours are associated with higher scores, and this relationship is unlikely to be due to chance.\nSleep Hours (T-Value: 1.89): The t-value is less than 2 in absolute values. This indicates that the coefficient for Sleep Hours may be statistically insignificant, suggesting no evidence that more sleep hours are associated with higher scores.\nTV Watching (T-Value: -2.1): The t-value is -2.1, which is more than 2 in absolute values. This suggests that there might be some evidence that more TV watching is associated with lower scores.\n\nSummary:\n\nT-values help you determine the significance of your estimated coefficients.\nA t-value greater than 2 (or less than -2) typically indicates a significant result, meaning the variable likely has an effect.\nA t-value between -2 and 2 suggests the variable’s effect is not statistically significant, and any observed relationship might be due to chance.\n\n\n\n7.3.3 Standard Error\nUnderstanding the standard error also helps to assess regression estimates. It measures the average distance that the observed values fall from the regression line. It provides an estimate of the variability of the coefficient.\nThe standard error (SE) of a coefficient quantifies the precision of the coefficient estimate. Smaller standard errors indicate more precise estimates.\nInterpreting SE:\n\nSmall SE: Indicates that the estimated coefficient is precise and likely to be close to the true population value.\nLarge SE: Suggests more variability in the estimate, making it less reliable and potentially due to random chance.\n\nHere is an example of stylized regression table might look like, including standard errors:\n\n\n\nVariable\nCoefficient\nStandard error\n\n\n\n\nIntercept\n2.5\n0.5\n\n\nStudy Hours\n0.8\n0.2\n\n\nParty Hours\n-0.5\n0.05\n\n\nTV Watching\n-0.3\n0.15\n\n\n\nUnfortunately, the standard error itself does not allow to judge on an estimate. However, knowing the estimate and the SE allows us to calculate the t-values as follows:\nLet \\(\\hat{\\alpha }\\) be an estimator of parameter \\(\\alpha\\) in some statistical model. Then a t-statistic for this parameter is any quantity of the form\n\\[\nt_{\\hat {\\alpha }}={\\frac {{\\hat {\\alpha }}-\\alpha _{0}}{s.e. ({\\hat{\\alpha }})}}\n\\left(=\\frac{\\text{estimated value - hypothesized value}}{\\text{standard error of the estimator}}\\right)\n\\]\nCalculating t-values\nWe can calculate the t-values for each coefficient using the formula:\n\\[\nt_{\\hat{\\alpha}} = \\frac{\\hat{\\alpha}}{s.e.(\\hat{\\alpha})}\n\\]\nGiven the regression table:\n\n\n\nVariable\nCoefficient\nStandard Error\n\n\n\n\nIntercept\n2.5\n0.5\n\n\nStudy Hours\n0.8\n0.2\n\n\nParty Hours\n-0.5\n0.05\n\n\nTV Watching\n-0.3\n0.15\n\n\n\nWe calculate the t-values as follows:\n\nIntercept: \\[\nt_{\\text{Intercept}} = \\frac{2.5}{0.5} = 5.0\n\\]\nStudy Hours: \\[\nt_{\\text{Study Hours}} = \\frac{0.8}{0.2} = 4.0\n\\]\nParty Hours: \\[\nt_{\\text{Party Hours}} = \\frac{-0.5}{0.05} = -10.0\n\\]\nTV Watching: \\[\nt_{\\text{TV Watching}} = \\frac{-0.3}{0.15} = -2.0\n\\]\n\nHere’s the updated table with the t-values:\n\n\n\nVariable\nCoefficient\nStandard Error\nT-Value\n\n\n\n\nIntercept\n2.5\n0.5\n5.0\n\n\nStudy Hours\n0.8\n0.2\n4.0\n\n\nParty Hours\n-0.5\n0.05\n-10.0\n\n\nTV Watching\n-0.3\n0.15\n-2.0\n\n\n\nExplanation of the T-Values:\n\nIntercept (t-value: 5.0): The t-value of 5.0 indicates that the intercept is highly significant.\nStudy Hours (t-value: 4.0): The t-value of 4.0 suggests that the coefficient for Study Hours is statistically significant.\nParty Hours (t-value: -10.0): The t-value of -10.0 shows a highly significant negative effect of Party Hours on the dependent variable.\nTV Watching (t-value: -2.0): The t-value of -2.0 indicates that the coefficient for TV Watching is significant, but less so compared to the other variables.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#sec-scrcj",
    "href": "regression.html#sec-scrcj",
    "title": "7  Regression analysis",
    "section": "7.4 Statistical control requires causal justification",
    "text": "7.4 Statistical control requires causal justification\n\n\n\n\n\n\nTip 7.1\n\n\n\nRead Wysocki et al. (2022) which is freely available here. Here you find a good summary of the paper.\n\n\nScientific research revolves around challenging our own views and findings. A good researcher does not merely present their results; instead, they engage in discussions about potential limitations and pitfalls to draw valid conclusions. Engaging in polemics goes against the essence of good research. We should not conceal potential weaknesses in our scientific strategy or empirical approach; rather, we should emphasize their existence. Even if this disappoints individuals seeking easy answers, it is crucial to acknowledge these limitations. The Catalogue of Bias is an excellent resource that provides insight into various potential pitfalls and challenges encountered during research, which may sometimes be difficult to completely rule out.\n\n7.4.1 Confounding variables\nA confounding variable is a factor that was not accounted for or controlled in a study but has the potential to influence the results. In other words, the true effects of the treatment or intervention can be obscured or muddled by the presence of this variable.\n\n\n\n\n\n\nDo not ignore confounders!\n\n\n\n\n\nA confounding variable is not inherently problematic in regression analysis. It only becomes an issue if the analysis fails to account for its confounding effect in both the identification strategy and the execution of the regression. When the confounding effect is neglected, we refer to this as omitted variable bias. This bias is further detailed in Section 7.4.3.\n\n\n\nFor example, let’s consider a scenario where two groups of individuals are observed: one group took vitamin C daily, while the other group did not. Over the course of a year, the number of colds experienced by each group is recorded. It might be observed that the group taking vitamin C had fewer colds compared to the group that did not. However, it would be incorrect to conclude that vitamin C directly reduces the occurrence of colds. Since this study is observational and not a true experiment, numerous confounding variables are at play. One potential confounding variable could be the individuals’ level of health consciousness. Those who take vitamins regularly might also engage in other health-conscious behaviors, such as frequent handwashing, which could independently contribute to a lower risk of catching colds.\nTo address confounding variables, researchers employ control measures. The idea is to create conditions where confounding variables are minimized or eliminated. In the aforementioned example, researchers could pair individuals who have similar levels of health consciousness and randomly assign one person from each pair to take vitamin C daily (while the other person receives a placebo). Any differences observed in the number of colds between the groups would be more likely attributable to the vitamin C, compared to the original observational study. Well-designed experiments are crucial as they actively control for potential confounding variables.\nConsider another scenario where a researcher claims that eating seaweed prolongs life. However, upon reading interviews with the study subjects, it becomes apparent that they were all over 80 years old, followed a very healthy diet, slept an average of 8 hours per day, drank ample water, and engaged in regular exercise. In this case, it is not possible to determine whether longevity was specifically caused by seaweed consumption due to the presence of numerous confounding variables. The healthy diet, sufficient sleep, hydration, and exercise could all independently contribute to longer life. These variables act as confounding factors.\nA common error in research studies is to fail to control for confounding variables, leaving the results open to scrutiny. The best way to head off confounding variables is to do a well-designed experiment in a controlled setting. Observational studies are great for surveys and polls, but not for showing cause-and-effect relationships, because they don’t control for confounding variables.\nControl variables are usually variables that you are not particularly interested in, but that are related to the dependent variable. You want to remove their effects from the equation. A control variable enters a regression in the same way as an independent variable – the method is the same.\n\n\n\n\n\n\nTip 7.2\n\n\n\nNick Huntington-Klein (2025) offers Causal Inference Animated Plots on his homepage. Read this online section and consider the animated graphs.\n\n\n\nHuntington-Klein, N. (2025). Causal inference animated plots. online. https://www.nickchk.com/causalgraphs.html\n\n\n7.4.2 Other controls\n\n“Although it is possible to remove bias from an estimate of a causal path by controlling for a confounding variable, it is also easy to add bias to an estimate by controlling for a variable that is either not a confounder or does not block a confounding path” (Wysocki et al., 2022, p. 4)\n\nThe impact of statistical control varies depending on the type of control variable used. Cinelli et al. (2022) discuss various functional relationships and provides a comprehensive list of variables that researchers might consider controlling for. However, it is important to emphasize that incorporating more control variables does not always lead to less biased results.\n\nCinelli, C., Forney, A., & Pearl, J. (2022). A crash course in good and bad controls. Sociological Methods & Research, 53(3), 1071–1104. https://doi.org/10.1177/00491241221099552\n\n\n\n\n\n\n\nExercise 7.1 Identification with observational data\nWysocki et al. (2022) discuss how to identify effect using observational data using regression analysis. Using Directed Acyclic Graphs (DAGs), the paper discusses various types of variables associated with a variable of interest. Assume you are interested in the impact of a variable \\(X\\) on \\(Y\\).\nFor a regression analysis, how should you treat the following types of variables: (1) Confounder, (2) Mediator, (3) Collider, (4) Proxy, and (5) Independent.\nPlease visualize these relationships using DAGs and discuss how each of these variable types should be incorporated into a regression analysis.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA confounding variable is associated with both the independent variable (\\(X\\)) and the dependent variable (\\(Y\\)), potentially skewing the apparent relationship between them.\nConfounders should be included in the regression analysis as control variables. Failing to control for confounders can lead to biased estimates of the effect of \\(X\\) on \\(Y\\) (omitted variable bias).\n\n\n\n\n\n\n\n\n\nA mediator is a variable through which an independent variable influences a dependent variable. In this case, \\(X\\) affects the mediator \\(C\\), which in turn affects \\(Y\\).\nWhen considering mediators, you should not include both \\(X\\) and \\(C\\) in the regression model to understand the overall impact of \\(X\\) on \\(Y\\).\n\n\n\n\n\n\n\n\n\nA collider is a variable that is influenced by both \\(X\\) and \\(Y\\). Conditioning on a collider can create a spurious association.\nAvoid including colliders in the regression analysis. Including a collider can introduce bias and create a misleading relationship between \\(X\\) and \\(Y\\).\n\n\n\n\n\n\n\n\n\nA proxy variable serves as a substitute for the variable of interest, often because the actual variable is difficult to measure.\nProxies can be included in the regression analysis as long as they are empirically valid representations of the variable they are intended to substitute and as long as measurement errors don’t exist. However, there is no need to include it as \\(C\\) does not have and impact on \\(Y\\) and if there is a measurement errors it can worsen your estimates.\n\n\n\n\n\n\n\n\n\nAn independent is a variable that has no causal relation to either the outcome or predictor. Controlling for such a variable will have no systematic impact on the causal estimate.\n\n\n\n\n\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2).\n\n\n7.4.3 Omitted variable bias and ceteris paribus\nFrom the Gauss-Markov theorem we know that if the OLS assumptions are fulfilled, the OLS estimator is (in the sense of smallest variance) the best linear conditionally unbiased estimator (BLUE). However, OLS estimates can suffer from omitted variable bias when any regressor, X, is correlated with any omitted variable that matters for variable Y.\nFor omitted variable bias to occur, two conditions must be fulfilled:\n\nX is correlated with the omitted variable.\nThe omitted variable is a determinant of the dependent variable Y.\n\nIn regression analysis, “ceteris paribus” is a Latin phrase that translates to “all other things being equal” or “holding everything else constant.” It is a concept used to examine the relationship between two variables while assuming that all other factors or variables remain unchanged.\nWhen we say ceteris paribus in the context of regression analysis, we are isolating the effect of a specific independent variable on the dependent variable while assuming that the values of the other independent variables remain constant. By holding other variables constant, we can focus on understanding the direct relationship between the variables of interest.\nFor example, consider a regression analysis that examines the relationship between income (dependent variable) and education level (independent variable) while controlling for age, gender, and work experience. By stating ceteris paribus, we are assuming that age, gender, and work experience remain constant, and we are solely interested in understanding the impact of education level on income.\n\n\n\n\n\n\n\nExercise 7.2 Look at the Output\n\n\n\nFigure 7.5: Regression output\n\n\n\n\n\n\nIn Figure 7.5 you see an excerpt of a regression output taken from a statistical program named Stata. Some t-values and p-values are missing.\n\nCalculate the t-value of the coefficient mpg. Is the coefficient at a level of \\(\\alpha=0.05\\) statistically significant?\n\nIs the coefficient foreign at a level of \\(\\alpha=0.05\\) statistically significant?\n\nIs the constant at a level of \\(\\alpha=0.05\\) statistically significant?\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.3 Look at Stata Output\nIn Figure 7.6 you find two regression outputs from Stata. Try to interpret the p-values and the confidence intervals. How are the t-values calculated. Can you use the magic number 1.96 to say if a corresponding estimated coefficient is statistically significant, or not? Which estimated model is better?\n\n\n\nFigure 7.6: Stata regression output\n\n\n\n\nOutput A\n\n\n\n\n\nOutput B\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.4 Explain the weight (Solutions online)\nIn the following exercise you need to use the programming language R.\n\nWrite down your name, your matriculation number, and the date.\n\nSet your working directory.\n\nClear your global environment.\n\nLoad the following package: tidyverse\n\n\nlibrary(\"tidyverse\")\n\nThe following table stems from a survey carried out at the Campus of the German Sport University of Cologne at Opening Day (first day of the new semester) between 8:00am and 8:20am. The survey consists of 6 individuals with the following information:\n\n\n\nTable 7.1: Dataset collected in Cologne\n\n\n\n\n\nid\nsex\nage\nweight\ncalories\nsport\n\n\n\n\n1\nf\n21\n48\n1700\n60\n\n\n2\nf\n19\n55\n1800\n120\n\n\n3\nf\n23\n50\n2300\n180\n\n\n4\nm\n18\n71\n2000\n60\n\n\n5\nm\n20\n77\n2800\n240\n\n\n6\nm\n61\n85\n2500\n30\n\n\n\n\n\n\nData Description:\n\nid: Variable with an anonymized identifier for each participant.\nsex: Gender, i.e., the participants replied to be either male (m) or female (f).\nage: The age in years of the participants at the time of the survey.\nweight: Number of kg the participants pretended to weight.\ncalories: Estimate of the participants on their average daily consumption of calories.\nsport: Estimate of the participants on their average daily time that they spend on doing sports (measured in minutes).\n\nWhich type of data do we have here? (Panel data, repeated cross-sectional data, cross-sectional data, time Series data)\nStore each of the five variables in a vector and put all five variables into a dataframe with the title df. If you fail here, read in the data using this line of code:\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/hubchev/courses/main/dta/df-calories.csv\")\n\n\nShow for all numerical variables the summary statistics including the mean, median, minimum, and the maximum.\nShow for all numerical variables the summary statistics including the mean and the standard deviation, separated by male and female. Use therefore the pipe operator.\nSuppose you want to analyze the general impact of average calories consumption per day on the weight. Discuss if the sample design is appropriate to draw conclusions on the population. What may cause some bias in the data? Discuss possibilities to improve the sampling and the survey, respectively.\nThe following plot visualizes the two variables weight and calories. Discuss what can be improved in the graphical visualization.\n\n\n\n\nFigure 7.7: Weight vs. Calories\n\n\n\n\n\n\n\nCreate a scatterplot matrix to visualize relationships between all numerical variables in the dataset.\nCalculate the Pearson Correlation Coefficient for the following pairs of variables:\n\ncalories and sport\nweight and calories\n\nThis will help in understanding the strength and direction of the linear relationship between these variables.\nGenerate a scatterplot with weight on the y-axis and calories on the x-axis. Include a linear fit to the data and label the points with the sex variable. This visualization can provide insights into the relationship between calorie consumption and weight, differentiated by gender.\nEstimate the following regression specification using the Ordinary Least Squares (OLS) method:\n\n\\[\nweight_i=\\beta_0+\\beta_1 calories_i+ \\epsilon_i\n\\]\n\n# OLS Regression\nreg_base &lt;- lm(weight ~ calories, data = df)\nsummary(reg_base)\n\n\nInterpret the results. In particular, interpret how many kg the estimated weight increases—on average and ceteris paribus—if calories increase by 100 calories. Additionally, discuss the statistical properties of the estimated coefficient \\(\\hat{\\beta_1}\\) and the meaning of the Adjusted R-squared.\nOLS estimates can suffer from omitted variable bias. State the two conditions that need to be fulfilled for omitted bias to occur.\nDiscuss potential confounding variables that may cause omitted variable bias. Given the dataset above how can some of the confounding variables be controlled for?\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.5 Explain the weight of students\nPlease consider the lecture notes Huber (2024) and the exercise that you find here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuber, S. (2024). How to use R for data science: Lecture notes. https://hubchev.github.io/ds/\n\n\n\n\n\n\n\nExercise 7.6 Causal inference and animated plots\nNick Huntington-Klein (2023) has created wonderful animated graphs that give great and quick insights into how causal inference works. Please read his online chapter on Causal Inference Animated Plots and discuss\n\nwhat he means when he speaks of closing the back-door path and controlling for,\nwhat methods exist to close the back-door path, and\nwhy it is sometimes necessary to omit variables from an estimated regression model.\n\n\n\n\n\n\nHuntington-Klein, N. (2023). The effect: An introduction to research design and causality. CRC Press. https://theeffectbook.net",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "regression.html#difference-in-difference",
    "href": "regression.html#difference-in-difference",
    "title": "7  Regression analysis",
    "section": "7.5 Difference in difference",
    "text": "7.5 Difference in difference\n\n\n\nFigure 7.8: David Card (*1956)\n\n\n\n\n\n\n\n\n\n\n\nSource: https://davidcard.berkeley.edu.\n\n\n\nDavid Card is one of the most influential labor economist of the 20th century and Nobel laureate of 2021. He is well-known for his research on the effects of the minimum wage on employment, which challenged the traditional view that increasing the minimum wage leads to a decrease in employment. In his article Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania (Card & Krueger, 1994) he and Alan Krueger (1960-2019) used a natural experiment to examine the effect of an increase in the minimum wage on employment. In particular, they identified a treatment group (restaurants in New Jersey) and a control group (restaurants in eastern Pennsylvania) to measure the effect of increasing the minimum wage that was increased in New Jersey but not in Pennsylvania. This increase did not lead to a decrease in employment, which contradicted the widely held view that increasing the minimum wage would lead to job loss. The empirical method that they used is called difference in difference and we discuss it in the following section.\n\nCard, D., & Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in new jersey and pennsylvania. The American Economic Review, 84(4), 772–793.\nThe difference in difference (DiD) method allows to estimate the causal effect of a treatment or intervention. In particular, it is popular to study the impact of policy changes and other interventions on a specific outcome of interest.\nThe basic idea behind the DiD method is to compare the change in an outcome variable between a treatment group and a control group over time. The treatment group is the group that is exposed to the intervention or treatment, while the control group is a group that is not exposed to the intervention. The difference in the change in the outcome variable between the two groups is then used to estimate the causal effect of the intervention.\nTo use the DiD method, researchers typically collect data on the outcome variable of interest for both the treatment and control groups before and after the intervention. This data is then used to calculate the difference in the change in the outcome variable between the two groups.\nFor example, if a study aims to examine the effect of a new policy on the employment rate, it should collect data on the employment rate for a group of individuals living in a region where the policy was implemented, and for a group living in a similar region where the policy was not implemented. The study can then compare the change in the employment rate for the two groups, before and after the implementation of the policy. The difference in the change in the employment rate between the two groups can be used to estimate of the causal effect of the policy on employment.\nIt is important to note that DiD assumes that there are no other factors that could be affecting the outcome variable of interest and that the treatment and control groups are similar in all ways except for the intervention. To control for these assumptions researchers can use statistical techniques such as matching to ensure that treatment and control groups are similar before the intervention.\nDiD is useful when we only have observational data and in situations where it is not possible or ethical to randomly assign individuals to a treatment or control group, for example, in the case of policy changes.\n\n\n\n\n\n\nTip 7.3\n\n\n\nDifferences-in-Differences and Rubin causal model\n\n\n\nFigure 7.9: Differences-in-Differences\n\n\n\n\n\n\n\n\n\nFigure 7.10: Tolerate time-invariant unobserved confounding\n\n\n\n\n\n\nFigure 7.9 and Figure 7.10 stem from a video of Brady Neal’s lecture on Difference-in-Differences. Please watch this video.\n\n\n\n\n\n\n\n\n\nExercise 7.7 Mastering DiD with Joshua Angrist\nWatch the video Introduction to Differences-in-Differences:\n\n\nThe video is part of a course called Mastering Econometrics with Joshua Angrist (MIT) produced by Marginal Revolution University. In it, Josh Angrist (see Figure 7.11) introduces differences within differences using one of the worst economic events in history: the Great Depression.\n\n\n\nFigure 7.11: Josh Angrist (*1960): Nobel Prize winner of economics in 2021\n\n\n\n\n\n\n\n\n\n\n\nSource: Youtube.\n\n\n\n\nIn the video, the treatment being examined is:\n\nBank failure.\n“Easy” money.\n“Tight” money.\nDifferences-in-Differences.\nNone of the above.\n\nIf the treatment were effective, which outcome would we expect to observe?\n\nFewer bank failures.\nIncreased bank failures.\nContinued parallel trends.\nNo differences in any variables unrelated to bank failure.\nNone of the above.\n\nPractically, how is DD (Differences-in-Differences) typically implemented?\n\nNon-parametric statistical techniques.\nRandomized trials.\nRegression analysis.\nInstrumental variables.\nNone of the above.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSolution to exercise Exercise 7.7\nAnswers: 1. b), 2. a), 3. c)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression analysis</span>"
    ]
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "8  Out-of-lab experiments",
    "section": "",
    "text": "8.1 Natural experiments\nIn Section 4.4, I introduced how experiments can be used to identify causes of effects and measure the impact of those causes using randomized controlled trials (RCTs). However, my previous description of experiments conducted outside of a laboratory setting—where researchers cannot as easily and precisely manipulate the independent variable—was brief. In this section, I will provide more detailed explanations.\nIn social science, a natural experiment is a research design that exploits naturally occurring circumstances or events to study the effects of an intervention or treatment. In these experiments, the treatment is not manipulated by the researcher, but is instead determined by exogenous, or external, factors. Exogenous variations refer to changes in the independent variable that are not caused by the researcher’s actions but instead occur naturally or through some external factor. These variations are often unpredictable and occur without the intervention of the researcher, making them an ideal source of variation to study the causal effects of a treatment or intervention. One example of a natural experiment is the partition of Germany after World War II, which created two economies that were initially similar but experienced vastly different economic and institutional environments. Another example is the introduction of a new policy or technology in one state or country but not in another, allowing for a comparison of outcomes before and after the treatment. A natural experiment might involve comparing the outcomes of two groups of people who were exposed to different levels of air pollution due to a policy change or a natural disaster. In this case, the variation in air pollution levels is exogenous, since it is not controlled by the researcher but rather determined by external factors.\nBy leveraging these exogenous variations, researchers can better estimate the causal effects of an intervention or treatment, and provide evidence for policy-makers to make more informed decisions. In the following, we will get known to some studies that are based on natural experiments.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Out-of-lab experiments</span>"
    ]
  },
  {
    "objectID": "experiments.html#natural-experiments",
    "href": "experiments.html#natural-experiments",
    "title": "8  Out-of-lab experiments",
    "section": "",
    "text": "Exercise 8.1 Natural experiments in research\n\nThink of other natural experiments that can be scientifically exploited.\nDownload Sieweke & Santoni (2020), see here.\nRead Sieweke & Santoni (2020, sec. 3.1) and study Sieweke & Santoni (2020, table V.).\n\n\n\n\n\n\nSieweke, J., & Santoni, S. (2020). Natural experiments in leadership research: An introduction, review, and guidelines. The Leadership Quarterly, 31(1), 101338.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Out-of-lab experiments</span>"
    ]
  },
  {
    "objectID": "experiments.html#case-study-bombing-of-japan",
    "href": "experiments.html#case-study-bombing-of-japan",
    "title": "8  Out-of-lab experiments",
    "section": "8.2 Case study: Bombing of Japan",
    "text": "8.2 Case study: Bombing of Japan\nIn their article “Bones, Bombs, and Break Points: The Geography of Economic Activity,” Davis & Weinstein (2002) explore another natural experiment that has shaped the economic geography of the world: the natural endowment of different regions with physical and institutional factors that affect their productivity and attractiveness to economic activity. Using a spatial econometric model, they test the relative importance of three such factors: climate, natural resources, and political borders. They find that political borders, such as the ones that emerged from colonialism or ethnic conflict, have the strongest effect on economic activity, even after controlling for other factors. This has important implications for policy, as it suggests that changing the institutional environment of a region can have a significant impact on its economic performance.\nOverall, Davis and Weinstein’s article demonstrates the power of natural experiments to shed light on important economic questions and inform policy debates. By examining the historical and geographical factors that have shaped economic activity around the world, they offer valuable insights into the mechanisms that drive economic growth and inequality.\n\n\n\nFigure 8.1: Paul Krugman *1953\n\n\n\nSource: https://commons.wikimedia.org/wiki/File:Paul_Krugman-press_conference_Dec_07th,_2008-8.jpg.\n\n\n\nBefore you read this article, let me explain that the theory which this article elaborates and tests goes back to the 2008 nobel-prize winner Paul Krugman (*1953) who founded the so-called New Economic Geography (NEG). Here is an excerpt of how the Royal Swedish Academy of Sciences summarizes Krugman’s contribution to the field (The Royal Swedish Academy of Sciences, 2008, p. 3):\n\nThe Royal Swedish Academy of Sciences. (2008). The prize in economic sciences 2008. https://www.nobelprize.org/prizes/economic-sciences/2008/popular-information/\n\nEconomic geography deals not only with what goods are produced where, but also with the distribution of capital and labor over countries and regions. The approach Krugman used in his foreign trade theory – the assumption of economies of scale in production and a preference for diversity in consumption – was also found to be appropriate for analyzing geographical issues. This allowed Krugman to integrate two disparate fields in a cohesive model.\nThe embryo of the theory which would come to be called the ``new economic geography’’ had already appeared in Krugman’s 1979 article. In the final pages, he asks what would happen if foreign trade became impossible, for instance due to excessively high transport costs or other obstacles. His line of reasoning is as follows. If two countries are exactly alike, then welfare will be the same in both countries. But if the countries are alike in all respects except that one of them has a slightly larger population than the other, then the real wages of labor will be somewhat higher in the country with more inhabitants. The reason is that firms in the more highly populated country can make better use of economies of scale, which implies lower prices to consumers and/or greater diversity in the supply of goods. This, in turn, enhances the welfare of consumers. As a result, labor, i.e., consumers, will tend to move to the country with more inhabitants, thereby increasing its population. Real wages and the supply of goods will then continue to increase even more in that country, thereby giving rise to further migration, and so on.\nTwelve years would pass, however, before Krugman reconsidered these ideas. In an article published in 1991, he developed these concepts into a comprehensive theory of location of labor and firms. Here, he assumes that although trade is possible, it is obstructed due to transport costs. Otherwise, labor is free to move to the country or region which can offer the highest welfare, in terms of real wages and diversity of goods. Firms’ location decisions imply a trade-off between utilizing economies of scale and saving on transport costs.\nConcentration or Decentralization?\nThe above considerations evolved into the so-called core-periphery model, which shows that the relation between economies of scale and transport costs can result in either concentration or decentralization of communities. Under certain conditions, the forces which contribute to concentration will dominate. Regional imbalances arise and most of the population will be concentrated in a high-technology core, whereas a small minority will inhabit the periphery and live off agriculture. Such a mechanism could underlie the explosive urbanization witnessed today throughout the world, with rapidly growing megacities surrounded by increasingly depopulated rural areas. This is not necessarily the only possibility, however. Under different conditions, the forces which give rise to decentralization will dominate. This promotes somewhat more balanced development. Krugman’s model can be used to account for the mechanisms at work in both directions. For example, his model indicates that declining transport costs easily generate concentration and urbanization – which seems particularly noteworthy since transport costs have exhibited a declining trend throughout the twentieth century.\n\nNumerous research papers inspired by Krugman’s New Economic Geography (NEG) focus on the origins and implications of the so-called first and second nature effects. These effects are used to explain the uneven distribution of economic activity both across countries and within regions of a country. Two primary explanations have been investigated: (1) Fundamentals, which refer to differences in the fundamental productivity of locations, and (2) Agglomeration forces, which are related to the proximity of economic agents that boost productivity and make a location more attractive.\nThese two mechanisms are obviously not exclusive and can both operate simultaneously. The key empirical question is to what extent observed patterns of economic activity are explained by these two mechanisms. Understanding whether fundamentals or agglomeration forces are responsible for the pattern of economic activity has significant implications for the persistence of spatial equilibria and policy making.\nFor instance, suppose only agglomeration forces are at play. In that case, the location of economic activity is relatively arbitrary, and a particular location is attractive mainly because other workers are choosing to locate there. This phenomenon is similar to selecting a nightclub: club A is crowded and everybody wants to be there only because it was the club had attracted the first person that night.\nIn contrast, if only fundamentals are in operation, the distribution of activity is determined by the distribution of these underlying factors. When agglomeration forces dominate fundamentals, the spatial distribution of activity becomes a matter of political interest. For example, regional policies can aim to move the distribution of economic activity between different equilibria. Using a temporary subsidy, regions can try to attract a ‘critical mass’ of economic activity. Once established, this critical mass will make the location more attractive, even when the subsidy has ended.\nThis approach is similar to nightclub policies, such as offering free entry or other discounts to the first people who are searching for a club. These incentives are an attempt to attract a critical mass of party-goers, making the nightclub more attractive and increasing the likelihood that other party-goers will choose the same club later.\n\n\n\n\n\n\n\nExercise 8.2 The impact of nuclear bombs on agglomerations\nBackground: The bombing of Japan during the Second World War devastated 66 cities that were targeted, leading to the destruction of approximately half of their housing stock and the deaths of around 300,000 Japanese. Although Hiroshima and Nagasaki are more well-known due to the nuclear bombs that were dropped on them, the majority of cities in the sample suffered little to no destruction, including several large cities. Despite the scale of the bombing, it was clearly a temporary shock that did not change the fundamental attractiveness of locations. However, the nuclear radiation in Hiroshima and Nagasaki could be considered an exception.\nData: The data set used in the paper consists of information on 303 Japanese cities with a population exceeding 30,000 in 1925. Population figures are recorded every five years, with the exception of the 1945 census, which took place in 1947. One way to gauge the intensity of the shocks experienced by the cities is by looking at the number of dead or missing residents.\n\nRead Davis & Weinstein (2002) which can be downloaded here and explain the implications of Figure 8.2:\n\n\n\n\nFigure 8.2: Two figures from Davis & Weinstein (2002)\n\n\n\n\n\n\nFigure 1 of Davis & Weinstein (2002)\n\n\n\n\n\n\n\n\n\nFigure 2 of Davis & Weinstein (2002)\n\n\n\n\n\n\n\n\nThe results of Davis and Weinstein (2002) show a striking persistence of city size even after terrible wartime destruction. This has important implications for attempts to use regional policy to shift economic activity between spatial equilibria. The following quote sums it up well: &gt; “An important practical question, then, is whether such spatial catastrophes are theoretical curiosa or a central tendency in the data. Our results provide an unambiguous answer: Even nuclear bombs have little effect on relative city sizes over the course of a couple of decades. The theoretical possibility of spatial catastrophes due to temporary shocks is not a central tendency borne out in the data.” Davis & Weinstein (2002, p. 1284)\nThe use of bombing as a temporary shock, however, can be seen as problematic: Although bombing causes casualties and destroys housing and social structures, legal property rights and operating licenses are not destroyed. Therefore, rebuilding cities may prove less burdensome than building new settlements from scratch. Moreover, even after a nuclear explosion, functioning infrastructure may still be in place, which could make rebuilding at the old site less costly.\n\nRead Bleakley & Lin (2012) and discuss how this is confronting the insights from Davis & Weinstein (2002). It is freely available here\n\n\n\n\n\n\nDavis, D. R., & Weinstein, D. E. (2002). Bones, bombs, and break points: The geography of economic activity. American Economic Review, 92(5), 1269–1289. http://ideas.repec.org/a/aea/aecrev/v92y2002i5p1269-1289.html\n\nBleakley, H., & Lin, J. (2012). Portage and path dependence. The Quarterly Journal of Economics, 127(2), 587–644.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Out-of-lab experiments</span>"
    ]
  },
  {
    "objectID": "experiments.html#field-experiments-would-you-work-more-if-wages-are-high",
    "href": "experiments.html#field-experiments-would-you-work-more-if-wages-are-high",
    "title": "8  Out-of-lab experiments",
    "section": "8.3 Field experiments: Would you work more if wages are high?",
    "text": "8.3 Field experiments: Would you work more if wages are high?\nUnlike laboratory experiments, which are conducted in a controlled environment, field experiments are conducted in real-world settings, such as schools, workplaces, and communities. They allow for testing causality by controlling the independent variable while observing the dependent variable in a naturalistic setting. They are a valuable tool for testing the effectiveness of policies and interventions in real-world situations. The level of external validity is usually much higher than alternative methods, meaning that the results can be generalized to other similar contexts beyond the specific setting where the experiment was conducted. In addition, field experiments can help identify unintended consequences of policies or interventions that might not be observable in laboratory experiments or observational studies.\nAnother advantage of field experiments is that they can be used to test theories in contexts where observational studies may be limited. For example, a theory may predict that a certain policy or intervention will have a specific effect, but it may be difficult to test this theory through observational studies due to confounding variables or selection bias.\nHowever, field experiments can be costly and time-consuming to conduct. Please give Harrison & List (2004) article a quick read, as it explains the nature and advantages of field experiments well.\n\nHarrison, G. W., & List, J. A. (2004). Field experiments. Journal of Economic Literature, 42(4), 1009–1055.\n\n\n\n\n\n\n\nExercise 8.3 Field experiments in research\n\nRead Fehr & Goette (2007). Summarize the results.\nExplain the identification strategy and the experimental design.\nRead Bandiera et al. (2011). Can you think of field studies that organizations could conduct to improve their business?\n\n\n\n\n\n\n\n\nBandiera, O., Barankay, I., & Rasul, I. (2011). Field experiments with firms. Journal of Economic Perspectives, 25(3), 63–82.\n\nFehr, E., & Goette, L. (2007). Do workers work more if wages are high? Evidence from a randomized field experiment. American Economic Review, 97(1), 298–317.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Out-of-lab experiments</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bandiera, O., Barankay, I., & Rasul, I. (2011). Field experiments\nwith firms. Journal of Economic Perspectives, 25(3),\n63–82.\n\n\nBékés, G., & Kézdi, G. (2021). Data analysis for business,\neconomics, and policy. Cambridge University Press.\n\n\nBergstrom, C. T., & West, J. D. (2021). Calling bullshit: The\nart of skepticism in a data-driven world. Penguin Books.\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in\ngraduate admissions: Data from Berkeley: Measuring bias is\nharder than is usually assumed, and the evidence is sometimes contrary\nto expectation. Science, 187(4175), 398–404.\n\n\nBleakley, H., & Lin, J. (2012). Portage and path dependence. The\nQuarterly Journal of Economics, 127(2), 587–644.\n\n\nCard, D., & Krueger, A. B. (1994). Minimum wages and employment: A\ncase study of the fast-food industry in new jersey and pennsylvania.\nThe American Economic Review, 84(4), 772–793.\n\n\nChivers, T., & Chivers, D. (2021). How to read numbers: A guide\nto statistics in the news (and knowing when to trust them).\nWeidenfeld & Nicolson.\n\n\nCinelli, C., Forney, A., & Pearl, J. (2022). A crash course in good\nand bad controls. Sociological Methods & Research,\n53(3), 1071–1104. https://doi.org/10.1177/00491241221099552\n\n\nCunningham, S. (2021). Causal inference: The mixtape. Accessed\nJanuary 30, 2023; Yale University Press. https://mixtape.scunning.com/\n\n\nCurtis, L. H., Hoffman, M. N., Califf, R. M., & Hammill, B. G.\n(2021). Life expectancy and voting patterns in the 2020 US\npresidential election. SSM-Population Health, 15,\n100840.\n\n\nDavis, D. R., & Weinstein, D. E. (2002). Bones, bombs, and break\npoints: The geography of economic activity. American Economic\nReview, 92(5), 1269–1289. http://ideas.repec.org/a/aea/aecrev/v92y2002i5p1269-1289.html\n\n\nFehr, E., & Goette, L. (2007). Do workers work more if wages are\nhigh? Evidence from a randomized field experiment.\nAmerican Economic Review, 97(1), 298–317.\n\n\nFeynman, R. P. (1985). Surely you’re joking, Mr.\nFeynman! Adventures of a curious character. W.W.\nNorton.\n\n\nHanck, C., Arnold, M., Gerber, A., & Schmelzer, M. (2020).\nIntroduction to econometrics with R. University of\nDuisburg-Essen. www.econometrics-with-r.org\n\n\nHarford, T. (2020). How to make the world add up: Ten rules for\nthinking differently about numbers. The Bridge Street Press.\n\n\nHarrison, G. W., & List, J. A. (2004). Field experiments.\nJournal of Economic Literature, 42(4), 1009–1055.\n\n\nHite, S. (1976). The hite report. A nationwide study of female\nsexuality. New York: Dell.\n\n\nHuber, S. (2024). How to use R for data science:\nLecture notes. https://hubchev.github.io/ds/\n\n\nHuff, D. (1954). How to lie with statistics. WW Norton &\ncompany.\n\n\nHuntington-Klein, N. (2023). The effect: An introduction to research\ndesign and causality. CRC Press. https://theeffectbook.net\n\n\nHuntington-Klein, N. (2025). Causal inference animated plots.\nonline. https://www.nickchk.com/causalgraphs.html\n\n\nHurston, Z. N. (2010). Dust tracks on a road. HarperCollins.\n\n\nIllowsky, B., & Dean, S. (2018). Introductory statistics.\nOpenstax. https://openstax.org/details/books/introductory-statistics\n\n\nJones, B. (2020). Avoiding data pitfalls: How to steer clear of\ncommon blunders when working with data and presenting analysis and\nvisualizations. John Wiley & Sons.\n\n\nKeele, L. (2015). The statistics of causal inference: A view from\npolitical methodology. Political Analysis, 23(3),\n313–335.\n\n\nLane, D. M. (2023). Introduction to statistics: Online statistics\neducation: A multimedia course of study. Accessed January 30, 2023;\nOnline Statistics Education: A Multimedia Course of Study. http://onlinestatbook.com\n\n\nLippert, G., & Sapy, B. (2003). Relation between the domestic\ndogs’ well-being and life expectancy statistical essay: Essay for the\nPrince Laurent Foundation Price. https://www.cavalierhealth.org/images/Lippert_Sapy_Domestic_Dogs_Life_Expectancy.pdf\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business\nReview, 85(6), 71–81. https://hbr.org/2007/06/how-successful-leaders-think\n\n\nMatthews, R. (2000). Storks deliver babies (p= 0.008). Teaching\nStatistics, 22(2), 36–38.\n\n\nNeal, B. (2020). Introduction to causal inference from a machine\nlearning perspective: Course lecture notes. Accessed January 30,\n2023. https://www.bradyneal.com/Introduction_to_Causal_Inference-Dec17_2020-Neal.pdf\n\n\nNosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D.,\nBreckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G.,\net al. (2015). Promoting an open research culture. Science,\n348(6242), 1422–1425.\n\n\nPaldam, M. (2021). Methods used in economic research: An empirical study\nof trends and levels. Economics, 15(1), 28–42.\n\n\nPeng, R. D. (2011). Reproducible research in computational science.\nScience, 334(6060), 1226–1227.\n\n\nSieweke, J., & Santoni, S. (2020). Natural experiments in leadership\nresearch: An introduction, review, and guidelines. The Leadership\nQuarterly, 31(1), 101338.\n\n\nSpiegelhalter, D. (2019). The art of statistics: Learning from\ndata. Penguin UK.\n\n\nTaddy, M. (2019). Business data science: Combining machine learning\nand economics to optimize, automate, and accelerate business\ndecisions (1st ed.). McGraw Hill Education.\n\n\nThe Royal Swedish Academy of Sciences. (2008). The prize in economic\nsciences 2008. https://www.nobelprize.org/prizes/economic-sciences/2008/popular-information/\n\n\nThe Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred\nNobel. (2002). Nobel prize outreach AB 2024. Fri. 15 nov 2024.\nhttps://www.nobelprize.org/prizes/economic-sciences/2002/summary/\n\n\nWeymar, P. (1955). Konrad Adenauer: Die autorisierte\nBiographie. Kindler.\n\n\nWikipedia. (2024). Survivorship bias. https://en.wikipedia.org/wiki/Survivorship_bias\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical\ncontrol requires causal justification. Advances in Methods and\nPractices in Psychological Science, 5(2).",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "exams.html",
    "href": "exams.html",
    "title": "Appendix A — Past exams",
    "section": "",
    "text": "Please feel free to download a collection of past exams here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Past exams</span>"
    ]
  },
  {
    "objectID": "example_questions.html",
    "href": "example_questions.html",
    "title": "Appendix B — Exam(ple) questions",
    "section": "",
    "text": "Exercise B.1 OLS method\nIn the course, we have discussed the OLS method.\n\nWhat does OLS stand for?\nExplain the OLS method for a simple linear regression using a two-dimensional plot.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nOLS stands for Ordinary Least Squares.\nIn the OLS method, the goal is to find the best-fitting line through a set of data points. More generally, it aims to make the best prediction by minimizing the sum of the squared residuals which are the vertical distances between each data point (\\(y_i\\)) and the predicted value on the line (\\(y_i^*\\)). The so-called residual is the difference between the observed value (\\(y_i\\)) and the value predicted by the regression line (\\(y_i^*\\)). The corresponding sketch is shown in Figure 7.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise B.2 Correlation\nFigure B.1 shows a screenshot of Josh Starmer’s explanation of the correlation coefficient. The video was linked in the lecture notes and watched during the lecture.\n\n\n\nFigure B.1: Pearson’s Correlation, Clearly Explained!!!\n\n\n\nSource: StatQuest with Josh Starmer\n\n\n\n\nExplain in detail why the covariance is divided by the denominator shown in the figure.\nDiscuss the interpretation of positive and negative values of the correlation coefficient. What does a correlation coefficient of zero mean?\nComment the following statement:\n\n“A correlation coefficient of zero is evdience that the the two variables are not associated.”\n\nExplain the limitations of the correlation coefficient for empirical research in great detail.\n\n\n\n\n\n\n\n\n\n\n\nSolution Exercise B.2\n\n\n\n\n\n\nThe correlation coefficient measures the direction of the linear relationship between two variables. It is calculated using the covariance of the variables divided by the product of their standard deviations.\nBy dividing the covariance by the product of the standard deviations of the two variables, we standardize the measure. This ensures that the correlation coefficient is dimensionless and falls within a fixed range regardless of the units of the original variables.\nIn contrast to the covariance, the correlation coefficient allows for comparison of the linear relationships between different pairs of variables because the denominator normalizes the covariance (from -1 to 1). Without this normalization, the magnitude of the covariance is influenced by the scales of the variables. Normalization removes the dependency on the scales, providing a pure measure of the linear relationship.\nIn summary, the covariance is divided by the product of the standard deviations to standardize, normalize, and make the measure dimensionless, allowing for meaningful and comparable interpretation of the linear relationship between the two variables.\nThe correlation coefficient provides a clearer interpretation: a correlation coefficient of 1 (-1) implies a perfect positive (negative) linear relationship, and 0 implies no linear relationship, that is, there is no consistent tendency for one variable to increase or decrease as the other variable changes.\nIn other words, a positive (negative) correlation coefficient indicates that as one variable increases, the other variable also tends to increase (decreases).\nIt’s essential to note that a correlation coefficient of zero does not necessarily mean there’s no relationship at all between the variables; it just implies that there’s no linear relationship. There could still be a nonlinear relationship or other forms of association between the variables.\nThe correlation coefficient is a limited indicator in empirical research, especially when the reasearch is interested in causality. In the following, I elaborate on some points in greater detail:\nCausation vs. Correlation: The correlation coefficient measures the strength and direction of a linear relationship between variables but does not imply causation. Just because two variables are correlated does not mean that one causes the other. It’s essential to conduct further research to establish causation.\nNonlinear Relationships: The correlation coefficient only measures linear relationships between variables. It may not capture more complex relationships. Therefore, it’s possible for variables to have a significant relationship that is not detected by the correlation coefficient.\nOutliers in the data can significantly influence the correlation coefficient. A few extreme data points can inflate or deflate the correlation coefficient, leading to misleading conclusions about the relationship between variables.\nConfounding Variables: Correlation does not account for the effects of confounding variables, which can influence the relationship between the variables of interest. Failing to control for confounders can lead to spurious correlations or misinterpretation of results.\nSample Size: The reliability of the correlation coefficient depends on the sample size. Small sample sizes may not provide enough statistical power to detect significant correlations accurately. Additionally, large sample sizes can sometimes yield statistically significant correlations that are not practically meaningful.\nThese limitations highlight the importance of interpreting correlation coefficients cautiously and considering additional factors when drawing conclusions in empirical research.\n\n\n\n\n\n\n\n\n\n\n\nExercise B.3 Regression\n\nIn class, we discussed the phrase `garbage in garbage out’. Explain what is meant by this.\nExplain what is meant with omitted variable bias and confounding variables in the multiple regression model.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Exam(ple) questions</span>"
    ]
  },
  {
    "objectID": "math_prelim.html",
    "href": "math_prelim.html",
    "title": "Appendix C — Mathematical preliminaries",
    "section": "",
    "text": "Please feel free to download and study my introduction for mathematics for economics here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Mathematical preliminaries</span>"
    ]
  }
]