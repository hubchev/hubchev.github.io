[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Business Leaders",
    "section": "",
    "text": "Preface\n\nAbout Data Science for Business Leaders\nThis course provides an overview of the tools and methods that data science offers to business leaders. Of course, we only scratch the surface on many topics. Many topics require more in-depth analysis to fully grasp the underlying principles and their benefits. In my opinion, business leaders often don’t understand every detail of the decisions they make. They can’t do that and often they don’t need to. However, they are good at identifying and prioritizing the key factors that can drive success. They have a sense of the essential elements that have a significant impact. Ultimately, leaders lead. They recognize, cultivate and manage the expertise of their employees and teammates. They orchestrate the knowledge around them, they lead their teams effectively and help them avoid critical missteps.\n\n\nAbout the cover and the logo of the notes\nI realize that having a visually appealing cover and logo does not directly help you become an exceptional leader. The images themselves do not teach you how to master the tools of data science or apply its methods effectively. However, I do believe that attractive visual elements appeal to many students and individuals. Therefore, as a teacher, I think it is wise to occasionally incorporate these elements into my notes. This approach may encourage you to continue reading and studying the material. By the way, it only took me a couple of minutes to create these two images using ChatGPT.\n\n\nAbout the notes\n\n\n\n\n\n\nA PDF version of these notes is available here..\n\n\n\nPlease note that while the PDF contains the same content, it has not been optimized for PDF format. Therefore, some parts may not appear as intended.\n\n\n\nThese notes aims to support my lecture at the HS Fresenius but are incomplete and no substitute for taking actively part in class.\nI hope you find this book helpful. Any feedback is both welcome and appreciated.\nThis is work in progress so please check for updates regularly.\nThese notes offer a curated collection of explanations, exercises, and tips to facilitate learning R without causing unnecessary frustration. However, these notes don’t aim to rival comprehensive textbooks.\nThese notes are published under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. This means it can be reused, remixed, retained, revised and redistributed as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license. \nI host the notes in a GitHub repo.\n\n\n\nStructure of these notes\n\n\nAbout the author\n\n\n\n\n\n\nContact:\n\n\n\n\n\nProf. Dr. Stephan Huber\nHochschule Fresenius für Wirtschaft & Medien GmbH\nIm MediaPark 4c\n50670 Cologne\nOffice: 4e OG-3\nTelefon: +49 221 973199-523\nMail: stephan.huber@hs-fresenius.de\nPrivate homepage: www.hubchev.github.io\nGithub: https://github.com/hubchev\n\n\n\n\n\n\nFigure 1: Prof. Dr. Stephan Huber\n\n\n\n\n\n\nI am a Professor of International Economics and Data Science at HS Fresenius, holding a Diploma in Economics from the University of Regensburg and a Doctoral Degree (summa cum laude) from the University of Trier. I completed postgraduate studies at the Interdisciplinary Graduate Center of Excellence at the Institute for Labor Law and Industrial Relations in the European Union (IAAEU) in Trier. Prior to my current position, I worked as a research assistant to Prof. Dr. Dr. h.c. Joachim Möller at the University of Regensburg, a post-doc at the Leibniz Institute for East and Southeast European Studies (IOS) in Regensburg, and a freelancer at Charles University in Prague.\nThroughout my career, I have also worked as a lecturer at various institutions, including the TU Munich, the University of Regensburg, Saarland University, and the Universities of Applied Sciences in Frankfurt and Augsburg. Additionally, I have had the opportunity to teach abroad for the University of Cordoba in Spain, the University of Perugia in Italy, and the Petra Christian University in Surabaya, Indonesia. My published work can be found in international journals such as the Canadian Journal of Economics and the Stata Journal. For more information on my work, please visit my private homepage at hubchev.github.io.\nI was always fascinated by data and statistics. For example, in 1992 I could name all soccer players in Germany’s first division including how many goals they scored. Later, in 2003 I joined the introductory statistics course of Daniel Rösch. I learned among others that probabilities often play a role when analyzing data. I continued my data science journey with Harry Haupt’s Introductory Econometrics course, where I studied the infamous Jeffrey M. Wooldridge (2002) textbook. It got me hooked and so I took all the courses Rolf Tschernig offered at his chair of Econometrics, where I became a tutor at the University of Regensburg and a research assistant of Joachim Möller. Despite everything we did had to do with how to make sense out of data, we never actually used the term data science which is also absent in the more 850 pages long textbook by Wooldridge (2002). The book also remains silent about machine learning or artificial intelligence. These terms became popular only after I graduated. The Harvard Business Review article by Davenport & Patil (2012) who claimed that data scientist is “The Sexiest Job of the 21st Century” may have boosted the popularity.\nThe term “data scientist” has become remarkably popular, and many people are eager to adopt this title. Although I am a professor of data science, my professional identity is more like that of an applied, empirically-oriented international economist. My hesitation to adopt the title “data scientist” also stems from the deep respect I have developed through my interactions with econometricians and statisticians. Considering their in-depth expertise, I feel like a passionate amateur.\nUltimately, I poke around in data to find something interesting. Much like my ten-year-old younger self who analyzed soccer statistics to gain a deeper understanding of the sport. The only thing that has changed since then is that I know more promising methods and can efficiently use tools for data processing and data analysis.\n\n\nTo students\nI’m not a business leader myself. I’m a professor, and like most of my peers, I don’t run a big company nor do I hold a top position in a multinational corporation. Actually, I believe that leading a company successfully full-time is not compatible with being a full-time professor, and vice versa. Despite this, in my role as a professor and study program director, I do have certain responsibilities and occasionally lead people, particularly students. This, however, doesn’t let me feel like I am a business leader. Is this necessary to teach you a course entitled “Data Science for Business Leaders”? I don’t think so: Firstly, if you want to become a business leader, do not pick a role model that works in academics like I do. Secondly, I believe I can provide you with valuable insights into data science to support you on the journey of becoming a business leader.    And thirdly, this course is not entitled “I am a business leader who teaches you data science”. Thus, this course is “for” business leaders and “for” you and not about me or my personality.\nEnjoy.\n\n\n\nFigure 1: Prof. Dr. Stephan Huber\n\n\n\nDavenport, T. H., & Patil, D. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(5), 70–76.\n\n\nWooldridge, J. M. (2002). Introductory econometrics: A modern approach. In Delhi: Cengage Learnng (2nd ed.). South-Western.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "esyl.html",
    "href": "esyl.html",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "1.1 Syllabus\nScope and Nature of Data Science\nEmerging Trends in a Data-Driven Business Environment\nData Science Process in Business\nData Literacy\nOverview of Data Science Methods\nIntroduction to Data Scientific Tools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-osly",
    "href": "esyl.html#sec-osly",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "Defining data science as an academic discipline (informatics, computer science, mathematics, statistics, econometrics, social science)\nImportance of data science in businesses\n\n\n\nEvolution of computers, computing, and data processing\nBusiness intelligence (performance marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\n\nWorkflows and data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve data)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview of programming languages (R, Python, SQL, …)\nOverview of no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-howchatgptwrote",
    "href": "esyl.html#sec-howchatgptwrote",
    "title": "1  (Extended) Syllabus",
    "section": "1.2 How I let ChatGPT wrote the extended syllabus",
    "text": "1.2 How I let ChatGPT wrote the extended syllabus\nHere are the seven prompts to ChatGPT 4.0:\n\nAct as a professor of data science. Write lecture notes for management students. The first chapter of the notes should contain the following:\n\nScope and nature of data science\n\nDefining data science as an academic discipline (informatics, computer science, mathematics,statistics, econometrics, social science)\nImportance of data science in businesses\n\n\nThe chapter of the lecture notes should contain the following:\n\nEmerging Trends in a Data-Driven Business Environment\n\nEvolution of computers, computing, and data processing\nBusiness Intelligence (Performance Marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\nThe third chapter of the lecture notes should contain the following:\n\nData science process in business\n\nWorkflows and Data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\nDo the same for the fourth section, which contains:\n\nData literacy\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve Kdata)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\nAnd the next section:\n\nOverview of data science methods\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\nAnd the final section:\n\nIntroduction to data scientific tools\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview on Programming languages (R, Phyton, SQL, …)\nOverview on no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)\n\n\nI am not happy with the third section. In particular the section “Workflows and Data Science Life Cycles”. Can you re-write it? Consider the learning objective a bit better, that is, summarize the strengths, weaknesses and use-cases of data science processes and workflows and demonstrate the ability to justify and explain tools and methods used along the process.\n\nI manually cut some parts of the last answer and pasted it into the respective section.\nI assembled everything by copy and paste it step-by-step into Quarto and render it. That was all I did to create the notes.\n\n\n\n\n\n\n\nExercise 1.1 What do think about the extended syllabus and my approach\nWhat do you think about the extended syllabus? Is it well-written? Does it include a lot of details or errors? What aspects do you think we should work on manually? How could my prompts be improved to yield better results?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-esylgpt",
    "href": "esyl.html#sec-esylgpt",
    "title": "1  (Extended) Syllabus",
    "section": "1.3 The ChatGPT generated extended syllabus",
    "text": "1.3 The ChatGPT generated extended syllabus\nThe following text was generated as described in Section 1.2 using OpenAI (2024).\n\n1.3.1 Scope and Nature of Data Science\nWelcome to the introductory chapter on Data Science, designed specifically for management students. In this chapter, we will explore the multifaceted discipline of data science, understanding its definitions, scope, and the pivotal role it plays in the business world today.\n\n1.3.1.1 Defining Data Science\nData Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It is a confluence of several disciplines including informatics, computer science, mathematics, statistics, econometrics, and social science. This integration allows for a comprehensive approach to solving complex problems, making informed decisions, and generating actionable insights through data analysis.\n\n1.3.1.1.1 Informatics\nIt is the study of computational systems and the way humans interact with these systems. In data science, informatics plays a critical role in the management and processing of data.\n\n\n1.3.1.1.2 Computer Science\nThis foundational pillar focuses on the development of algorithms and software that are used to process, store, and analyze data.\n\n\n1.3.1.1.3 Mathematics and Statistics\nMathematics provides the theoretical foundation for algorithms and models, while statistics is crucial for understanding and interpreting data. Together, they enable data scientists to perform quantitative analysis and predictive modeling.\n\n\n1.3.1.1.4 Econometrics\nEconometrics applies statistical methods to economic data, enabling the analysis of economic relationships. In data science, econometrics helps in forecasting economic trends and evaluating economic policies through data.\n\n\n1.3.1.1.5 Social Science\nThe incorporation of social science principles allows data scientists to understand human behavior and societal trends, which is invaluable for businesses looking to market their products or services effectively.\n\n\n\n1.3.1.2 Importance of Data Science in Businesses\nData Science has become a cornerstone in the business world, offering unprecedented opportunities for growth, efficiency, and innovation. Here’s how:\n\n1.3.1.2.1 Data-Driven Decision Making\nBusinesses can make more informed decisions by analyzing vast amounts of data. This leads to better strategic planning, operational efficiency, and competitive advantage.\n\n\n1.3.1.2.2 Customer Insights\nThrough data analysis, companies can gain deep insights into customer behavior, preferences, and trends. This information is critical for tailoring marketing strategies, improving customer experiences, and boosting customer loyalty.\n\n\n1.3.1.2.3 Risk Management\nData science enables businesses to identify and mitigate risks by analyzing historical data, detecting patterns, and predicting future outcomes. This is particularly useful in finance, insurance, and cybersecurity.\n\n\n1.3.1.2.4 Innovation and Product Development\nBy understanding market needs and customer feedback through data analysis, businesses can innovate and develop new products and services that meet or exceed customer expectations.\n\n\n1.3.1.2.5 Operational Efficiency\nData science can optimize business operations, reduce costs, and improve service delivery through the analysis of operational data, leading to streamlined processes and better resource management.\n\n\n1.3.1.2.6 Competitive Edge\nIn a data-driven world, businesses that leverage data science effectively can stay ahead of the curve, identifying trends and opportunities before their competitors.\nIn conclusion, data science is an essential academic discipline that integrates various fields to analyze and interpret complex data. Its importance in the business sector cannot be overstated, as it enables organizations to make informed decisions, understand their customers, manage risks, foster innovation, improve operations, and maintain a competitive edge in the market. As we delve deeper into this course, we will explore the methodologies, tools, and applications of data science in greater detail, equipping you with the knowledge and skills to leverage data science in your future management roles.\n\n\n\n\n1.3.2 Emerging Trends in a Data-Driven Business Environment\nIn this chapter, we explore the dynamic landscape of a data-driven business environment. The rapid evolution of technology and its integration into business processes have led to the emergence of new trends that are shaping the future of industries across the globe. Let’s dive into these trends and understand their significance.\n\n1.3.2.1 Evolution of Computers, Computing, and Data Processing\nThe journey from the first generation of computers to the modern era of quantum computing marks a significant evolution in technology. Initially, computers were large, expensive, and limited in their capabilities. Over the decades, advancements in semiconductor technology, the invention of the microprocessor, and the development of personal computers transformed computing into an accessible and essential tool for businesses. Today, cloud computing and edge computing have further revolutionized data processing, allowing for more efficient data storage, access, and analysis.\n\n\n1.3.2.2 Business Intelligence\nBusiness Intelligence (BI) refers to the use of data analysis in business to support decision-making processes. BI tools analyze historical and current data to provide actionable insights, helping businesses to improve their performance. Performance marketing, a subset of BI, focuses on analyzing marketing campaigns in real time to optimize marketing strategies and expenditures for better ROI.\n\n\n1.3.2.3 Artificial Intelligence, Machine Learning, Deep Learning, and Algorithms\nArtificial Intelligence (AI) and its subsets, Machine Learning (ML) and Deep Learning (DL), are at the forefront of technological innovation. AI involves creating systems capable of performing tasks that typically require human intelligence. ML and DL are about teaching computers to learn from data, improving their accuracy over time without being explicitly programmed. These technologies are transforming business operations, from customer service automation and predictive analytics to personalized marketing and decision-making processes.\n\n\n1.3.2.4 Big Data\nBig Data refers to the vast volumes of data generated every second from various sources like social media, business transactions, and IoT devices. The ability to process and analyze this data has unlocked new opportunities for businesses to gain insights into customer behavior, market trends, and operational efficiency. Big data analytics is now a crucial tool for strategic planning and competitive analysis.\n\n\n1.3.2.5 Internet of Things, Cloud Computing, Blockchain\n\nInternet of Things (IoT): IoT technology connects everyday devices to the internet, enabling them to send and receive data. This interconnectivity offers businesses real-time insights into their operations, asset tracking, and supply chain management.\nCloud Computing: Cloud computing provides businesses with scalable computing resources over the internet, facilitating remote data storage, processing, and management. It supports flexibility, reduces IT costs, and enhances collaboration.\nBlockchain: Blockchain technology offers a secure, decentralized platform for transactions. It’s particularly valuable for enhancing transparency, security, and efficiency in business operations, supply chain management, and financial services.\n\n\n\n1.3.2.6 Industry 4.0 and Remote Working\nIndustry 4.0, also known as the fourth industrial revolution, integrates digital technologies into manufacturing and industry, including IoT, AI, and robotics. It represents a shift towards smart, automated production processes and data exchange. Coupled with the rise of remote working, Industry 4.0 technologies enable businesses to operate more flexibly, with teams collaborating effectively from various locations, leveraging digital tools and platforms for communication and project management.\nIn conclusion, the evolution of technology and its integration into business practices have brought about significant changes in the way companies operate. From the way data is processed and analyzed to the automation of manufacturing processes and the flexibility of remote working, these emerging trends are shaping the future of a data-driven business environment. As we move forward, understanding and adapting to these trends will be crucial for businesses looking to thrive in the digital age.\n\n\n\n1.3.3 Data Science Process in Business\nThis chapter delves into the structured approach behind the application of data science in business settings. We will explore various data science workflows and life cycles that guide the process from raw data to actionable insights. Additionally, we will outline the different roles within a data science team and their contributions to this process.\n\n1.3.3.1 Workflows and Data Science Life Cycles\nData science projects in business environments follow structured workflows and life cycles to ensure that the analysis is efficient, reproducible, and scalable. Several frameworks guide these processes, each with its strengths and applications.\n\n1.3.3.1.1 OSEMN Framework\nOSEMN (Obtain, Scrub, Explore, Model, iNterpret) is a streamlined approach to data science projects:\n\nObtain: Acquiring the data from various sources.\nScrub: Cleaning the data to ensure it is accurate and usable.\nExplore: Analyzing the data to find patterns and relationships.\nModel: Applying statistical models to predict or classify data.\nInterpret: Drawing conclusions and making recommendations based on the model’s results.\n\n\nStrengths: The OSEMN (Obtain, Scrub, Explore, Model, iNterpret) framework is straightforward and easy to understand, making it accessible for teams of all skill levels. It covers the essential steps of a data science project in a logical sequence.\nWeaknesses: Its simplicity may overlook the complexity of certain stages, such as model validation or deployment.\nUse-Cases: Ideal for small to medium-sized projects where the primary goal is to gain insights from data through exploration and modeling.\n\n\n\n1.3.3.1.2 CRISP-DM\nCRISP-DM stands for Cross-Industry Standard Process for Data Mining. It’s a comprehensive framework that includes six phases:\n\nBusiness Understanding: Define the project objectives and requirements.\nData Understanding: Collect and explore the data.\nData Preparation: Clean and preprocess the data.\nModeling: Select and apply modeling techniques.\nEvaluation: Assess the model’s performance.\nDeployment: Implement the model in a real-world setting.\n\n\nStrengths: CRISP-DM (Cross-Industry Standard Process for Data Mining) is industry-agnostic and provides a detailed structure that includes understanding the business problem and deploying the solution. It encourages iterative learning and refinement.\nWeaknesses: Can be perceived as too rigid for projects requiring rapid development and deployment. The model doesn’t explicitly address the updating or maintenance of deployed solutions.\nUse-Cases: Suitable for projects that require close alignment with business objectives and thorough consideration of deployment strategies.\n\n\n\n1.3.3.1.3 Kanban\nKanban is a lean method to manage and improve work across human systems. In data science, it helps in visualizing work, limiting work-in-progress, and maximizing efficiency.\n\nStrengths: Kanban is highly flexible and promotes continuous delivery. It allows teams to adapt quickly to changes and prioritize tasks effectively.\nWeaknesses: Without strict stages or phases, projects might lack direction or oversight, potentially leading to inefficiencies.\nUse-Cases: Best for dynamic environments where priorities shift frequently and teams must remain agile to respond to business needs.\n\n\n\n1.3.3.1.4 TDSP (Team Data Science Process)\nTDSP is a standardized approach to data science projects that helps teams to improve quality and efficiency. It includes:\n\nStrengths: TDSP offers a structured approach with a strong emphasis on standardized documentation and project management methodologies, facilitating collaboration and scalability.\nWeaknesses: Its comprehensive nature might introduce overhead and slow down smaller projects.\nUse-Cases: Ideal for larger teams working on complex projects that require coordination across different roles and departments.\n\n\n\n\n1.3.3.2 Types of Data Science Roles\nIn a business environment, a data science team might consist of various specialized roles, each contributing uniquely to the data science process.\n\n1.3.3.2.1 Data Engineer\nFocuses on the design, construction, and maintenance of the systems that data analysts and data scientists use for their work. They ensure that data flows smoothly from source to database to analytics.\n\n\n1.3.3.2.2 Data Analyst\nWorks on processing and performing statistical analysis on existing datasets. They interpret the data to help the business make more informed decisions.\n\n\n1.3.3.2.3 Machine Learning Engineer\nDevelops algorithms and predictive models to solve specific business problems using machine learning techniques.\n\n\n1.3.3.2.4 Business Intelligence Analyst\nAnalyzes data to provide insights that help businesses with strategic planning. They use BI tools to convert data into understandable reports and dashboards.\n\n\n1.3.3.2.5 Database Administrator\nResponsible for managing, backing up, and ensuring the availability of the data stored in an organization’s databases.\n\n\n1.3.3.2.6 Data Product Manager\nOversees the development of data-driven products or services, ensuring that they meet the users’ needs and the business objectives.\nIn summary, the data science process in business involves a structured approach to turning data into actionable insights. This process is supported by various frameworks and relies on the collaboration of professionals in specialized roles. Understanding these aspects of data science is crucial for anyone looking to leverage this discipline in a business context.\n\n\n\n\n1.3.4 Data Literacy\nData literacy is the ability to read, understand, create, and communicate data as information. It encompasses a broad range of skills necessary for effectively working with data, from the initial stages of data collection to the final stages of analyzing and sharing findings. In this chapter, we will break down the conceptual framework of data literacy and explore its various components in detail.\n\n1.3.4.1 Conceptual Framework\nAt the heart of data literacy is a deep knowledge and understanding of how data can be used to make decisions, solve problems, and communicate ideas. This conceptual framework involves:\n\nUnderstanding the nature of data: Recognizing different types of data (quantitative vs. qualitative) and their sources.\nComprehending the applications of data: Knowing how data can be used in various contexts to derive insights and inform decisions.\n\n\n\n1.3.4.2 Data Collection\nThe first step in the data lifecycle involves identifying, collecting, and assessing data:\n\nIdentify: Determining the data needed to answer a question or solve a problem.\nCollect: Gathering data from various sources, whether they are existing datasets or new data collected through surveys, experiments, or observations.\nAssess: Evaluating the quality of the data, including its relevance, accuracy, and completeness.\n\n\n\n1.3.4.3 Data Management\nOnce data is collected, it must be managed effectively:\n\nOrganize: Arranging data in a structured format that facilitates analysis.\nClean: Removing errors or inconsistencies in the data.\nConvert: Transforming data into a format suitable for analysis.\nCurate: Selecting, annotating, and maintaining valuable data for current and future use.\nPreserve: Ensuring that data remains accessible and usable over time.\n\n\n\n1.3.4.4 Data Evaluation\nEvaluation is critical to understanding what the data signifies:\n\nPlan: Designing a methodology for analyzing the data.\nConduct: Performing the analysis using appropriate statistical methods and tools.\nEvaluate: Assessing the quality and reliability of the analysis.\nAssess: Interpreting the results in the context of the research question or business problem.\n\n\n\n1.3.4.5 Data Application\nThe final step involves applying the insights gained from data analysis:\n\nShare: Communicating findings to stakeholders through reports, presentations, or visualizations.\nReflect: Considering the implications of the results and how they can inform future actions.\nEvaluate results: Comparing findings with those from other studies or data analyses to draw broader conclusions.\nEthical considerations: Ensuring that the use of data respects privacy, confidentiality, and ethical standards.\nScientific standards: Adhering to rigorous standards of validity, reliability, and objectivity in data handling and analysis.\n\nIn summary, data literacy is a comprehensive set of skills that enable individuals to navigate the complex world of data from collection to application. By understanding and applying the concepts outlined in this chapter, individuals can enhance their ability to make informed decisions, solve problems, and communicate effectively using data.\n\n\n\n1.3.5 Overview of Data Science Methods\nData science encompasses a wide array of methods and techniques for analyzing data, drawing insights, and making predictions. This chapter provides an overview of some core data science methods, including data exploration, data mining, machine learning approaches, and various types of analyses.\n\n1.3.5.1 Data Exploration and Data Mining\n\n1.3.5.1.1 Data Exploration\nData exploration involves analyzing data sets to find initial patterns, characteristics, and points of interest without making any prior assumptions. It typically includes summarizing the main characteristics of the data through visualizations and statistics.\n\n\n1.3.5.1.2 Data Mining\nData mining is the process of discovering patterns and knowledge from large amounts of data. The data sources can include databases, data warehouses, the internet, and other sources. Data mining techniques include clustering, classification, regression, and association rule learning.\n\n\n\n1.3.5.2 Supervised and Unsupervised Learning\n\n1.3.5.2.1 Supervised Learning\nSupervised learning is a type of machine learning where the algorithm learns from labeled training data, helping to predict outcomes for unforeseen data. It is divided into two main categories: regression and classification.\n\n\n1.3.5.2.2 Unsupervised Learning\nUnsupervised learning involves training on data without labeled responses. The system tries to learn the patterns and the structure from the data without any supervision. Common unsupervised learning methods include clustering and dimensionality reduction.\n\n\n\n1.3.5.3 Regression and Classification\n\n1.3.5.3.1 Regression\nRegression methods are used to predict a continuous outcome variable based on one or more predictor variables. The goal is to find the relationship between variables and forecast an outcome. Linear regression is one of the most basic types of regression analysis.\n\n\n1.3.5.3.2 Classification\nClassification methods are used to predict or identify the category to which a new observation belongs. Examples include spam detection in email service providers and customer churn prediction.\n\n\n\n1.3.5.4 Predictive Analysis\nPredictive analysis uses statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. It’s used in various fields, including finance, healthcare, marketing, and weather forecasting, to make more informed decisions.\n\n\n1.3.5.5 Causal Analysis\nCausal analysis seeks to identify and understand the cause-and-effect relationships between variables. Unlike correlation, which merely indicates that two variables move together, causation establishes that a change in one variable is responsible for a change in another.\nIn conclusion, these methods and techniques form the backbone of data science, enabling professionals to extract valuable insights, make predictions, and inform decision-making processes. Understanding these methods is crucial for anyone looking to delve into data science or apply its principles in their field.\n\n\n\n1.3.6 Introduction to Data Scientific Tools\nThe practice of data science requires not only a solid understanding of theories and methodologies but also proficiency in a variety of tools and technologies. This chapter introduces essential tools for writing and publishing reports, collaborating in teams, programming, as well as no-code and low-code platforms, and development environments.\n\n1.3.6.1 Writing and Publishing Reports\n\n1.3.6.1.1 Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. Its simplicity and ease of conversion to HTML and other formats make it an ideal choice for writing and publishing reports, documentation, and articles.\n\n\n1.3.6.1.2 Quarto\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It enables users to create dynamic and reproducible reports and articles that can include executable code from various programming languages, such as R and Python.\n\n\n\n1.3.6.2 Collaborating in Teams Using a Version Control System\n\n1.3.6.2.1 Git\nGit is a distributed version control system that enables multiple developers to work together on the same project efficiently. It tracks changes in source code during software development, supporting collaboration and fostering code integrity.\n\n\n\n1.3.6.3 Overview of Programming Languages\n\n1.3.6.3.1 R\nR is a programming language and free software environment for statistical computing and graphics, widely used among statisticians and data miners.\n\n\n1.3.6.3.2 Python\nPython is a high-level, interpreted programming language known for its simplicity and versatility. It has a wide range of libraries for data analysis, machine learning, and data visualization, making it a popular choice in data science.\n\n\n1.3.6.3.3 SQL\nSQL (Structured Query Language) is the standard language for managing and manipulating relational databases. It allows users to query, update, and manage data.\n\n\n\n1.3.6.4 Overview of No-Code and Low-Code Tools for Data Science\n\n1.3.6.4.1 makeML\nA no-code platform for machine learning, makeML simplifies the process of training and deploying ML models without writing extensive code.\n\n\n1.3.6.4.2 PyCaret\nPyCaret is a low-code machine learning library in Python that automates machine learning workflows. It enables data scientists to perform end-to-end experiments quickly and efficiently.\n\n\n1.3.6.4.3 Rapidminer\nRapidminer is a data science platform that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics.\n\n\n1.3.6.4.4 KNIME\nKNIME is an open-source, graphical workbench for the entire analysis process: data access, data transformation, initial investigation, powerful predictive analytics, visualization, and reporting.\n\n\n\n1.3.6.5 Development Environments\n\n1.3.6.5.1 Unix-like Systems\nUnix-like operating systems, including Linux and macOS, provide powerful tools and environments for software development and data science.\n\n\n1.3.6.5.2 Containers\nContainers, such as Docker, allow for the packaging of applications and their dependencies in a virtual container that can run on any Linux server, enabling easy deployment and scalability.\n\n\n1.3.6.5.3 APIs\nApplication Programming Interfaces (APIs) enable different software applications to communicate with each other, facilitating data exchange and integration.\n\n\n1.3.6.5.4 Jupyter\nJupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text.\n\n\n1.3.6.5.5 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for coding, debugging, and visualizing data.\nIn summary, the array of tools and technologies available to data scientists is broad and varied, catering to different aspects of the data science workflow. From data manipulation and analysis to collaboration and report writing, mastering these tools is essential for effective data science practice.\n\n\n\n\nOpenAI. (2024). ChatGPT (April 18, 2024). Large language model. https://chat.openai.comt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "pitfalls.html",
    "href": "pitfalls.html",
    "title": "3  Data Pitfalls",
    "section": "",
    "text": "3.1 Epistemic Errors\nEpistemic errors occur when there are mistakes in our understanding and conceptualization of data. These errors arise from cognitive biases, misunderstandings, and incorrect assumptions about the nature of data and the reality it represents. Recognizing and addressing these errors is crucial for accurate data analysis and effective decision-making.\nOne significant type of epistemic error is the data-reality gap, which refers to the difference between the data we collect and the reality it is supposed to represent. For instance, a survey on customer satisfaction that only includes responses from a self-selected group of highly engaged customers may not accurately reflect the overall customer base. To avoid this pitfall, it is essential to ensure that your data collection methods are representative and unbiased, and to validate your data against external benchmarks or additional data sources.\nAnother common epistemic error involves the influence of human biases during data collection and interpretation. Known as the all too human data error, this occurs when personal biases or inaccuracies affect the data. An example would be a researcher’s personal bias influencing the design of a study or the interpretation of its results. To mitigate this, implement rigorous protocols for data collection and analysis, and consider using double-blind studies and peer reviews to minimize bias.\nInconsistent ratings can also lead to epistemic errors. This happens when there is variability in data collection methods, resulting in inconsistent or unreliable data. For example, different evaluators might rate the same product using different criteria or standards. To avoid this issue, standardize data collection processes and provide training for all individuals involved in data collection to ensure consistency.\nThe black swan pitfall refers to the failure to account for rare, high-impact events that fall outside regular expectations. Financial models that did not predict the 2008 financial crisis due to the unexpected nature of the events that led to it are an example of this error. To prevent such pitfalls, consider a wide range of possible outcomes in your models and incorporate stress testing to understand the impact of rare events.\nFalsifiability and the God pitfall involve the tendency to accept hypotheses that cannot be tested or disproven. This error might occur when assuming that a correlation between two variables implies causation without the ability to test alternative explanations. To avoid this, ensure that your hypotheses are testable and that you actively seek out potential falsifications. Use control groups and randomized experiments to validate causal relationships.\nTo avoid epistemic errors, critically assess your assumptions, methodologies, and interpretations. Engage in critical thinking by regularly questioning your assumptions and seeking alternative explanations for your findings. Employ methodological rigor by using standardized and validated methods for data collection and analysis. Engage with peers to review and critique your work, providing a fresh perspective and identifying potential biases. Finally, stay updated with the latest research and best practices in your field to avoid outdated or incorrect methodologies.\nUnderstanding and addressing epistemic errors can significantly improve the reliability and accuracy of your data analyses, leading to better decision-making and more trustworthy insights.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#epistemic-errors",
    "href": "pitfalls.html#epistemic-errors",
    "title": "3  Data Pitfalls",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the concept of epistemic errors and their impact on data analysis.\nIdentify and describe common types of epistemic errors.\nApply strategies to avoid epistemic errors in your data work.\n\n\n\n\n\n\n\n\n\n\n\nThe Data-Reality Gap\n\n\n\nThe difference between the data we collect and the reality it is supposed to represent.\n\n\n\n\n\n\n\n\n\nAll Too Human Data\n\n\n\nErrors introduced by human biases or inaccuracies during data collection and interpretation.\n\n\n\n\n\n\n\n\n\nInconsistent Ratings\n\n\n\nVariability in data collection methods that leads to inconsistent or unreliable data.\n\n\n\n\n\n\n\n\n\nThe Black Swan Pitfall\n\n\n\nThe failure to account for rare, high-impact events that fall outside the realm of regular expectations.\n\n\n\n\n\n\n\n\n\nFalsifiability and the God Pitfall\n\n\n\nThe tendency to accept hypotheses that cannot be tested or disproven.\n\n\n\n\n\n\n\n\n\n\n\nExercise 3.1  \n\n\n\nFigure 3.1: Bananas in various stages of ripeness\n\n\n\nSource: Jones (2020, p. 33)\n\n\n\n\nRate the ripeness level of the bananas pictured by Figure 3.1. Compare your assessment to that of a colleague and discuss any differences in your ratings. What might account for the variance in perception of the bananas’ ripeness between you and your colleague?\nSpecify how you rated the second and the last bananas on the ripeness scale?\nUpon reevaluation, it appears that the second and the last bananas are identical in ripeness. How would you justify your initial decision now? This scenario underscores an important lesson for interpreting polls and surveys: it illustrates how subjective assessments can lead to variance in results. It highlights the necessity of ensuring clarity and consistency in the criteria used for evaluations to minimize subjective discrepancies.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#technical-trespasses",
    "href": "pitfalls.html#technical-trespasses",
    "title": "3  Data Pitfalls",
    "section": "3.2 Technical trespasses",
    "text": "3.2 Technical trespasses\nThe second pitfall refers to the mishandling of data through a lack of technical expertise or the misapplication of statistical methods. It can lead to significant distortions in data analysis and result in flawed decisions. This might include using inappropriate statistical tests, excessive model complexity, or improper data manipulation. Understanding essential statistics and maintaining data integrity are vital to prevent these errors. Business leaders need to ensure that their teams have the requisite technical knowledge and to foster a culture of cooperation with data science experts to safeguard against these common technical blunders. This will ultimately support more reliable and actionable insights for strategic decision-making.\nHere are some examples things that frequently causes wrong conclusions when analyzing data:\n\n\n\n\n\n\nMisinterpreting correlation as causation\n\n\n\nA classic mistake is interpreting a correlation between two variables as evidence that one causes the other. Without proper experimental design or further statistical analysis, such claims can lead to ill-informed business decisions.\n\n\n\n\n\n\n\n\nInappropriate use of average\n\n\n\nAverages can be misleading when data is skewed or has outliers. For example, using the mean salary in a company where the CEO’s salary is drastically higher than the rest of the employees can give a false impression of the typical compensation.\n\n\n\n\n\n\n\n\nNeglecting the test assumptions\n\n\n\nApplying statistical tests without checking the necessary assumptions can invalidate the results. For instance, using a t-test on data that is not normally distributed or has unequal variances between groups might lead to incorrect conclusions.\n\n\n\n\n\n\n\n\nOverfitting predictive models\n\n\n\nCreating a model that performs extremely well on historical data but fails to predict future outcomes undermines its utility. Overfit models are often too complex, capturing noise rather than underlying trends in the training data.\n\n\n\n\n\n\n\n\nIgnoring the effect of sample size\n\n\n\nSmall samples can produce volatile or non-representative results; business decisions made on such shaky ground may not stand up when scaled to the full population or market.\n\n\n\n\n\n\n\n\nCherry-picking data\n\n\n\nSelecting data that supports a preconceived hypothesis, while ignoring data that doesn’t, compromises the objectivity of the analysis and can mislead stakeholders about the reality of the situation.\n\n\n\n\n\n\n\n\nImproper data visualization\n\n\n\nMisrepresenting data through poor chart choices or scales can mislead viewers. For example, truncating the y-axis can exaggerate trends, and using pie charts for too many categories can confuse rather than clarify.\n\n\n\n\n\n\n\n\n\nExercise 3.2 Survivorship bias\nRead “How Successful Leaders Think” by Roger Martin (2007) and the chapter “Identification” of “Quantitative Methods” by Huber (2024c).\nHere is a summary of Martin (2007) taken from the Harvard Business Review Store:\n\nIn search of lessons to apply in our own careers, we often try to emulate what effective leaders do. Roger Martin says this focus is misplaced, because moves that work in one context may make little sense in another. A more productive, though more difficult, approach is to look at how such leaders think. After extensive interviews with more than 50 of them, the author discovered that most are integrative thinkers–that is, they can hold in their heads two opposing ideas at once and then come up with a new idea that contains elements of each but is superior to both. Martin argues that this process of consideration and synthesis (rather than superior strategy or faultless execution) is the hallmark of exceptional businesses and the people who run them. To support his point, he examines how integrative thinkers approach the four stages of decision making to craft superior solutions. First, when determining which features of a problem are salient, they go beyond those that are obviously relevant. Second, they consider multidirectional and nonlinear relationships, not just linear ones. Third, they see the whole problem and how the parts fit together. Fourth, they creatively resolve the tensions between opposing ideas and generate new alternatives. According to the author, integrative thinking is an ability everyone can hone. He points to several examples of business leaders who have done so, such as Bob Young, co-founder and former CEO of Red Hat, the dominant distributor of Linux open-source software. Young recognized from the beginning that he didn’t have to choose between the two prevailing software business models. Inspired by both, he forged an innovative third way, creating a service offering for corporate customers that placed Red Hat on a path to tremendous success.\n\n\nDiscuss the concepts introduced by Martin (2007) critically:\n\n\nDoes he provide evidence for his ideas to work?\nIs there a proof that his suggestions can yield success?\nIs there some evidence about whether his ideas are superior to alternative causes of action?\nWhat can we learn from the article?\nDoes his argumentation fulfill highest academic standards?\nWhat is his identification strategy with respect to the causes of effects and the effects of causes?\nMartin (2007, p. 81) speculates:\n\n\n“At some point, integrative thinking will no longer be just a tacit skill (cultivated knowingly or not) in the heads of a select few.”\n\n\nIf teachers in business schools would have followed his ideas of integrative thinkers being more successful, almost 20 years later, this should be the dominant way to think as a business leader. Is that the case? And if so, can you still gain some competitive advantage by thinking that way?\n\n\n\n\nFigure 3.2: Distribution of bullet holes in returned aircraft\n\n\n\nSource: Martin Grandjean (vector), McGeddon (picture), Cameron Moll (concept), CC BY-SA 4.0, Link\n\n\n\n\nFigure 3.2 visualizes the distribution of bullet holes in aircraft that returned from combat in World War II. Imagine you are an aircraft engineer. What does this picture teach you?\nInform yourself about the concept of survivorship bias explained in Wikipedia (2024).\nIn Martin (2007), the author provides an example of a successful company to support his management ideas. Discuss whether this article relates to survivorship bias.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#mathematical-miscues",
    "href": "pitfalls.html#mathematical-miscues",
    "title": "3  Data Pitfalls",
    "section": "3.3 Mathematical Miscues",
    "text": "3.3 Mathematical Miscues\nThis pitfall deals with the errors that can occur when mathematical reasoning is flawed or when calculations are incorrectly applied within data analysis. This encompasses a range of issues, from simple arithmetic mistakes to more complex misunderstandings of mathematical principles that underlie data models. It might involve misjudging how certain metrics are calculated, which could significantly alter an analysis’s outcomes.\nAdditionally, this pitfall includes a lack of understanding the nuanced relationships within mathematical functions that can result in poor model performance or misestimation of forecasts and trends. Business leaders should be keenly aware that even advanced data analytics rely on the precise and proper application of mathematics. Therefore, ensuring precise calculations and fostering a thorough understanding of the mathematical techniques used in data analysis is vital to avoid undermining data-driven business decisions with slip-ups in mathematical reasoning.\nHere is an incomplete list of things that are often misused:\n\n\n\n\n\n\nMisuse of averages\n\n\n\nArithmetic Mean vs. Median: Incorrectly using the arithmetic mean without considering outliers or skewed distributions, which can misrepresent the central tendency of data.\nWeighted Averages: Failing to appropriately weight averages based on sample sizes or importance of data points, leading to biased results.\n\n\n\n\n\n\n\n\nImproper scaling and normalization\n\n\n\nScaling data improperly without considering the original distribution or range, which can distort comparisons and statistical analyses. Improper normalization techniques that fail to account for the underlying data structure or variance, leading to skewed interpretations.\n\n\n\n\n\n\n\n\nMisapplication of probability and statistics\n\n\n\nMisunderstanding statistical significance and confidence intervals, leading to incorrect conclusions about the reliability of findings. Inappropriate use of probability distributions or assumptions, which can invalidate statistical tests or predictions.\n\n\n\n\n\n\n\n\nErrors in calculating percentages and ratios\n\n\n\nIncorrectly calculating percentages or ratios without ensuring consistent bases or understanding the context, leading to misrepresentation of trends or relationships.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#statistical-slipups",
    "href": "pitfalls.html#statistical-slipups",
    "title": "3  Data Pitfalls",
    "section": "3.4 Statistical slipups",
    "text": "3.4 Statistical slipups\nThis pitfall is about common errors when applying statistical methods. These errors include misinterpreting statistical significance, sampling biases, ignoring multicollinearity in regression analysis, misapplying hypothesis tests, and failing to adjust for multiple comparisons.\nFor example, misinterpreting a small p-value as evidence of causation rather than correlation can lead to misleading conclusions. Similarly, using a convenience sample that does not accurately represent the population can skew results and compromise the generalizability of findings. In regression analysis, overlooking multicollinearity among predictor variables can result in inaccurate assessments of their individual impacts.\n\n3.4.1 Descriptive debacles\nThis pitfall refers to errors in data analysis stemming from inadequate or misleading use of descriptive statistics. This bias occurs when analysts present summary statistics that are incomplete, misleading, or not properly contextualized. It also includes misrepresenting data through inappropriate visualizations or failing to provide sufficient detail in describing data characteristics.\nIn practical terms, this bias leads to misunderstandings or incorrect interpretations of data. For exmple, using measures like mean or median without considering data variability can oversimplify complex datasets. Similarly, misleading graphs or charts may obscure important trends or relationships within the data.\nTo mitigate this bias, analysts should employ a range of appropriate descriptive statistics tailored to data types and research questions. They should ensure visual representations accurately depict data characteristics without distorting interpretations. Providing adequate context and explanations helps stakeholders understand data limitations and implications more effectively.\n\n\n3.4.2 Inferential infernos\nFrequently, researchers draw erroneous or exaggerated conclusions from statistical analyses. This bias manifests when statistical significance is misinterpreted, leading to unwarranted claims of causation or generalization beyond the scope of the data.\nResearchers may overlook alternative explanations or fail to consider confounding variables, thereby overstating the implications of their findings. To mitigate this bias, it is essential to apply statistical tests accurately, interpret results cautiously, and acknowledge the limitations and uncertainties of the data.\n\n\n\n\n\n\nRecommended reading\n\n\n\nRead Wysocki et al. (2022) which is freely available here. Here you find a good summary of the paper.\n\n\n\n\n3.4.3 Slippery sampling\nSystematic errors can be introduced in research studies due to flawed or biased sampling methods. This bias occurs when the sample selected for study does not accurately represent the target population, leading to skewed or misleading conclusions.\nIn practice, researchers may inadvertently use convenience sampling methods or fail to account for selection biases, resulting in a sample that is not representative of the broader population. As a result, findings based on such samples may not generalize well beyond the specific sample group, undermining the external validity of the study.\n\n\n\n\n\n\nRecommended reading\n\n\n\nRead chapter Sampling of Huber (2024c).\n\n\n\n\n3.4.4 Insensitivity to sample size\nThis bias refers to the tendency to overlook or underestimate the impact of sample size on the reliability and validity of study results. This bias occurs when analysts or researchers fail to recognize that smaller sample sizes may produce more variable or less representative data, leading to less reliable conclusions.\nIn practical terms, studies with smaller sample sizes are more prone to random fluctuations and may not accurately reflect the characteristics of the larger population. Consequently, findings based on small samples may lack statistical power and may not be generalizable to broader contexts or populations.\nHere is an incomplete summary of common statistical slipups:\n\n\n\n\n\n\nMisunderstanding statistical significance\n\n\n\nIncorrect interpretation of p-values and confidence intervals can lead to erroneous conclusions about the strength of relationships in data.\n\n\n\n\n\n\n\n\nSampling errors\n\n\n\nBiased sampling or inadequate sample size selection can distort results and impair the generalizability of findings.\n\n\n\n\n\n\n\n\nIgnoring multicollinearity\n\n\n\nFailure to account for correlations between predictor variables in regression analysis can lead to unreliable coefficient estimates and misjudgment of variable impacts.\n\n\n\n\n\n\n\n\nMisapplication of hypothesis tests\n\n\n\nImproper formulation of hypotheses or disregarding test assumptions can invalidate statistical results and mislead interpretations.\n\n\n\n\n\n\n\n\nFailure to adjust for multiple comparisons\n\n\n\nConducting multiple tests without adjusting significance levels increases the likelihood of false positives and misleading conclusions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#analytical-aberrations",
    "href": "pitfalls.html#analytical-aberrations",
    "title": "3  Data Pitfalls",
    "section": "3.5 Analytical Aberrations",
    "text": "3.5 Analytical Aberrations\n\n3.5.1 The intuition/analysis false dichotomy\nThis error stems from misconception that intuition and analytical thinking involves when trying to use it as approaches to decision-making. In reality, effective decision-making often integrates both intuitive insights and analytical rigor. Intuition can provide valuable initial impressions or creative ideas, while analysis offers systematic evaluation and validation of hypotheses. Acknowledging and balancing these two aspects enhances decision-making processes by leveraging diverse perspectives and ensuring comprehensive consideration of factors influencing outcomes.\n\n\n\n\n\n\nRecommended reading\n\n\n\nRead the chapter Decision making basics of Huber (2024b)\n\n\nWhile intuition comes with multiple cognitive biases. Intuition remains valuable in decision-making for several reasons. Firstly, it often draws on subconscious processing of vast amounts of information, leading to quick insights that can guide effective decisions. Secondly, intuition can fill gaps left by incomplete data, offering solutions when data is scarce or ambiguous. Thirdly, it allows for creativity and innovation by providing unconventional perspectives that data alone may not reveal. Fourthly, intuition serves as a valuable complement to analytical thinking, providing a holistic approach to problem-solving. Lastly, it facilitates swift decision-making in fast-paced environments where immediate action is crucial. Integrating intuition with data-driven approaches enhances decision-making by leveraging both analytical rigor and intuitive wisdom.\n\n\n3.5.2 Exuberant extrapolations\nExuberant Extrapolations refer to the tendency to make overly optimistic or exaggerated predictions based on limited data or trends. This pitfall occurs when analysts or decision-makers extend current trends far into the future without considering potential deviations or external factors that could influence outcomes. It can lead to unrealistic expectations, flawed projections, and poor decision-making due to overconfidence in extrapolated outcomes. Mitigating this pitfall requires cautious interpretation of trends, consideration of uncertainties, and validation through robust analysis and scenario planning to ensure more realistic and reliable forecasts.\n\n\n\n\n\n\n\nExercise 3.3 Past and present\nAgain, let us consider “How Successful Leaders Think” by Roger Martin (2007).\nDiscuss the insights that he puts forward. Can we really learn from the experience of successfull leaders?\n\n\n\n\n\n\n3.5.3 Ill-advised interpolations\nIll-advised interpolations occur when analysts or decision-makers infer trends or make assumptions between data points without sufficient evidence or justification. This pitfall involves filling gaps in data with speculative assumptions or linear extrapolations that do not accurately reflect the underlying patterns or relationships. It can lead to misleading interpretations, erroneous conclusions, and misguided decision-making based on incomplete or flawed information.\n\n\n3.5.4 Funky forecasts\nThis pitfall arises when forecasters fail to account for changing circumstances, unexpected events, or complex interactions among factors influencing future trends. It can lead to unreliable projections, erroneous planning, and ineffective decision-making based on overly simplistic or deterministic forecasts. To mitigate this pitfall, analysts should use robust forecasting techniques, incorporate scenario analysis to account for uncertainties, and continuously update forecasts based on new information and changing conditions.\n\n\n3.5.5 Moronic measures\nMoronic measures refer to the use of inappropriate or inadequate metrics for evaluating performance or assessing outcomes. This pitfall occurs when decision-makers rely on metrics that do not accurately capture the intended goals or fail to account for relevant factors influencing performance. It can lead to misleading assessments, misguided resource allocation, and ineffective strategies due to a lack of alignment between chosen metrics and organizational objectives. To mitigate this pitfall, organizations should carefully select metrics that are relevant, measurable, and aligned with strategic priorities. They should also consider using complementary metrics and qualitative assessments to provide a comprehensive understanding of performance outcomes. By using appropriate measures and periodically reviewing their relevance, organizations can enhance decision-making and ensure that performance evaluations contribute meaningfully to achieving desired outcomes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#graphical-gaffes",
    "href": "pitfalls.html#graphical-gaffes",
    "title": "3  Data Pitfalls",
    "section": "3.6 Graphical Gaffes",
    "text": "3.6 Graphical Gaffes\n\n\n\n\n\n\nRecommended reading\n\n\n\nRead the chapter Visualize data of Huber (2024a) and the literature that you find cited therein.\n\n\n\n3.6.1 Challenging charts\nPitfalls in data visualization are pretty commen. Charts are frequently poorly designed, misleading, or difficult to interpret. Actually, it is simply difficult to effectively communicate insights with graphical visualzations of data due to cluttered layouts, inappropriate chart types, distorted scales, or ambiguous labeling. It can lead to confusion, misinterpretation of data trends, and inaccurate decision-making based on flawed visual presentations. To mitigate this pitfall, analysts should adhere to best practices in data visualization, such as using clear and simple designs, choosing appropriate chart types for the data being presented, ensuring accurate scaling and labeling, and providing contextual information to aid interpretation.\n\n\n3.6.2 Data dogmatism\nData Dogmatism refers to the pitfall of rigidly adhering to data-driven decisions without considering broader context, qualitative insights, or expert judgment. This pitfall occurs when organizations or decision-makers prioritize data alone as the sole basis for decision-making, ignoring nuanced factors that may influence outcomes. It can lead to overly deterministic decisions, missed opportunities, and ineffective strategies that fail to account for human expertise, market dynamics, or unforeseen variables. To mitigate this pitfall, organizations should adopt a balanced approach that integrates data-driven insights with qualitative analysis, expert opinions, and contextual understanding. By recognizing the limitations of data and embracing a more holistic decision-making framework, organizations can enhance flexibility, innovation, and strategic agility in response to complex challenges.\n\n\n3.6.3 The optimize/satisfice false dichotomy\nThis dichotomy refers to the misconception that decision-making strategies must exclusively prioritize either optimizing for the best possible outcome or satisficing by accepting a satisfactory but not necessarily optimal solution. This pitfall arises when decision-makers perceive a dichotomy between these approaches, potentially overlooking opportunities for adaptive strategies that blend elements of both optimization and satisficing. It can lead to suboptimal decisions, missed opportunities for innovation, or inefficient resource allocation due to rigid adherence to a singular decision-making approach. To mitigate this pitfall, organizations should adopt a flexible and adaptive decision-making framework that considers both optimizing for excellence where feasible and satisficing when constraints or uncertainties warrant. By embracing a nuanced approach that balances ambition with pragmatism, organizations can enhance resilience, innovation, and effectiveness in achieving strategic objectives.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "pitfalls.html#design-dangers",
    "href": "pitfalls.html#design-dangers",
    "title": "3  Data Pitfalls",
    "section": "3.7 Design Dangers",
    "text": "3.7 Design Dangers\n\n\n\n\n\n\nRecommended reading\n\n\n\nRead the chapter Visualize data of Huber (2024a) and the literature that you find cited therein.\n\n\nThis section encompasses pitfalls related to the design and implementation of data-driven projects or experiments. It includes errors such as biased sampling methods, inadequate experimental design, or flawed data collection protocols.\n\n3.7.1 Confusing Colors\nIn data visualization the choice of colors in charts matter. When colors are poorly contrasted, misaligned with data meaning, or inconsistently applied across visual elements, it may be difficult to distinguish different data categories or trends. The wrong colors can lead to misunderstandings and inaccurate conclusions. And by the way, many people are color blind.\n\n\n3.7.2 Usability uh-ohs\nThis pitfall occurs when systems lack intuitive interfaces, are overly complex, or fail to meet user needs and expectations. It can result in frustration, inefficiencies, or resistance to adoption among users, leading to underutilization of valuable data resources. To mitigate this pitfall, organizations should prioritize user-centric design principles, conduct usability testing with diverse user groups, and solicit feedback to refine interfaces and functionalities. By enhancing usability, organizations can improve user satisfaction, facilitate smoother workflows, and maximize the utility of data-driven tools for informed decision-making.\n\n3.7.2.1 Good visualizations are discoverable and understandable\nGood visualizations are designed to be easily discovered and understood by users. They prioritize clarity and accessibility, allowing users to quickly grasp the presented information without ambiguity or unnecessary complexity.\n\n\n3.7.2.2 Don’t blame people for getting confused or making errors\nUsers should not be blamed for confusion or errors in understanding visualizations. Instead, designers should strive to create intuitive and user-friendly designs that minimize cognitive load and facilitate accurate interpretation of data.\n\n\n3.7.2.3 Designing for pleasure and emotion is important\nEffective visualizations consider the emotional and aesthetic impact on users. Design elements that evoke positive emotions and engage users can enhance comprehension and retention of information, making the visualization more impactful and memorable.\n\n\n3.7.2.4 Complexity is good, confusion is bad\nComplexity in visualizations can enrich understanding by presenting detailed information and relationships. However, complexity should not lead to confusion or overwhelm users. Designers must balance complexity with clarity to ensure that insights are communicated effectively without causing cognitive overload.\n\n\n3.7.2.5 Absolute precision isn’t always necessary\nVisualizations do not always need to convey absolute precision in every detail. Depending on the context and audience, approximations or visual representations that convey trends and patterns effectively may be sufficient. Designers should prioritize meaningful insights over exhaustive accuracy in all data points.\n\n\n\nFigure 3.1: Bananas in various stages of ripeness\nFigure 3.2: Distribution of bullet holes in returned aircraft\n\n\n\nHuber, S. (2024a). How to use R for data science: Lecture notes. https://hubchev.github.io/ds/\n\n\nHuber, S. (2024b). Managerial economics: Lecture notes. https://hubchev.github.io/me/\n\n\nHuber, S. (2024c). Quantitative methods: Lecture notes. https://hubchev.github.io/qm/\n\n\nJones, B. (2020). Avoiding data pitfalls: How to steer clear of common blunders when working with data and presenting analysis and visualizations. John Wiley & Sons.\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business Review, 85(6), 71–81.\n\n\nWikipedia. (2024). Survivorship bias. https://en.wikipedia.org/wiki/Survivorship_bias\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical control requires causal justification. Advances in Methods and Practices in Psychological Science, 5(2). https://doi.org/10.1177/25152459221095823",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Pitfalls</span>"
    ]
  },
  {
    "objectID": "htur4ds.html",
    "href": "htur4ds.html",
    "title": "4  How to use R for data science",
    "section": "",
    "text": "The programming language R is one of the major tools to do data science. I wrote some lecture notes on How to use R for data science (Huber, 2024).\nPlease read these notes.\n\n\n\n\n\n\nTip\n\n\n\nWe have discussed the following chapters 1, 2, 3, 5, 6, 7, 8 and some exercises of chapter 9. In particular, we have put some emphasise on chapter 6, sections 7.2-7.4, and chapter 8. Moreover, we have explicitely studied the exercises Consumer prices over time, Load the Stata dataset “auto” using R, Convergence,\n\n\n\n\n\n\nHuber, S. (2024). How to use R for data science: Lecture notes. https://hubchev.github.io/ds/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>How to use R for data science</span>"
    ]
  },
  {
    "objectID": "git-github.html",
    "href": "git-github.html",
    "title": "5  Collaborating with Git and GitHub",
    "section": "",
    "text": "5.1 Introduction\nGit is open-source software for version control. It allows developers to track and manage changes to their codebase and files. Users can access a comprehensive history of their project and revert to previous versions of their data if necessary. It helps to overcome the FINAL.doc problem depicted in Figure 5.1.\nGitHub is an incredibly popular (see statistics in Figure 5.2) online platform that implements Git’s capabilities by providing a web interface for collaboration.\nWhile you can use Git and GitHub independently, most developers integrate it with GitHub for enhanced project management and collaboration. This combination helps maintain local and remote copies of a project, facilitating teamwork and data backup as GitHub is sort of a backup as data loss at your local machine do not matter if you have a remote version saved on GitHub.\nGit and GitHub support simultaneous multi-user access, unlike systems that are optimized for single-user like Dropbox.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#introduction",
    "href": "git-github.html#introduction",
    "title": "5  Collaborating with Git and GitHub",
    "section": "",
    "text": "Figure 5.2: GitHub is big\n\n\n\nSource: https://github.com/about as of April 2024",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitsection",
    "href": "git-github.html#sec-gitsection",
    "title": "5  Collaborating with Git and GitHub",
    "section": "5.2 Install Git",
    "text": "5.2 Install Git\nTo install the version control system Git, follow the instructions here.\n\n\n\nFigure 5.3: Memorizing six git commands\n\n\n\nSource: DEV Community on GitHub\n\n\n\nFamiliarize yourself with Git by using the resources available here. Specifically, work through the resources listed in the box below. Although Git may appear complex, it is generally not too challenging for most users. Many people use Git primarily to track their work and to host and share files conveniently with just a handful of commands. While Git is a robust system with many capabilities, you don’t need to memorize all the commands (see Figure 5.3). In fact, you typically use only a few basic ones as shown in Table 5.1.\nIn the upcoming sections, I will demonstrate some use cases both in the terminal Section 5.3 and within RStudio Section 5.4. In Section 5.5, I show how to contribute to a repository using Git and GitHub.\n\n\n\n\n\n\nLearning resources\n\n\n\nPlenty books and tutorial exist that introduce Git and GitHub. I’d like to highlight the following sources:\n\nThe book comprehensive book Happy Git and GitHub for the useR by Bryan (2023)\nThe much shorter book [Version Control with Git and GitHub] by Halbritter & Telford (2023)\nThe online tutorial How to Use Git/GitHub with R of David Keyes who explains in short videos how to setup Git and GitHub in RStudio using among others the usethis package.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.1: Most important git commands\n\n\n\n\n\n\n\n\n\nGit Command\nDescription\n\n\n\n\ngit init\nInitialize a new Git repository in the current directory.\n\n\ngit clone &lt;url&gt;\nClone a repository from a remote URL to your local machine.\n\n\ngit add &lt;file&gt;\nAdd a specific file to the staging area in preparation for committing.\n\n\ngit add .\nAdd all changed files in the current directory to the staging area.\n\n\ngit commit -m \"message\"\nCommit the staged changes to the repository with a descriptive message.\n\n\ngit status\nDisplay the status of the working directory and staging area.\n\n\ngit push &lt;remote&gt; &lt;branch&gt;\nPush committed changes in your local branch to the remote repository.\n\n\ngit pull &lt;remote&gt; &lt;branch&gt;\nPull changes from the remote repository into your current branch and merge them.\n\n\ngit branch &lt;name&gt;\nCreate a new branch with the specified name.\n\n\ngit checkout &lt;branch&gt;\nSwitch to another branch and update the working directory.\n\n\ngit merge &lt;branch&gt;\nMerge a specified branch into the current branch.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitterminal",
    "href": "git-github.html#sec-gitterminal",
    "title": "5  Collaborating with Git and GitHub",
    "section": "5.3 Using Git from the terminal",
    "text": "5.3 Using Git from the terminal\n\n\n\nFigure 5.4: Three git commands you really need\n\n\n\n\n\n\nThis tutorial will guide you through the basic Git operations using the Bash command line, commonly referred to as the terminal. Essentially, it focuses on the three Git commands illustrated in Figure 5.4.\n\n5.3.1 Configuring Git\nBefore you start using Git, you need to configure your Git environment. Set your username and email address with these commands:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n\n5.3.2 Initializing a Repository\nTo create a new Git repository, use the git init command in the directory you want to version control:\ncd /path/to/a/directory\nmkdir my_project\ncd my_project\ngit init\nIn case you are not familiar with using the terminal please consider Table 5.2 where I introduce the most basic commands that we use. For example, with cd you can change your directory and with mkdir you create a new directory. If you are not familiar with the file system of your computer please read the section Navigating the file system of Huber (2024). With git init you initialize the directory to be a git repository. This will create a hidden folder “.git” in which Git keeps track of all your changes.\n\n\n\n\n\n\nMost common bash commands\n\n\n\n\n\n\n\n\nTable 5.2: Most common bash commands\n\n\n\n\n\n\n\n\n\n\nBash Command (macOS/Linux)\nWindows Command Prompt Equivalent\nDescription\n\n\n\n\npwd\ncd\nPrints the current directory’s path.\n\n\nls\ndir\nLists all files and directories in the current directory.\n\n\ncd\ncd\nChanges the directory.\n\n\nmkdir\nmkdir\nCreates a new directory.\n\n\nrmdir\nrmdir\nRemoves an empty directory.\n\n\ntouch\ncopy nul\nCreates a new empty file or updates an existing file’s timestamp.\n\n\nrm\ndel or erase\nRemoves files. rmdir /s is used for directories.\n\n\ncp\ncopy\nCopies files or directories.\n\n\nmv\nmove\nMoves or renames files or directories.\n\n\necho\necho\nDisplays a line of text/string.\n\n\ncat\ntype\nConcatenates and displays the content of files.\n\n\ngrep\nfind or findstr\nSearches for patterns in files.\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Staging Changes\nTo track changes in your repository, you need to stage them using the git add command. To stage a single file:\ngit add filename.txt\nTo stage all changes in the directory:\ngit add .\n\n\n5.3.4 Committing Changes\nAfter staging, you can commit it to the repository. A commit records changes to the repository and must include a message describing what changed:\ngit commit -m \"A message\"\n\n\n5.3.5 Pushing Changes\nTo share your commits with others or store them in a remote repository (GitHub), use git push. A prerequisite here is that you need to be connected to a remote repo. Therefore, you must add a remote repository by copying the URL of the GitHub repo as shown in Figure 5.5. Then you can add the remote repository and push it to the repo with these lines of code:\ngit remote add origin https://github.com/username/repository.git\ngit push -u origin main\n\n\n\nFigure 5.5: Copy the https URL of your repo\n\n\n\n\n\n\n\n\n5.3.6 Undo changes\nWith git reset and git revert you can go back in time and undo specific changes, respectively. For example, with\ngit log\ngit reset --hard &lt;commit_id_hash&gt;\nyou can view the commit history and find the hash identifier of the commit to move the HEAD pointer to that commit. This effectively removes all commits after commit you choose from the current branch’s history. Be cautious when using git reset –hard as it discards all changes made after the specified commit. Make sure you have backups or are certain you want to discard these changes before proceeding.\nWith\ngit revert &lt;commit_id_hash&gt;\nyou revert the changes introduced by that commit only. It will create a new commit that undoes the changes made in commit chosen while keeping the other commits that may have followed the chosen commit intact. It’s a safer approach compared to git reset --hard, as it preserves the commit history and allows you to selectively undo changes without affecting the rest of the commits.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitstudio",
    "href": "git-github.html#sec-gitstudio",
    "title": "5  Collaborating with Git and GitHub",
    "section": "5.4 Using Git from RStudio",
    "text": "5.4 Using Git from RStudio\nIntegrating Git with RStudio enhances your project management by utilizing version control directly within the IDE. Here’s how you can set up and use Git in RStudio using R code.\n\n5.4.1 Set up Git in RStudio\nFirst, ensure the usethis package is installed and loaded:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis)\n\nConfigure your Git settings in RStudio:\n\nuse_git_config(user.name = \"Your Name\", user.email = \"Your@email.com\")\n\nYou can change the configuration of your user name and email using the edit_git_config() function.\nStart a new project in RStudio, which will also initialize a Git repository:\n\ncreate_project(\"~/Music/\")\nuse_git()\n\nAfter restarting RStudio, you will notice a Git tab in the top right panel, indicating that Git is now active for your project.\n\n\n5.4.2 Connecting RStudio Projects with GitHub repositories\nTo connect your RStudio project with GitHub, you need a Personal Access Token (PAT) on GitHub. If you haven’t one already, you can use the create_github_token() function from usethis package, and store the PAT securely with gitcreds_set from the gitcreds package:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis gitcreds)\ncreate_github_token()\ngitcreds::gitcreds_set()\n\nNow, the procedure depends on whether the project has bin initialized on your local machine and you want to create a repo on GitHub, or the repo already exists on GitHub and you want to connect that remote repo with your local PC. Both ways are described below.\n\n5.4.2.1 Project exists on RStudio first\nAfter initializing Git in your project, use the use_github() function from usethis to create a new GitHub repository and connect it directly:\n\nuse_github()\n\nThis creates a repo on your GitHub account.\n\n\n5.4.2.2 Project exists on GitHub first\nAlternatively, suppose you have created a repository on GitHub first, then start a new project in RStudio using the version control option, specifying your new repository’s URL. Just click File &gt; New Project &gt; Version Control and then link the GitHub repo by putting the URL into the respective box of the menu. See Figure 5.5 how to get the URL of a repo.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-pullrequest",
    "href": "git-github.html#sec-pullrequest",
    "title": "5  Collaborating with Git and GitHub",
    "section": "5.5 Make a contribution using Git and GitHub",
    "text": "5.5 Make a contribution using Git and GitHub\nThis is a guide for beginners on how to make a contribution using Git and GitHub. If you are looking to make your first contribution, follow the steps below.\n\n\n\n\n\n\nWatch\n\n\n\nthis video where I do all the following steps in real time. It takes about 15 minutes.\n\n\n1. Create an account on GitHub\nIt is for free and should just take some minutes.\n2. Install Git\nHere is a tutorial on how to set up Git.\n3. Fork this repository\nClick on the fork button (see Figure 5.6) on the top of this page: https://github.com/hubchev/make_a_pull_request. This will create a copy of this repository in your account.\n\n\n\nFigure 5.6: Fork the repo\n\n\n\n\n\n\n4. Clone the forked repository\nGo to your GitHub account, open the forked repository, click on the code button and then click the copy to clipboard icon, see Figure 5.5.\nThen, open a terminal and run the following git command:\ngit clone \"url you just copied\"\nwhere “url you just copied” (without the quotation marks) is the url to this repository (your fork of this project). See the previous steps to obtain the url.\nFor example:\ngit clone https://github.com/this-is-you/make_a_pull_request.git\nwhere this-is-you is your GitHub username. Here you’re copying the contents of the first-contributions repository on GitHub to your computer.\n5. Create a branch\nChange to the repository directory on your computer (if you are not already there):\ncd make_a_pull_request\nNow create a branch using the git switch command:\ngit switch -c your-new-branch-name\nFor example:\ngit switch -c add-Stephan-Huber\n6. Make changes.\nNow open theI_am_a_data_scientist.md file in a text editor. (You find this file in the repository.) Add your name, your GitHub account and the project you are working on. You can put it anywhere in between. Now, save the file.\nIf you go to the project directory and execute the command git status, you’ll see there are changes.\n7. Add changes (staging). Add those changes to the branch you just created using the git add command:\ngit add .\n8. Commit changes. Now commit those changes using the git commit command:\ngit commit -m \"Add your-name to the list\"\nreplacing your-name with your name.\n9. Use Git Bash. Open Git Bash and set your email and your nickname on GitHub:\ngit config --global user.name \"FIRST_NAME LAST_NAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n10. Push changes to GitHub.\nPush your changes using the command git push:\ngit push -u origin your-new-branch-name\nreplacing your-new-branch-name with the name of the branch you created earlier.\nIf you get any errors while pushing that refers to authentication failed something, go to GitHub’s tutorial on generating and configuring an SSH key to your account. Alternatively, you can watch this YouTube tutorial\n11. Submit your changes for review on GitHub.\nIf you go to your repository on GitHub, you’ll see a Compare & pull request button. Click on that button.\nNow submit the pull request.\nSoon I’ll be merging all your changes into the main branch of this project. You will get a notification email once the changes have been merged.\nCongrats! You just completed the standard fork -&gt; clone -&gt; edit -&gt; pull request workflow that you’ll often encounter as a contributor!\n\n\n\n\nFigure 5.1: The FINAL.doc problem\nFigure 5.2: GitHub is big\nFigure 5.3: Memorizing six git commands\nFigure 5.4: Three git commands you really need\nFigure 5.5: Copy the https URL of your repo\nFigure 5.6: Fork the repo\n\n\n\nBryan, J. (2023). Happy git and GitHub for the useR. https://happygitwithr.com/\n\n\nHalbritter, A., & Telford, R. J. (2023). Version control with git and GitHub. https://biostats-r.github.io/biostats/github/\n\n\nHuber, S. (2024). How to use R for data science: Lecture notes. https://hubchev.github.io/ds/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "54_quarto.html",
    "href": "54_quarto.html",
    "title": "6  Markdown and Quarto",
    "section": "",
    "text": "Verbal and non-verbal communication is important in business. This section is about writing and publishing texts, leaving out things like body language and writing skills. I will introduce some applications (Markdown, RMarkdown, Quarto) that data scientists often use to write and publish their work. I will also discuss the version control system git and the online platform GitHub, which can be used in combination with Quarto and Markdown to create, store, manage and share files. These tools are the backbone of most data science collaborations. Once you master these tools, they can significantly enhance your efficiency and make your presentations more impactful, even if you are not directly involved in the field of data science.\n\nQuarto, a modern documentation system, is an excellent choice for writing, especially for projects that require rigorous data analysis, visualization, and reproducibility. This tutorial will guide you through producing various forms of text with Quarto. You can write reports, articles, theses, books, websites and many more with Quarto.\nStep 1: Learn Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. It’s an essential skill for using Quarto effectively. Start by learning enough Markdown to structure your thesis, including headings, lists, links, and code blocks.\nYou can learn Markdown (not R Markdown!) in 10 minutes. Just go to www.markdowntutorial.com and work through the interactive lessons.\nStep 2: Learn Quarto\nRead Telford (2023): Enough Markdown to Write a Thesis. This resource covers the basics and some advanced Markdown features that are useful for academic writing.\nMore extensive resources on how to do things with Quarto can be found at quarto.org.\n\n\n\n\n\n\nQuarto and R markdown\n\n\n\nQuarto is a relatively new tool. It can be considered the successor to R Markdown, as it is built upon R Markdown. Consequently, almost all R Markdown documents are compatible with Quarto. However, Quarto includes several improvements over R Markdown that enhance its ease of use. For a detailed description of all the differences and similarities between the two, you can read this article. For an introduction to R Markdown see Chapter 7.\n\n\n\n\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "50_rmarkdown.html",
    "href": "50_rmarkdown.html",
    "title": "7  Write with R Markdown",
    "section": "",
    "text": "Figure 7.1: Example of an R Markdown file\n\n\n\n\n\n\nR Markdown provides an authoring framework for data science. You can use a single R Markdown file to transcript your work, run code, and generate high quality reports, books, websites, articles, theses, blogs, and many more (see Figure 7.1).\nIn contrast to Quarto (see Chapter 6), which is the more recent format, R Markdown is around for some time and hence there are uncountable resources to learn it. For example:\n\nThe R Markdown Cheatsheet (see Figure 7.2) from Posit offers an overview on the most important features of R Markdown.\n\n\n\n\nFigure 7.2: R Markdown Cheatsheet from Posit\n\n\n\n\n\n\n\nThe book R Markdown Cookbook by Xie et al. (2020) (see Figure 7.3) offers an introduction. The online version of the book is regularly updated and free of costs.\n\n\n\n\nFigure 7.3: Xie et al. (2020): R Markdown Cookbook\n\n\n\n\n\n\n\nThe book R Markdown: The Definitive Guide by Xie et al. (2018) offers a comprehensive introduction. The online version of the book is regularly updated and free of costs.\n\n\n\n\nFigure 7.4: Xie et al. (2018): R Markdown: The Definitive Guide\n\n\n\n\n\n\nPlease watch the video What is R Markdown? and then study the R Markdown tutorial from RStudio.\n\n\n\n\n\n\n\n\nWorking directory in R Markdown\n\n\n\nThe working directory is by default set to the directory that contains the Rmd document. In case you want to use another directory you can do so by changing the working directory with setwd(). However, that is not persistent in R Markdown and only works for the current code chunk. After the code chunk has been evaluated, the working directory will be restored to the directory where the Rmd file is placed.\n\n\n\n\n\n\n\n\n\nExercise 7.1 Start Markdown and R Markdown\n\nYou can learn Markdown (not R Markdown!) in 10 minutes. Just go to https://www.markdowntutorial.com and work throught the interactive lessons.\nNow create your first R Markdown file in 3 minutes by doing the following:\n\nclick in RStudio on File &gt; New File &gt; R Markdown\nclick OK\nlook for a button entitled Knit and click it\nsave your file (it will be saved with .Rmd file extension)\n\nPlay around with the file. For example, change the output format can you create a word file or a presentation. Play around with the code chunks. Add a picture that you find somewhere online.\nSet your working directory to the folder where you have saved your first Rmd-file. Can you come up with a way to generate different output format with just one function.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.2 R Markdown cite literature\n\nCreate a new R Markdown file (File &gt; New File &gt; R Markdown), save the file in an empty folder, and knit it.\n\n\n\nMake a new script with File &gt; New File &gt; R Script.\nGo to https://scholar.google.de/ and search for osrmtime.\nClick on “cite” and “BibTeX”. Copy and paste everything that you see into your script and save the script as lit.bib. R Studio will ask you if you confirm the file type change. Click yes. Your lit.bib file should look like this:\n\n@article{huber2016calculate,\n  title={Calculate travel time and distance with OpenStreetMap \n    data using the Open Source Routing Machine (OSRM)},\n  author={Huber, Stephan and Rust, Christoph},\n  journal={The Stata Journal},\n  volume={16},\n  number={2},\n  pages={416--423},\n  year={2016},\n  publisher={SAGE Publications Sage CA: Los Angeles, CA}\n}\n\nAdd the text “bibliography: references.bib” to your YAML header of your R Markdown file so that it looks somehow like that:\n\n---\ntitle: \"Untitled\"\nauthor: \"Stephan Huber\"\ndate: \"`r Sys.Date()`\"\noutput: html_document\nbibliography: lit.bib\n---\n\nNow you can cite the OSRMTIME paper with @huber2016calculate somewhere in the text of your R Markdown file.\nKnit the R Markdown file and you should see the paper cited and a reference list at the end of the html report.\nYou can manipulate the citation style you can specify a CSL (Citation Style Language) file in the YAML header. For example the APA style can be chosen with:\n\ncsl: \"https://www.zotero.org/styles/apa.csl\"\nMany more citation styles can be found on github.com/citation-style-language and on the Zotero Style Repository.\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.3 Preparing APA journal articles (papaja)\nThere is an easy way to write a manuscript that follows all the APA rules using the package papaja written by two psychologists from Cologne. Please read their manual and consider their repository on GitHub.\nNow, install and load the package:\n\ninstall.packages(\"papaja\")\nlibrary(\"papaja\")\n\nThen, click “File &gt; New File &gt; R Markdown” and choose the “APA-style manuscript” from the section “from template”. Knit the R markdown template and you will have a template for a APA manuscript.\nApart from the obvious adjustments, I recommend to make at least two general adjustments: Change classoption to “doc” and linenumbers to “no”.\n\n\n\n\n\n\n\n\n\n\n\nExercise 7.4 R Markdown template\nPlease follow the instructions below to access the file “23-09_ds-project-desc.Rmd” from my GitHub account:\n\nDownload the file from my GitHub account by clicking on the link provided here.\nSave the file in your working directory.\nUse the knit function to run the file, but be aware that it may not work properly at first. If you encounter any issues, troubleshooting may be required. Don`t worry, error messages will usually provide guidance to help you resolve the issue. Please note that the YAML header is sensitive to spacing, so be careful when setting it up to avoid breaking the code.\nIn the project template, I have used BibTeX to cite literature. This method is excellent for automating tedious tasks such as citing papers and generating reference lists based on citation styles, saving time and reducing the likelihood of citation errors. The literature cited is in a separate file, which can be found on one of my GitHub repositories.\n\n\n\n\n\n\n\n\nFigure 7.1: Example of an R Markdown file\nFigure 7.2: R Markdown Cheatsheet from Posit\nFigure 7.3: Xie et al. (2020): R Markdown Cookbook\nFigure 7.4: Xie et al. (2018): R Markdown: The Definitive Guide\n\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2018). R markdown: The definitive guide. Chapman; Hall/CRC.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Write with R Markdown</span>"
    ]
  },
  {
    "objectID": "website.html",
    "href": "website.html",
    "title": "8  Create and host a website",
    "section": "",
    "text": "8.1 Creating a website with Quarto\nThis tutorial guides you through creating a simple, yet professional-looking website using Quarto.\nStep W1: Install Quarto\nEnsure Quarto is installed on your system. If not, download and install it from Quarto’s official website.\nStep W2: Create a website\nFollow the tutorial that you find here.\nStep W3: Copy the _site directory\nAfter you have rendered your website a directory “_site” appears in the project folder that contains your website. Copy all files of that directory to a directory where you want to save your website. Let’s say my_website.\nIn the terminal you can do this with\nmkdir /home/sthu/my_website/\ncp -r /home/sthu/quarto_website/_site/* /home/sthu/my_website/",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "website.html#hosting-the-website-on-github",
    "href": "website.html#hosting-the-website-on-github",
    "title": "8  Create and host a website",
    "section": "8.2 Hosting the website on GitHub",
    "text": "8.2 Hosting the website on GitHub\nR Studio and Quarto offers you various ways to publish the website. I explain you a way that worked out well for me.\nStep G1: Create a GitHub account\nGitHub will host your thesis website and manage version control for your thesis project. If you don’t already have a GitHub account, you’ll need to create one: Sign up at GitHub.\nStep G2: Create a repository\nCreate a repository. Name the repo with your username followed by github.io. You find a tutorial here.\nStep G3: Obtain a personal access token\nA personal access token (PAT) is required to authenticate with GitHub from Quarto and RStudio. This token allows you to push changes to your repository securely. Follow the instructions to create a personal access token on GitHub. Alternatively, you can do the following in R:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis)\ncreate_github_token() \n\nMake sure to note down your token and keep it secure. You’ll use this token in RStudio and Quarto to authenticate your GitHub operations.\nStep G4: Install and Learn Git\nSee Section 5.2.\nStep G5: Upload the website to GitHub\nUse the Terminal of R Studio. Go to the directory with your website that you have copied in Step W3. Then initiate a git repository on the command line, connect it to the repository created in Steph G2 on GitHub and finally push it:\ncd /home/sthu/my_website/\necho \"# test\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/test-hsf/test.git\ngit push -u origin main\nAlternatively, you can clone a repository, make some changes, and then push those changes back to GitHub. Here are the Bash commands to accomplish this:\n# Clone the repository\ngit clone https://github.com/your-username/your-repository.git\n\n# Make changes, here adding a new file as an example\necho \"Some content for the new file\" &gt; newfile.txt\n\n# Add the new file to the repository\ngit add newfile.txt\n\n# Commit the changes\ngit commit -m \"Add new file\"\n\n# Push the changes back to GitHub\ngit push origin main",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bryan, J. (2023). Happy git and GitHub for the useR. https://happygitwithr.com/\n\n\nDavenport, T. H., & Patil, D. (2012). Data scientist: The sexiest\njob of the 21st century. Harvard Business Review,\n90(5), 70–76.\n\n\nHalbritter, A., & Telford, R. J. (2023). Version control with\ngit and GitHub. https://biostats-r.github.io/biostats/github/\n\n\nHuber, S. (2024a). How to use R for data science:\nLecture notes. https://hubchev.github.io/ds/\n\n\nHuber, S. (2024b). Managerial economics: Lecture notes. https://hubchev.github.io/me/\n\n\nHuber, S. (2024c). Quantitative methods: Lecture notes. https://hubchev.github.io/qm/\n\n\nJones, B. (2020). Avoiding data pitfalls: How to steer clear of\ncommon blunders when working with data and presenting analysis and\nvisualizations. John Wiley & Sons.\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business\nReview, 85(6), 71–81.\n\n\nOpenAI. (2024). ChatGPT (April 18, 2024). Large language model.\nhttps://chat.openai.comt\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/\n\n\nWikipedia. (2024). Survivorship bias. https://en.wikipedia.org/wiki/Survivorship_bias\n\n\nWooldridge, J. M. (2002). Introductory econometrics: A modern approach.\nIn Delhi: Cengage Learnng (2nd ed.). South-Western.\n\n\nWysocki, A. C., Lawson, K. M., & Rhemtulla, M. (2022). Statistical\ncontrol requires causal justification. Advances in Methods and\nPractices in Psychological Science, 5(2). https://doi.org/10.1177/25152459221095823\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2018). R markdown: The\ndefinitive guide. Chapman; Hall/CRC.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown\ncookbook. Chapman; Hall/CRC.",
    "crumbs": [
      "References"
    ]
  }
]