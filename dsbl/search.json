[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Business Leaders",
    "section": "",
    "text": "Preface\n\nAbout Data Science for Business Leaders\nThis course provides an overview of the tools and methods that data science offers to business leaders. Of course, we only scratch the surface on many topics. Many topics require more in-depth analysis to fully grasp the underlying principles and their benefits. In my opinion, business leaders often don’t understand every detail of the decisions they make. They can’t do that and often they don’t need to. However, they are good at identifying and prioritizing the key factors that can drive success. They have a sense of the essential elements that have a significant impact. Ultimately, leaders lead. They recognize, cultivate and manage the expertise of their employees and teammates. They orchestrate the knowledge around them, they lead their teams effectively and help them avoid critical missteps.\n\n\nAbout the cover and the logo of the notes\nI realize that having a visually appealing cover and logo does not directly help you become an exceptional leader. The images themselves do not teach you how to master the tools of data science or apply its methods effectively. However, I do believe that attractive visual elements appeal to many students and individuals. Therefore, as a teacher, I think it is wise to occasionally incorporate these elements into my notes. This approach may encourage you to continue reading and studying the material. By the way, it only took me a couple of minutes to create these two images using ChatGPT.\n\n\nAbout the notes\n\n\n\n\n\n\nA PDF version of these notes is available here..\n\n\n\nPlease note that while the PDF contains the same content, it has not been optimized for PDF format. Therefore, some parts may not appear as intended.\n\n\n\nThese notes aims to support my lecture at the HS Fresenius but are incomplete and no substitute for taking actively part in class.\nI hope you find this book helpful. Any feedback is both welcome and appreciated.\nThis is work in progress so please check for updates regularly.\nThese notes offer a curated collection of explanations, exercises, and tips to facilitate learning R without causing unnecessary frustration. However, these notes don’t aim to rival comprehensive textbooks.\nThese notes are published under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. This means it can be reused, remixed, retained, revised and redistributed as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license. \nI host the notes in a GitHub repo.\n\n\n\nStructure of these notes\n\n\nAbout the author\n\n\n\n\n\n\nContact:\n\n\n\n\n\nProf. Dr. Stephan Huber\nHochschule Fresenius für Wirtschaft & Medien GmbH\nIm MediaPark 4c\n50670 Cologne\nOffice: 4e OG-3\nTelefon: +49 221 973199-523\nMail: stephan.huber@hs-fresenius.de\nPrivate homepage: www.hubchev.github.io\nGithub: https://github.com/hubchev\n\n\n\n\n\n\nFigure 1: Prof. Dr. Stephan Huber\n\n\n\n\n\n\nI am a Professor of International Economics and Data Science at HS Fresenius, holding a Diploma in Economics from the University of Regensburg and a Doctoral Degree (summa cum laude) from the University of Trier. I completed postgraduate studies at the Interdisciplinary Graduate Center of Excellence at the Institute for Labor Law and Industrial Relations in the European Union (IAAEU) in Trier. Prior to my current position, I worked as a research assistant to Prof. Dr. Dr. h.c. Joachim Möller at the University of Regensburg, a post-doc at the Leibniz Institute for East and Southeast European Studies (IOS) in Regensburg, and a freelancer at Charles University in Prague.\nThroughout my career, I have also worked as a lecturer at various institutions, including the TU Munich, the University of Regensburg, Saarland University, and the Universities of Applied Sciences in Frankfurt and Augsburg. Additionally, I have had the opportunity to teach abroad for the University of Cordoba in Spain, the University of Perugia in Italy, and the Petra Christian University in Surabaya, Indonesia. My published work can be found in international journals such as the Canadian Journal of Economics and the Stata Journal. For more information on my work, please visit my private homepage at hubchev.github.io.\nI was always fascinated by data and statistics. For example, in 1992 I could name all soccer players in Germany’s first division including how many goals they scored. Later, in 2003 I joined the introductory statistics course of Daniel Rösch. I learned among others that probabilities often play a role when analyzing data. I continued my data science journey with Harry Haupt’s Introductory Econometrics course, where I studied the infamous Jeffrey M. Wooldridge (2002) textbook. It got me hooked and so I took all the courses Rolf Tschernig offered at his chair of Econometrics, where I became a tutor at the University of Regensburg and a research assistant of Joachim Möller. Despite everything we did had to do with how to make sense out of data, we never actually used the term data science which is also absent in the more 850 pages long textbook by Wooldridge (2002). The book also remains silent about machine learning or artificial intelligence. These terms became popular only after I graduated. The Harvard Business Review article by Davenport & Patil (2012) who claimed that data scientist is “The Sexiest Job of the 21st Century” may have boosted the popularity.\nThe term “data scientist” has become remarkably popular, and many people are eager to adopt this title. Although I am a professor of data science, my professional identity is more like that of an applied, empirically-oriented international economist. My hesitation to adopt the title “data scientist” also stems from the deep respect I have developed through my interactions with econometricians and statisticians. Considering their in-depth expertise, I feel like a passionate amateur.\nUltimately, I poke around in data to find something interesting. Much like my ten-year-old younger self who analyzed soccer statistics to gain a deeper understanding of the sport. The only thing that has changed since then is that I know more promising methods and can efficiently use tools for data processing and data analysis.\n\n\nTo students\nI’m not a business leader myself. I’m a professor, and like most of my peers, I don’t run a big company nor do I hold a top position in a multinational corporation. Actually, I believe that leading a company successfully full-time is not compatible with being a full-time professor, and vice versa. Despite this, in my role as a professor and study program director, I do have certain responsibilities and occasionally lead people, particularly students. This, however, doesn’t let me feel like I am a business leader. Is this necessary to teach you a course entitled “Data Science for Business Leaders”? I don’t think so: Firstly, if you want to become a business leader, do not pick a role model that works in academics like I do. Secondly, I believe I can provide you with valuable insights into data science to support you on the journey of becoming a business leader.    And thirdly, this course is not entitled “I am a business leader who teaches you data science”. Thus, this course is “for” business leaders and “for” you and not about me or my personality.\nEnjoy.\n\n\n\nFigure 1: Prof. Dr. Stephan Huber\n\n\n\nDavenport, T. H., & Patil, D. (2012). Data scientist: The sexiest job of the 21st century. Harvard Business Review, 90(5), 70–76.\n\n\nWooldridge, J. M. (2002). Introductory econometrics: A modern approach. In Delhi: Cengage Learnng (2nd ed.). South-Western.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "esyl.html",
    "href": "esyl.html",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "1.1 Syllabus\nScope and Nature of Data Science\nEmerging Trends in a Data-Driven Business Environment\nData Science Process in Business\nData Literacy\nOverview of Data Science Methods\nIntroduction to Data Scientific Tools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-osly",
    "href": "esyl.html#sec-osly",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "Defining data science as an academic discipline (informatics, computer science, mathematics, statistics, econometrics, social science)\nImportance of data science in businesses\n\n\n\nEvolution of computers, computing, and data processing\nBusiness intelligence (performance marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\n\nWorkflows and data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve data)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview of programming languages (R, Python, SQL, …)\nOverview of no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-howchatgptwrote",
    "href": "esyl.html#sec-howchatgptwrote",
    "title": "1  (Extended) Syllabus",
    "section": "1.2 How I let ChatGPT wrote the extended syllabus",
    "text": "1.2 How I let ChatGPT wrote the extended syllabus\nHere are the seven prompts to ChatGPT 4.0:\n\nAct as a professor of data science. Write lecture notes for management students. The first chapter of the notes should contain the following:\n\nScope and nature of data science\n\nDefining data science as an academic discipline (informatics, computer science, mathematics,statistics, econometrics, social science)\nImportance of data science in businesses\n\n\nThe chapter of the lecture notes should contain the following:\n\nEmerging Trends in a Data-Driven Business Environment\n\nEvolution of computers, computing, and data processing\nBusiness Intelligence (Performance Marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\nThe third chapter of the lecture notes should contain the following:\n\nData science process in business\n\nWorkflows and Data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\nDo the same for the fourth section, which contains:\n\nData literacy\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve Kdata)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\nAnd the next section:\n\nOverview of data science methods\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\nAnd the final section:\n\nIntroduction to data scientific tools\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview on Programming languages (R, Phyton, SQL, …)\nOverview on no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)\n\n\nI am not happy with the third section. In particular the section “Workflows and Data Science Life Cycles”. Can you re-write it? Consider the learning objective a bit better, that is, summarize the strengths, weaknesses and use-cases of data science processes and workflows and demonstrate the ability to justify and explain tools and methods used along the process.\n\nI manually cut some parts of the last answer and pasted it into the respective section.\nI assembled everything by copy and paste it step-by-step into Quarto and render it. That was all I did to create the notes.\n\n\n\n\n\n\n\nExercise 1.1 What do think about the extended syllabus and my approach\nWhat do you think about the extended syllabus? Is it well-written? Does it include a lot of details or errors? What aspects do you think we should work on manually? How could my prompts be improved to yield better results?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-esylgpt",
    "href": "esyl.html#sec-esylgpt",
    "title": "1  (Extended) Syllabus",
    "section": "1.3 The ChatGPT generated extended syllabus",
    "text": "1.3 The ChatGPT generated extended syllabus\nThe following text was generated as described in Section 1.2 using OpenAI (2024).\n\n1.3.1 Scope and Nature of Data Science\nWelcome to the introductory chapter on Data Science, designed specifically for management students. In this chapter, we will explore the multifaceted discipline of data science, understanding its definitions, scope, and the pivotal role it plays in the business world today.\n\n1.3.1.1 Defining Data Science\nData Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It is a confluence of several disciplines including informatics, computer science, mathematics, statistics, econometrics, and social science. This integration allows for a comprehensive approach to solving complex problems, making informed decisions, and generating actionable insights through data analysis.\n\n1.3.1.1.1 Informatics\nIt is the study of computational systems and the way humans interact with these systems. In data science, informatics plays a critical role in the management and processing of data.\n\n\n1.3.1.1.2 Computer Science\nThis foundational pillar focuses on the development of algorithms and software that are used to process, store, and analyze data.\n\n\n1.3.1.1.3 Mathematics and Statistics\nMathematics provides the theoretical foundation for algorithms and models, while statistics is crucial for understanding and interpreting data. Together, they enable data scientists to perform quantitative analysis and predictive modeling.\n\n\n1.3.1.1.4 Econometrics\nEconometrics applies statistical methods to economic data, enabling the analysis of economic relationships. In data science, econometrics helps in forecasting economic trends and evaluating economic policies through data.\n\n\n1.3.1.1.5 Social Science\nThe incorporation of social science principles allows data scientists to understand human behavior and societal trends, which is invaluable for businesses looking to market their products or services effectively.\n\n\n\n1.3.1.2 Importance of Data Science in Businesses\nData Science has become a cornerstone in the business world, offering unprecedented opportunities for growth, efficiency, and innovation. Here’s how:\n\n1.3.1.2.1 Data-Driven Decision Making\nBusinesses can make more informed decisions by analyzing vast amounts of data. This leads to better strategic planning, operational efficiency, and competitive advantage.\n\n\n1.3.1.2.2 Customer Insights\nThrough data analysis, companies can gain deep insights into customer behavior, preferences, and trends. This information is critical for tailoring marketing strategies, improving customer experiences, and boosting customer loyalty.\n\n\n1.3.1.2.3 Risk Management\nData science enables businesses to identify and mitigate risks by analyzing historical data, detecting patterns, and predicting future outcomes. This is particularly useful in finance, insurance, and cybersecurity.\n\n\n1.3.1.2.4 Innovation and Product Development\nBy understanding market needs and customer feedback through data analysis, businesses can innovate and develop new products and services that meet or exceed customer expectations.\n\n\n1.3.1.2.5 Operational Efficiency\nData science can optimize business operations, reduce costs, and improve service delivery through the analysis of operational data, leading to streamlined processes and better resource management.\n\n\n1.3.1.2.6 Competitive Edge\nIn a data-driven world, businesses that leverage data science effectively can stay ahead of the curve, identifying trends and opportunities before their competitors.\nIn conclusion, data science is an essential academic discipline that integrates various fields to analyze and interpret complex data. Its importance in the business sector cannot be overstated, as it enables organizations to make informed decisions, understand their customers, manage risks, foster innovation, improve operations, and maintain a competitive edge in the market. As we delve deeper into this course, we will explore the methodologies, tools, and applications of data science in greater detail, equipping you with the knowledge and skills to leverage data science in your future management roles.\n\n\n\n\n1.3.2 Emerging Trends in a Data-Driven Business Environment\nIn this chapter, we explore the dynamic landscape of a data-driven business environment. The rapid evolution of technology and its integration into business processes have led to the emergence of new trends that are shaping the future of industries across the globe. Let’s dive into these trends and understand their significance.\n\n1.3.2.1 Evolution of Computers, Computing, and Data Processing\nThe journey from the first generation of computers to the modern era of quantum computing marks a significant evolution in technology. Initially, computers were large, expensive, and limited in their capabilities. Over the decades, advancements in semiconductor technology, the invention of the microprocessor, and the development of personal computers transformed computing into an accessible and essential tool for businesses. Today, cloud computing and edge computing have further revolutionized data processing, allowing for more efficient data storage, access, and analysis.\n\n\n1.3.2.2 Business Intelligence\nBusiness Intelligence (BI) refers to the use of data analysis in business to support decision-making processes. BI tools analyze historical and current data to provide actionable insights, helping businesses to improve their performance. Performance marketing, a subset of BI, focuses on analyzing marketing campaigns in real time to optimize marketing strategies and expenditures for better ROI.\n\n\n1.3.2.3 Artificial Intelligence, Machine Learning, Deep Learning, and Algorithms\nArtificial Intelligence (AI) and its subsets, Machine Learning (ML) and Deep Learning (DL), are at the forefront of technological innovation. AI involves creating systems capable of performing tasks that typically require human intelligence. ML and DL are about teaching computers to learn from data, improving their accuracy over time without being explicitly programmed. These technologies are transforming business operations, from customer service automation and predictive analytics to personalized marketing and decision-making processes.\n\n\n1.3.2.4 Big Data\nBig Data refers to the vast volumes of data generated every second from various sources like social media, business transactions, and IoT devices. The ability to process and analyze this data has unlocked new opportunities for businesses to gain insights into customer behavior, market trends, and operational efficiency. Big data analytics is now a crucial tool for strategic planning and competitive analysis.\n\n\n1.3.2.5 Internet of Things, Cloud Computing, Blockchain\n\nInternet of Things (IoT): IoT technology connects everyday devices to the internet, enabling them to send and receive data. This interconnectivity offers businesses real-time insights into their operations, asset tracking, and supply chain management.\nCloud Computing: Cloud computing provides businesses with scalable computing resources over the internet, facilitating remote data storage, processing, and management. It supports flexibility, reduces IT costs, and enhances collaboration.\nBlockchain: Blockchain technology offers a secure, decentralized platform for transactions. It’s particularly valuable for enhancing transparency, security, and efficiency in business operations, supply chain management, and financial services.\n\n\n\n1.3.2.6 Industry 4.0 and Remote Working\nIndustry 4.0, also known as the fourth industrial revolution, integrates digital technologies into manufacturing and industry, including IoT, AI, and robotics. It represents a shift towards smart, automated production processes and data exchange. Coupled with the rise of remote working, Industry 4.0 technologies enable businesses to operate more flexibly, with teams collaborating effectively from various locations, leveraging digital tools and platforms for communication and project management.\nIn conclusion, the evolution of technology and its integration into business practices have brought about significant changes in the way companies operate. From the way data is processed and analyzed to the automation of manufacturing processes and the flexibility of remote working, these emerging trends are shaping the future of a data-driven business environment. As we move forward, understanding and adapting to these trends will be crucial for businesses looking to thrive in the digital age.\n\n\n\n1.3.3 Data Science Process in Business\nThis chapter delves into the structured approach behind the application of data science in business settings. We will explore various data science workflows and life cycles that guide the process from raw data to actionable insights. Additionally, we will outline the different roles within a data science team and their contributions to this process.\n\n1.3.3.1 Workflows and Data Science Life Cycles\nData science projects in business environments follow structured workflows and life cycles to ensure that the analysis is efficient, reproducible, and scalable. Several frameworks guide these processes, each with its strengths and applications.\n\n1.3.3.1.1 OSEMN Framework\nOSEMN (Obtain, Scrub, Explore, Model, iNterpret) is a streamlined approach to data science projects:\n\nObtain: Acquiring the data from various sources.\nScrub: Cleaning the data to ensure it is accurate and usable.\nExplore: Analyzing the data to find patterns and relationships.\nModel: Applying statistical models to predict or classify data.\nInterpret: Drawing conclusions and making recommendations based on the model’s results.\n\n\nStrengths: The OSEMN (Obtain, Scrub, Explore, Model, iNterpret) framework is straightforward and easy to understand, making it accessible for teams of all skill levels. It covers the essential steps of a data science project in a logical sequence.\nWeaknesses: Its simplicity may overlook the complexity of certain stages, such as model validation or deployment.\nUse-Cases: Ideal for small to medium-sized projects where the primary goal is to gain insights from data through exploration and modeling.\n\n\n\n1.3.3.1.2 CRISP-DM\nCRISP-DM stands for Cross-Industry Standard Process for Data Mining. It’s a comprehensive framework that includes six phases:\n\nBusiness Understanding: Define the project objectives and requirements.\nData Understanding: Collect and explore the data.\nData Preparation: Clean and preprocess the data.\nModeling: Select and apply modeling techniques.\nEvaluation: Assess the model’s performance.\nDeployment: Implement the model in a real-world setting.\n\n\nStrengths: CRISP-DM (Cross-Industry Standard Process for Data Mining) is industry-agnostic and provides a detailed structure that includes understanding the business problem and deploying the solution. It encourages iterative learning and refinement.\nWeaknesses: Can be perceived as too rigid for projects requiring rapid development and deployment. The model doesn’t explicitly address the updating or maintenance of deployed solutions.\nUse-Cases: Suitable for projects that require close alignment with business objectives and thorough consideration of deployment strategies.\n\n\n\n1.3.3.1.3 Kanban\nKanban is a lean method to manage and improve work across human systems. In data science, it helps in visualizing work, limiting work-in-progress, and maximizing efficiency.\n\nStrengths: Kanban is highly flexible and promotes continuous delivery. It allows teams to adapt quickly to changes and prioritize tasks effectively.\nWeaknesses: Without strict stages or phases, projects might lack direction or oversight, potentially leading to inefficiencies.\nUse-Cases: Best for dynamic environments where priorities shift frequently and teams must remain agile to respond to business needs.\n\n\n\n1.3.3.1.4 TDSP (Team Data Science Process)\nTDSP is a standardized approach to data science projects that helps teams to improve quality and efficiency. It includes:\n\nStrengths: TDSP offers a structured approach with a strong emphasis on standardized documentation and project management methodologies, facilitating collaboration and scalability.\nWeaknesses: Its comprehensive nature might introduce overhead and slow down smaller projects.\nUse-Cases: Ideal for larger teams working on complex projects that require coordination across different roles and departments.\n\n\n\n\n1.3.3.2 Types of Data Science Roles\nIn a business environment, a data science team might consist of various specialized roles, each contributing uniquely to the data science process.\n\n1.3.3.2.1 Data Engineer\nFocuses on the design, construction, and maintenance of the systems that data analysts and data scientists use for their work. They ensure that data flows smoothly from source to database to analytics.\n\n\n1.3.3.2.2 Data Analyst\nWorks on processing and performing statistical analysis on existing datasets. They interpret the data to help the business make more informed decisions.\n\n\n1.3.3.2.3 Machine Learning Engineer\nDevelops algorithms and predictive models to solve specific business problems using machine learning techniques.\n\n\n1.3.3.2.4 Business Intelligence Analyst\nAnalyzes data to provide insights that help businesses with strategic planning. They use BI tools to convert data into understandable reports and dashboards.\n\n\n1.3.3.2.5 Database Administrator\nResponsible for managing, backing up, and ensuring the availability of the data stored in an organization’s databases.\n\n\n1.3.3.2.6 Data Product Manager\nOversees the development of data-driven products or services, ensuring that they meet the users’ needs and the business objectives.\nIn summary, the data science process in business involves a structured approach to turning data into actionable insights. This process is supported by various frameworks and relies on the collaboration of professionals in specialized roles. Understanding these aspects of data science is crucial for anyone looking to leverage this discipline in a business context.\n\n\n\n\n1.3.4 Data Literacy\nData literacy is the ability to read, understand, create, and communicate data as information. It encompasses a broad range of skills necessary for effectively working with data, from the initial stages of data collection to the final stages of analyzing and sharing findings. In this chapter, we will break down the conceptual framework of data literacy and explore its various components in detail.\n\n1.3.4.1 Conceptual Framework\nAt the heart of data literacy is a deep knowledge and understanding of how data can be used to make decisions, solve problems, and communicate ideas. This conceptual framework involves:\n\nUnderstanding the nature of data: Recognizing different types of data (quantitative vs. qualitative) and their sources.\nComprehending the applications of data: Knowing how data can be used in various contexts to derive insights and inform decisions.\n\n\n\n1.3.4.2 Data Collection\nThe first step in the data lifecycle involves identifying, collecting, and assessing data:\n\nIdentify: Determining the data needed to answer a question or solve a problem.\nCollect: Gathering data from various sources, whether they are existing datasets or new data collected through surveys, experiments, or observations.\nAssess: Evaluating the quality of the data, including its relevance, accuracy, and completeness.\n\n\n\n1.3.4.3 Data Management\nOnce data is collected, it must be managed effectively:\n\nOrganize: Arranging data in a structured format that facilitates analysis.\nClean: Removing errors or inconsistencies in the data.\nConvert: Transforming data into a format suitable for analysis.\nCurate: Selecting, annotating, and maintaining valuable data for current and future use.\nPreserve: Ensuring that data remains accessible and usable over time.\n\n\n\n1.3.4.4 Data Evaluation\nEvaluation is critical to understanding what the data signifies:\n\nPlan: Designing a methodology for analyzing the data.\nConduct: Performing the analysis using appropriate statistical methods and tools.\nEvaluate: Assessing the quality and reliability of the analysis.\nAssess: Interpreting the results in the context of the research question or business problem.\n\n\n\n1.3.4.5 Data Application\nThe final step involves applying the insights gained from data analysis:\n\nShare: Communicating findings to stakeholders through reports, presentations, or visualizations.\nReflect: Considering the implications of the results and how they can inform future actions.\nEvaluate results: Comparing findings with those from other studies or data analyses to draw broader conclusions.\nEthical considerations: Ensuring that the use of data respects privacy, confidentiality, and ethical standards.\nScientific standards: Adhering to rigorous standards of validity, reliability, and objectivity in data handling and analysis.\n\nIn summary, data literacy is a comprehensive set of skills that enable individuals to navigate the complex world of data from collection to application. By understanding and applying the concepts outlined in this chapter, individuals can enhance their ability to make informed decisions, solve problems, and communicate effectively using data.\n\n\n\n1.3.5 Overview of Data Science Methods\nData science encompasses a wide array of methods and techniques for analyzing data, drawing insights, and making predictions. This chapter provides an overview of some core data science methods, including data exploration, data mining, machine learning approaches, and various types of analyses.\n\n1.3.5.1 Data Exploration and Data Mining\n\n1.3.5.1.1 Data Exploration\nData exploration involves analyzing data sets to find initial patterns, characteristics, and points of interest without making any prior assumptions. It typically includes summarizing the main characteristics of the data through visualizations and statistics.\n\n\n1.3.5.1.2 Data Mining\nData mining is the process of discovering patterns and knowledge from large amounts of data. The data sources can include databases, data warehouses, the internet, and other sources. Data mining techniques include clustering, classification, regression, and association rule learning.\n\n\n\n1.3.5.2 Supervised and Unsupervised Learning\n\n1.3.5.2.1 Supervised Learning\nSupervised learning is a type of machine learning where the algorithm learns from labeled training data, helping to predict outcomes for unforeseen data. It is divided into two main categories: regression and classification.\n\n\n1.3.5.2.2 Unsupervised Learning\nUnsupervised learning involves training on data without labeled responses. The system tries to learn the patterns and the structure from the data without any supervision. Common unsupervised learning methods include clustering and dimensionality reduction.\n\n\n\n1.3.5.3 Regression and Classification\n\n1.3.5.3.1 Regression\nRegression methods are used to predict a continuous outcome variable based on one or more predictor variables. The goal is to find the relationship between variables and forecast an outcome. Linear regression is one of the most basic types of regression analysis.\n\n\n1.3.5.3.2 Classification\nClassification methods are used to predict or identify the category to which a new observation belongs. Examples include spam detection in email service providers and customer churn prediction.\n\n\n\n1.3.5.4 Predictive Analysis\nPredictive analysis uses statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. It’s used in various fields, including finance, healthcare, marketing, and weather forecasting, to make more informed decisions.\n\n\n1.3.5.5 Causal Analysis\nCausal analysis seeks to identify and understand the cause-and-effect relationships between variables. Unlike correlation, which merely indicates that two variables move together, causation establishes that a change in one variable is responsible for a change in another.\nIn conclusion, these methods and techniques form the backbone of data science, enabling professionals to extract valuable insights, make predictions, and inform decision-making processes. Understanding these methods is crucial for anyone looking to delve into data science or apply its principles in their field.\n\n\n\n1.3.6 Introduction to Data Scientific Tools\nThe practice of data science requires not only a solid understanding of theories and methodologies but also proficiency in a variety of tools and technologies. This chapter introduces essential tools for writing and publishing reports, collaborating in teams, programming, as well as no-code and low-code platforms, and development environments.\n\n1.3.6.1 Writing and Publishing Reports\n\n1.3.6.1.1 Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. Its simplicity and ease of conversion to HTML and other formats make it an ideal choice for writing and publishing reports, documentation, and articles.\n\n\n1.3.6.1.2 Quarto\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It enables users to create dynamic and reproducible reports and articles that can include executable code from various programming languages, such as R and Python.\n\n\n\n1.3.6.2 Collaborating in Teams Using a Version Control System\n\n1.3.6.2.1 Git\nGit is a distributed version control system that enables multiple developers to work together on the same project efficiently. It tracks changes in source code during software development, supporting collaboration and fostering code integrity.\n\n\n\n1.3.6.3 Overview of Programming Languages\n\n1.3.6.3.1 R\nR is a programming language and free software environment for statistical computing and graphics, widely used among statisticians and data miners.\n\n\n1.3.6.3.2 Python\nPython is a high-level, interpreted programming language known for its simplicity and versatility. It has a wide range of libraries for data analysis, machine learning, and data visualization, making it a popular choice in data science.\n\n\n1.3.6.3.3 SQL\nSQL (Structured Query Language) is the standard language for managing and manipulating relational databases. It allows users to query, update, and manage data.\n\n\n\n1.3.6.4 Overview of No-Code and Low-Code Tools for Data Science\n\n1.3.6.4.1 makeML\nA no-code platform for machine learning, makeML simplifies the process of training and deploying ML models without writing extensive code.\n\n\n1.3.6.4.2 PyCaret\nPyCaret is a low-code machine learning library in Python that automates machine learning workflows. It enables data scientists to perform end-to-end experiments quickly and efficiently.\n\n\n1.3.6.4.3 Rapidminer\nRapidminer is a data science platform that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics.\n\n\n1.3.6.4.4 KNIME\nKNIME is an open-source, graphical workbench for the entire analysis process: data access, data transformation, initial investigation, powerful predictive analytics, visualization, and reporting.\n\n\n\n1.3.6.5 Development Environments\n\n1.3.6.5.1 Unix-like Systems\nUnix-like operating systems, including Linux and macOS, provide powerful tools and environments for software development and data science.\n\n\n1.3.6.5.2 Containers\nContainers, such as Docker, allow for the packaging of applications and their dependencies in a virtual container that can run on any Linux server, enabling easy deployment and scalability.\n\n\n1.3.6.5.3 APIs\nApplication Programming Interfaces (APIs) enable different software applications to communicate with each other, facilitating data exchange and integration.\n\n\n1.3.6.5.4 Jupyter\nJupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text.\n\n\n1.3.6.5.5 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for coding, debugging, and visualizing data.\nIn summary, the array of tools and technologies available to data scientists is broad and varied, catering to different aspects of the data science workflow. From data manipulation and analysis to collaboration and report writing, mastering these tools is essential for effective data science practice.\n\n\n\n\nOpenAI. (2024). ChatGPT (april 18, 2024). Large language model. https://chat.openai.comt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "identification.html",
    "href": "identification.html",
    "title": "2  Methods of data science",
    "section": "",
    "text": "2.1 Identification\nRead “How Successful Leaders Think” by Roger Martin (2007) and the chapter “Identification” of “Quantitative Methods” by Huber (2024). Discuss the concepts introduced by Martin (2007) critically:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods of data science</span>"
    ]
  },
  {
    "objectID": "identification.html#identification",
    "href": "identification.html#identification",
    "title": "2  Methods of data science",
    "section": "",
    "text": "Does he provide evidence for his ideas to work?\nIs there a proof that his suggestions can yield success?\nIs there some evidence about whether his ideas are superior to alternative causes of action?\nWhat can we learn from the article?\nDoes his argumentation fulfill highest academic standards?\nWhat is his identification strategy with respect to the causes of effects and the effects of causes?\nMartin (2007, p. 81) speculates:\n\n\n“At some point, integrative thinking will no longer be just a tacit skill (cultivated knowingly or not) in the heads of a select few.”\n\n\nIf teachers in business schools would have followed his ideas of integrative thinkers being more successful, almost 20 years later, this should be the dominant way to think as a business leader. Is that the case? And if so, can you still gain some competitive advantage by thinking that way?\n\n\n\n\n\nHuber, S. (2024). Quantitative methods: Lecture notes. https://hubchev.github.io/qm/\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business Review, 85(6), 75–83.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods of data science</span>"
    ]
  },
  {
    "objectID": "htur4ds.html",
    "href": "htur4ds.html",
    "title": "3  How to use R for data science",
    "section": "",
    "text": "The programming language R is one of the major tools to do data science. I wrote some lecture notes on How to use R for data science (Huber, 2024).\nPlease read these notes.\n\n\n\n\nHuber, S. (2024). How to use R for data science. https://hubchev.github.io/ds/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How to use R for data science</span>"
    ]
  },
  {
    "objectID": "git-github.html",
    "href": "git-github.html",
    "title": "4  Collaborating with Git and GitHub",
    "section": "",
    "text": "4.1 Introduction\nGit is open-source software for version control. It allows developers to track and manage changes to their codebase and files. Users can access a comprehensive history of their project and revert to previous versions of their data if necessary. It helps to overcome the FINAL.doc problem depicted in Figure 4.1.\nGitHub is an incredibly popular (see statistics in Figure 4.2) online platform that implements Git’s capabilities by providing a web interface for collaboration.\nWhile you can use Git and GitHub independently, most developers integrate it with GitHub for enhanced project management and collaboration. This combination helps maintain local and remote copies of a project, facilitating teamwork and data backup as GitHub is sort of a backup as data loss at your local machine do not matter if you have a remote version saved on GitHub.\nGit and GitHub support simultaneous multi-user access, unlike systems that are optimized for single-user like Dropbox.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#introduction",
    "href": "git-github.html#introduction",
    "title": "4  Collaborating with Git and GitHub",
    "section": "",
    "text": "Figure 4.2: GitHub is big\n\n\n\nSource: https://github.com/about as of April 2024",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitsection",
    "href": "git-github.html#sec-gitsection",
    "title": "4  Collaborating with Git and GitHub",
    "section": "4.2 Install Git",
    "text": "4.2 Install Git\nTo install the version control system Git, follow the instructions here.\n\n\n\nFigure 4.3: Memorizing six git commands\n\n\n\nSource: DEV Community on GitHub\n\n\n\nFamiliarize yourself with Git by using the resources available here. Specifically, work through the resources listed in the box below. Although Git may appear complex, it is generally not too challenging for most users. Many people use Git primarily to track their work and to host and share files conveniently with just a handful of commands. While Git is a robust system with many capabilities, you don’t need to memorize all the commands (see Figure 4.3). In fact, you typically use only a few basic ones as shown in Table 4.1.\nIn the upcoming sections, I will demonstrate some use cases both in the terminal Section 4.3 and within RStudio Section 4.4. In Section 4.5, I show how to contribute to a repository using Git and GitHub.\n\n\n\n\n\n\nLearning resources\n\n\n\nPlenty books and tutorial exist that introduce Git and GitHub. I’d like to highlight the following sources:\n\nThe book comprehensive book Happy Git and GitHub for the useR by Bryan (2023)\nThe much shorter book [Version Control with Git and GitHub] by Halbritter & Telford (2023)\nThe online tutorial How to Use Git/GitHub with R of David Keyes who explains in short videos how to setup Git and GitHub in RStudio using among others the usethis package.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.1: Most important git commands\n\n\n\n\n\n\n\n\n\nGit Command\nDescription\n\n\n\n\ngit init\nInitialize a new Git repository in the current directory.\n\n\ngit clone &lt;url&gt;\nClone a repository from a remote URL to your local machine.\n\n\ngit add &lt;file&gt;\nAdd a specific file to the staging area in preparation for committing.\n\n\ngit add .\nAdd all changed files in the current directory to the staging area.\n\n\ngit commit -m \"message\"\nCommit the staged changes to the repository with a descriptive message.\n\n\ngit status\nDisplay the status of the working directory and staging area.\n\n\ngit push &lt;remote&gt; &lt;branch&gt;\nPush committed changes in your local branch to the remote repository.\n\n\ngit pull &lt;remote&gt; &lt;branch&gt;\nPull changes from the remote repository into your current branch and merge them.\n\n\ngit branch &lt;name&gt;\nCreate a new branch with the specified name.\n\n\ngit checkout &lt;branch&gt;\nSwitch to another branch and update the working directory.\n\n\ngit merge &lt;branch&gt;\nMerge a specified branch into the current branch.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitterminal",
    "href": "git-github.html#sec-gitterminal",
    "title": "4  Collaborating with Git and GitHub",
    "section": "4.3 Using Git from the terminal",
    "text": "4.3 Using Git from the terminal\n\n\n\nFigure 4.4: Three git commands you really need\n\n\n\n\n\n\nThis tutorial will guide you through the basic Git operations using the Bash command line, commonly referred to as the terminal. Essentially, it focuses on the three Git commands illustrated in Figure 4.4.\n\n4.3.1 Configuring Git\nBefore you start using Git, you need to configure your Git environment. Set your username and email address with these commands:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\n\n4.3.2 Initializing a Repository\nTo create a new Git repository, use the git init command in the directory you want to version control:\ncd /path/to/a/directory\nmkdir my_project\ncd my_project\ngit init\nIn case you are not familiar with using the terminal please consider Table 4.2 where I introduce the most basic commands that we use. For example, with cd you can change your directory and with mkdir you create a new directory. If you are not familiar with the file system of your computer please read the section Navigating the file system of Huber (2024). With git init you initialize the directory to be a git repository. This will create a hidden folder “.git” in which Git keeps track of all your changes.\n\n\n\n\n\n\nMost common bash commands\n\n\n\n\n\n\n\n\nTable 4.2: Most common bash commands\n\n\n\n\n\n\n\n\n\n\nBash Command (macOS/Linux)\nWindows Command Prompt Equivalent\nDescription\n\n\n\n\npwd\ncd\nPrints the current directory’s path.\n\n\nls\ndir\nLists all files and directories in the current directory.\n\n\ncd\ncd\nChanges the directory.\n\n\nmkdir\nmkdir\nCreates a new directory.\n\n\nrmdir\nrmdir\nRemoves an empty directory.\n\n\ntouch\ncopy nul\nCreates a new empty file or updates an existing file’s timestamp.\n\n\nrm\ndel or erase\nRemoves files. rmdir /s is used for directories.\n\n\ncp\ncopy\nCopies files or directories.\n\n\nmv\nmove\nMoves or renames files or directories.\n\n\necho\necho\nDisplays a line of text/string.\n\n\ncat\ntype\nConcatenates and displays the content of files.\n\n\ngrep\nfind or findstr\nSearches for patterns in files.\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 Staging Changes\nTo track changes in your repository, you need to stage them using the git add command. To stage a single file:\ngit add filename.txt\nTo stage all changes in the directory:\ngit add .\n\n\n4.3.4 Committing Changes\nAfter staging, you can commit it to the repository. A commit records changes to the repository and must include a message describing what changed:\ngit commit -m \"A message\"\n\n\n4.3.5 Pushing Changes\nTo share your commits with others or store them in a remote repository (GitHub), use git push. A prerequisite here is that you need to be connected to a remote repo. Therefore, you must add a remote repository by copying the URL of the GitHub repo as shown in Figure 4.5. Then you can add the remote repository and push it to the repo with these lines of code:\ngit remote add origin https://github.com/username/repository.git\ngit push -u origin main\n\n\n\nFigure 4.5: Copy the https URL of your repo\n\n\n\n\n\n\n\n\n4.3.6 Undo changes\nWith git reset and git revert you can go back in time and undo specific changes, respectively. For example, with\ngit log\ngit reset --hard &lt;commit_id_hash&gt;\nyou can view the commit history and find the hash identifier of the commit to move the HEAD pointer to that commit. This effectively removes all commits after commit you choose from the current branch’s history. Be cautious when using git reset –hard as it discards all changes made after the specified commit. Make sure you have backups or are certain you want to discard these changes before proceeding.\nWith\ngit revert &lt;commit_id_hash&gt;\nyou revert the changes introduced by that commit only. It will create a new commit that undoes the changes made in commit chosen while keeping the other commits that may have followed the chosen commit intact. It’s a safer approach compared to git reset --hard, as it preserves the commit history and allows you to selectively undo changes without affecting the rest of the commits.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-gitstudio",
    "href": "git-github.html#sec-gitstudio",
    "title": "4  Collaborating with Git and GitHub",
    "section": "4.4 Using Git from RStudio",
    "text": "4.4 Using Git from RStudio\nIntegrating Git with RStudio enhances your project management by utilizing version control directly within the IDE. Here’s how you can set up and use Git in RStudio using R code.\n\n4.4.1 Set up Git in RStudio\nFirst, ensure the usethis package is installed and loaded:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis)\n\nConfigure your Git settings in RStudio:\n\nuse_git_config(user.name = \"Your Name\", user.email = \"Your@email.com\")\n\nYou can change the configuration of your user name and email using the edit_git_config() function.\nStart a new project in RStudio, which will also initialize a Git repository:\n\ncreate_project(\"~/Music/\")\nuse_git()\n\nAfter restarting RStudio, you will notice a Git tab in the top right panel, indicating that Git is now active for your project.\n\n\n4.4.2 Connecting RStudio Projects with GitHub repositories\nTo connect your RStudio project with GitHub, you need a Personal Access Token (PAT) on GitHub. If you haven’t one already, you can use the create_github_token() function from usethis package, and store the PAT securely with gitcreds_set from the gitcreds package:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis gitcreds)\ncreate_github_token()\ngitcreds::gitcreds_set()\n\nNow, the procedure depends on whether the project has bin initialized on your local machine and you want to create a repo on GitHub, or the repo already exists on GitHub and you want to connect that remote repo with your local PC. Both ways are described below.\n\n4.4.2.1 Project exists on RStudio first\nAfter initializing Git in your project, use the use_github() function from usethis to create a new GitHub repository and connect it directly:\n\nuse_github()\n\nThis creates a repo on your GitHub account.\n\n\n4.4.2.2 Project exists on GitHub first\nAlternatively, suppose you have created a repository on GitHub first, then start a new project in RStudio using the version control option, specifying your new repository’s URL. Just click File &gt; New Project &gt; Version Control and then link the GitHub repo by putting the URL into the respective box of the menu. See Figure 4.5 how to get the URL of a repo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "git-github.html#sec-pullrequest",
    "href": "git-github.html#sec-pullrequest",
    "title": "4  Collaborating with Git and GitHub",
    "section": "4.5 Make a contribution using Git and GitHub",
    "text": "4.5 Make a contribution using Git and GitHub\nThis is a guide for beginners on how to make a contribution using Git and GitHub. If you are looking to make your first contribution, follow the steps below.\n1. Create an account on GitHub\nIt is for free and should just take some minutes.\n2. Install Git\nHere is a tutorial on how to set up Git.\n3. Fork this repository\nClick on the fork button (see Figure 4.6) on the top of this page: https://github.com/hubchev/make_a_pull_request. This will create a copy of this repository in your account.\n\n\n\nFigure 4.6: Fork the repo\n\n\n\n\n\n\n4. Clone the forked repository\nGo to your GitHub account, open the forked repository, click on the code button and then click the copy to clipboard icon, see Figure 4.5.\nThen, open a terminal and run the following git command:\ngit clone \"url you just copied\"\nwhere “url you just copied” (without the quotation marks) is the url to this repository (your fork of this project). See the previous steps to obtain the url.\nFor example:\ngit clone https://github.com/hubchev/ds_summer23.git\nwhere this-is-you is your GitHub username. Here you’re copying the contents of the first-contributions repository on GitHub to your computer.\n5. Create a branch\nChange to the repository directory on your computer (if you are not already there):\ncd ds_summer23\nNow create a branch using the git switch command:\ngit switch -c your-new-branch-name\nFor example:\ngit switch -c add-Stephan-Huber\n6. Make changes.\nNow open theI_am_a_data_scientist.md file in a text editor. (You find this file in the repository.) Add your name, your GitHub account and the project you are working on. You can put it anywhere in between. Now, save the file.\nIf you go to the project directory and execute the command git status, you’ll see there are changes.\n7. Add changes (staging). Add those changes to the branch you just created using the git add command:\ngit add .\n8. Commit changes. Now commit those changes using the git commit command:\ngit commit -m \"Add your-name to the list\"\nreplacing your-name with your name.\n9. Use Git Bash. Open Git Bash and set your email and your nickname on GitHub:\ngit config --global user.name \"FIRST_NAME LAST_NAME\"\ngit config --global user.email \"MY_NAME@example.com\"\n10. Push changes to GitHub.\nPush your changes using the command git push:\ngit push -u origin your-new-branch-name\nreplacing your-new-branch-name with the name of the branch you created earlier.\nIf you get any errors while pushing that refers to authentication failed something, go to GitHub’s tutorial on generating and configuring an SSH key to your account. Alternatively, you can watch this YouTube tutorial\n11. Submit your changes for review on GitHub.\nIf you go to your repository on GitHub, you’ll see a Compare & pull request button. Click on that button.\nNow submit the pull request.\nSoon I’ll be merging all your changes into the main branch of this project. You will get a notification email once the changes have been merged.\nCongrats! You just completed the standard fork -&gt; clone -&gt; edit -&gt; pull request workflow that you’ll often encounter as a contributor!\n\n\n\n\nFigure 4.1: The FINAL.doc problem\nFigure 4.2: GitHub is big\nFigure 4.3: Memorizing six git commands\nFigure 4.4: Three git commands you really need\nFigure 4.5: Copy the https URL of your repo\nFigure 4.6: Fork the repo\n\n\n\nBryan, J. (2023). Happy git and GitHub for the useR. https://happygitwithr.com/\n\n\nHalbritter, A., & Telford, R. J. (2023). Version control with git and GitHub. https://biostats-r.github.io/biostats/github/\n\n\nHuber, S. (2024). How to use R for data science. https://hubchev.github.io/ds/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Collaborating with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "54_quarto.html",
    "href": "54_quarto.html",
    "title": "5  Markdown and Quarto",
    "section": "",
    "text": "Verbal and non-verbal communication is important in business. This section is about writing and publishing texts, leaving out body language and writing skills. I will introduce some applications (Markdown, RMarkdown, Quarto) that data scientists often use to write and publish their work. I will also discuss the version control system git and the online platform GitHub, which can be used to create, store, manage and share files. These tools are the backbone of most data science collaborations. Once you master these tools, they can significantly enhance your efficiency and make your presentations more impactful, even if you are not directly involved in the field of data science.\n\nQuarto, a modern documentation system, is an excellent choice for writing, especially for projects that require rigorous data analysis, visualization, and reproducibility. This tutorial will guide you through producing various forms of text with Quarto. You can write reports, articles, theses, books, websites and many more with Quarto.\nStep 1: Learn Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. It’s an essential skill for using Quarto effectively. Start by learning enough Markdown to structure your thesis, including headings, lists, links, and code blocks.\nYou can learn Markdown (not R Markdown!) in 10 minutes. Just go to www.markdowntutorial.com and work through the interactive lessons.\nStep 2: Learn Quarto\nRead Telford (2023): Enough Markdown to Write a Thesis. This resource covers the basics and some advanced Markdown features that are useful for academic writing.\nMore extensive resources on how to do things with Quarto can be found at quarto.org.\n\n\n\n\n\n\nQuarto and R markdown\n\n\n\nQuarto is a relatively new tool. It can be considered the successor to R Markdown, as it is built upon R Markdown. Consequently, almost all R Markdown documents are compatible with Quarto. However, Quarto includes several improvements over R Markdown that enhance its ease of use. For a detailed description of all the differences and similarities between the two, you can read this article. For an introduction to R Markdown see Chapter 6.\n\n\n\n\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "50_rmarkdown.html",
    "href": "50_rmarkdown.html",
    "title": "6  Write with R Markdown",
    "section": "",
    "text": "Figure 6.1: Example of an R Markdown file\n\n\n\n\n\n\nR Markdown provides an authoring framework for data science. You can use a single R Markdown file to transcript your work, run code, and generate high quality reports, books, websites, articles, theses, blogs, and many more (see Figure 6.1).\nIn contrast to Quarto (see Chapter 5), which is the more recent format, R Markdown is around for some time and hence there are uncountable resources to learn it. For example:\n\nThe R Markdown Cheatsheet (see Figure 6.2) from Posit offers an overview on the most important features of R Markdown.\n\n\n\n\nFigure 6.2: R Markdown Cheatsheet from Posit\n\n\n\n\n\n\n\nThe book R Markdown Cookbook by Xie et al. (2020) (see Figure 6.3) offers an introduction. The online version of the book is regularly updated and free of costs.\n\n\n\n\nFigure 6.3: Xie et al. (2020): R Markdown Cookbook\n\n\n\n\n\n\n\nThe book R Markdown: The Definitive Guide by Xie et al. (2018) offers a comprehensive introduction. The online version of the book is regularly updated and free of costs.\n\n\n\n\nFigure 6.4: Xie et al. (2018): R Markdown: The Definitive Guide\n\n\n\n\n\n\nPlease watch the video What is R Markdown? and then study the R Markdown tutorial from RStudio.\n\n\n\n\n\n\n\n\nWorking directory in R Markdown\n\n\n\nThe working directory is by default set to the directory that contains the Rmd document. In case you want to use another directory you can do so by changing the working directory with setwd(). However, that is not persistent in R Markdown and only works for the current code chunk. After the code chunk has been evaluated, the working directory will be restored to the directory where the Rmd file is placed.\n\n\n\n\n\n\n\n\n\nExercise 6.1 Start Markdown and R Markdown\n\nYou can learn Markdown (not R Markdown!) in 10 minutes. Just go to https://www.markdowntutorial.com and work throught the interactive lessons.\nNow create your first R Markdown file in 3 minutes by doing the following:\n\nclick in RStudio on File &gt; New File &gt; R Markdown\nclick OK\nlook for a button entitled Knit and click it\nsave your file (it will be saved with .Rmd file extension)\n\nPlay around with the file. For example, change the output format can you create a word file or a presentation. Play around with the code chunks. Add a picture that you find somewhere online.\nSet your working directory to the folder where you have saved your first Rmd-file. Can you come up with a way to generate different output format with just one function.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.2 R Markdown cite literature\n\nCreate a new R Markdown file (File &gt; New File &gt; R Markdown), save the file in an empty folder, and knit it.\n\n\n\nMake a new script with File &gt; New File &gt; R Script.\nGo to https://scholar.google.de/ and search for osrmtime.\nClick on “cite” and “BibTeX”. Copy and paste everything that you see into your script and save the script as lit.bib. R Studio will ask you if you confirm the file type change. Click yes. Your lit.bib file should look like this:\n\n@article{huber2016calculate,\n  title={Calculate travel time and distance with OpenStreetMap \n    data using the Open Source Routing Machine (OSRM)},\n  author={Huber, Stephan and Rust, Christoph},\n  journal={The Stata Journal},\n  volume={16},\n  number={2},\n  pages={416--423},\n  year={2016},\n  publisher={SAGE Publications Sage CA: Los Angeles, CA}\n}\n\nAdd the text “bibliography: references.bib” to your YAML header of your R Markdown file so that it looks somehow like that:\n\n---\ntitle: \"Untitled\"\nauthor: \"Stephan Huber\"\ndate: \"`r Sys.Date()`\"\noutput: html_document\nbibliography: lit.bib\n---\n\nNow you can cite the OSRMTIME paper with @huber2016calculate somewhere in the text of your R Markdown file.\nKnit the R Markdown file and you should see the paper cited and a reference list at the end of the html report.\nYou can manipulate the citation style you can specify a CSL (Citation Style Language) file in the YAML header. For example the APA style can be chosen with:\n\ncsl: \"https://www.zotero.org/styles/apa.csl\"\nMany more citation styles can be found on github.com/citation-style-language and on the Zotero Style Repository.\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.3 Preparing APA journal articles (papaja)\nThere is an easy way to write a manuscript that follows all the APA rules using the package papaja written by two psychologists from Cologne. Please read their manual and consider their repository on GitHub.\nNow, install and load the package:\n\ninstall.packages(\"papaja\")\nlibrary(\"papaja\")\n\nThen, click “File &gt; New File &gt; R Markdown” and choose the “APA-style manuscript” from the section “from template”. Knit the R markdown template and you will have a template for a APA manuscript.\nApart from the obvious adjustments, I recommend to make at least two general adjustments: Change classoption to “doc” and linenumbers to “no”.\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.4 R Markdown template\nPlease follow the instructions below to access the file “23-09_ds-project-desc.Rmd” from my GitHub account:\n\nDownload the file from my GitHub account by clicking on the link provided here.\nSave the file in your working directory.\nUse the knit function to run the file, but be aware that it may not work properly at first. If you encounter any issues, troubleshooting may be required. Don`t worry, error messages will usually provide guidance to help you resolve the issue. Please note that the YAML header is sensitive to spacing, so be careful when setting it up to avoid breaking the code.\nIn the project template, I have used BibTeX to cite literature. This method is excellent for automating tedious tasks such as citing papers and generating reference lists based on citation styles, saving time and reducing the likelihood of citation errors. The literature cited is in a separate file, which can be found on one of my GitHub repositories.\n\n\n\n\n\n\n\n\nFigure 6.1: Example of an R Markdown file\nFigure 6.2: R Markdown Cheatsheet from Posit\nFigure 6.3: Xie et al. (2020): R Markdown Cookbook\nFigure 6.4: Xie et al. (2018): R Markdown: The Definitive Guide\n\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2018). R markdown: The definitive guide. Chapman; Hall/CRC.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Write with R Markdown</span>"
    ]
  },
  {
    "objectID": "website.html",
    "href": "website.html",
    "title": "7  Create and host a website",
    "section": "",
    "text": "7.1 Creating a website with Quarto\nThis tutorial guides you through creating a simple, yet professional-looking website using Quarto.\nStep W1: Install Quarto\nEnsure Quarto is installed on your system. If not, download and install it from Quarto’s official website.\nStep W2: Create a website\nFollow the tutorial that you find here.\nStep W3: Copy the _site directory\nAfter you have rendered your website a directory “_site” appears in the project folder that contains your website. Copy all files of that directory to a directory where you want to save your website. Let’s say my_website.\nIn the terminal you can do this with\nmkdir /home/sthu/my_website/\ncp -r /home/sthu/quarto_website/_site/* /home/sthu/my_website/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "website.html#hosting-the-website-on-github",
    "href": "website.html#hosting-the-website-on-github",
    "title": "7  Create and host a website",
    "section": "7.2 Hosting the website on GitHub",
    "text": "7.2 Hosting the website on GitHub\nR Studio and Quarto offers you various ways to publish the website. I explain you a way that worked out well for me.\nStep G1: Create a GitHub account\nGitHub will host your thesis website and manage version control for your thesis project. If you don’t already have a GitHub account, you’ll need to create one: Sign up at GitHub.\nStep G2: Create a repository\nCreate a repository. Name the repo with your username followed by github.io. You find a tutorial here.\nStep G3: Obtain a personal access token\nA personal access token (PAT) is required to authenticate with GitHub from Quarto and RStudio. This token allows you to push changes to your repository securely. Follow the instructions to create a personal access token on GitHub. Alternatively, you can do the following in R:\n\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(usethis)\ncreate_github_token() \n\nMake sure to note down your token and keep it secure. You’ll use this token in RStudio and Quarto to authenticate your GitHub operations.\nStep G4: Install and Learn Git\nSee Section 4.2.\nStep G5: Upload the website to GitHub\nUse the Terminal of R Studio. Go to the directory with your website that you have copied in Step W3. Then initiate a git repository on the command line, connect it to the repository created in Steph G2 on GitHub and finally push it:\ncd /home/sthu/my_website/\necho \"# test\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/test-hsf/test.git\ngit push -u origin main\nAlternatively, you can clone a repository, make some changes, and then push those changes back to GitHub. Here are the Bash commands to accomplish this:\n# Clone the repository\ngit clone https://github.com/your-username/your-repository.git\n\n# Make changes, here adding a new file as an example\necho \"Some content for the new file\" &gt; newfile.txt\n\n# Add the new file to the repository\ngit add newfile.txt\n\n# Commit the changes\ngit commit -m \"Add new file\"\n\n# Push the changes back to GitHub\ngit push origin main",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bryan, J. (2023). Happy git and GitHub for the useR. https://happygitwithr.com/\n\n\nDavenport, T. H., & Patil, D. (2012). Data scientist: The sexiest\njob of the 21st century. Harvard Business Review,\n90(5), 70–76.\n\n\nHalbritter, A., & Telford, R. J. (2023). Version control with\ngit and GitHub. https://biostats-r.github.io/biostats/github/\n\n\nHuber, S. (2024a). How to use R for data science.\nhttps://hubchev.github.io/ds/\n\n\nHuber, S. (2024b). Quantitative methods: Lecture notes. https://hubchev.github.io/qm/\n\n\nMartin, R. (2007). How successful leaders think. Harvard Business\nReview, 85(6), 75–83.\n\n\nOpenAI. (2024). ChatGPT (april 18, 2024). Large language model.\nhttps://chat.openai.comt\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/\n\n\nWooldridge, J. M. (2002). Introductory econometrics: A modern approach.\nIn Delhi: Cengage Learnng (2nd ed.). South-Western.\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2018). R markdown: The\ndefinitive guide. Chapman; Hall/CRC.\n\n\nXie, Y., Dervieux, C., & Riederer, E. (2020). R markdown\ncookbook. Chapman; Hall/CRC.",
    "crumbs": [
      "References"
    ]
  }
]