[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Business Leaders",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "esyl.html",
    "href": "esyl.html",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "1.1 Syllabus\nScope and Nature of Data Science\nEmerging Trends in a Data-Driven Business Environment\nData Science Process in Business\nData Literacy\nOverview of Data Science Methods\nIntroduction to Data Scientific Tools",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-osly",
    "href": "esyl.html#sec-osly",
    "title": "1  (Extended) Syllabus",
    "section": "",
    "text": "Defining data science as an academic discipline (informatics, computer science, mathematics, statistics, econometrics, social science)\nImportance of data science in businesses\n\n\n\nEvolution of computers, computing, and data processing\nBusiness intelligence (performance marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\n\nWorkflows and data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve data)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview of programming languages (R, Python, SQL, …)\nOverview of no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-howchatgptwrote",
    "href": "esyl.html#sec-howchatgptwrote",
    "title": "1  (Extended) Syllabus",
    "section": "1.2 How I let ChatGPT wrote the extended syllabus",
    "text": "1.2 How I let ChatGPT wrote the extended syllabus\nHere are the seven prompts to ChatGPT 4.0:\n\nAct as a professor of data science. Write lecture notes for management students. The first chapter of the notes should contain the following:\n\nScope and nature of data science\n\nDefining data science as an academic discipline (informatics, computer science, mathematics,statistics, econometrics, social science)\nImportance of data science in businesses\n\n\nThe chapter of the lecture notes should contain the following:\n\nEmerging Trends in a Data-Driven Business Environment\n\nEvolution of computers, computing, and data processing\nBusiness Intelligence (Performance Marketing, etc.)\nArtificial intelligence, machine learning, deep learning, and algorithms\nBig data\nInternet of things, cloud computing, blockchain\nIndustry 4.0 and remote working\n\n\nThe third chapter of the lecture notes should contain the following:\n\nData science process in business\n\nWorkflows and Data science life cycles (OSEMN, CRISP-DM, Kanban, TDSP, …)\nTypes of data science roles (data engineer, data analyst, machine learning engineer, business intelligence analyst, database administrator, data product manager, …)\n\n\nDo the same for the fourth section, which contains:\n\nData literacy\n\nConceptual framework (knowledge and understanding of data and applications of data)\nData collection (identify, collect, and assess data)\nData management (organize, clean, convert, curate, and preserve Kdata)\nData evaluation (plan, conduct, evaluate, and assess data analyses)\nData application (share, reflect, and evaluate results of analyses and compare them with other findings considering ethical issues and scientific standards)\n\n\nAnd the next section:\n\nOverview of data science methods\n\nData exploration and data mining\nSupervised and unsupervised learning\nRegression and classification\nPredictive analysis\nCausal analysis\n\n\nAnd the final section:\n\nIntroduction to data scientific tools\n\nWriting and publishing reports (Markdown, Quarto)\nCollaborating in teams using a version control system (git)\nOverview on Programming languages (R, Phyton, SQL, …)\nOverview on no-code and low-code tools for data science (makeML, PyCaret, Rapidminer, KNIME, etc.)\nDevelopment environments (Unix-like systems, containers, APIs, Jupyter, Rstudio, etc.)\n\n\nI am not happy with the third section. In particular the section “Workflows and Data Science Life Cycles”. Can you re-write it? Consider the learning objective a bit better, that is, summarize the strengths, weaknesses and use-cases of data science processes and workflows and demonstrate the ability to justify and explain tools and methods used along the process.\n\nI manually cut some parts of the last answer and pasted it into the respective section.\nI assembled everything by copy and paste it step-by-step into Quarto and render it. That was all I did to create the notes.\n\n\n\n\n\n\n\nExercise 1.1 What do think about the extended syllabus and my approach\nWhat do you think about the extended syllabus? Is it well-written? Does it include a lot of details or errors? What aspects do you think we should work on manually? How could my prompts be improved to yield better results?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "esyl.html#sec-esylgpt",
    "href": "esyl.html#sec-esylgpt",
    "title": "1  (Extended) Syllabus",
    "section": "1.3 The ChatGPT generated extended syllabus",
    "text": "1.3 The ChatGPT generated extended syllabus\nThe following text was generated as described in Section 1.2 using OpenAI (2024).\n\n1.3.1 Scope and Nature of Data Science\nWelcome to the introductory chapter on Data Science, designed specifically for management students. In this chapter, we will explore the multifaceted discipline of data science, understanding its definitions, scope, and the pivotal role it plays in the business world today.\n\n1.3.1.1 Defining Data Science\nData Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It is a confluence of several disciplines including informatics, computer science, mathematics, statistics, econometrics, and social science. This integration allows for a comprehensive approach to solving complex problems, making informed decisions, and generating actionable insights through data analysis.\n\n1.3.1.1.1 Informatics\nIt is the study of computational systems and the way humans interact with these systems. In data science, informatics plays a critical role in the management and processing of data.\n\n\n1.3.1.1.2 Computer Science\nThis foundational pillar focuses on the development of algorithms and software that are used to process, store, and analyze data.\n\n\n1.3.1.1.3 Mathematics and Statistics\nMathematics provides the theoretical foundation for algorithms and models, while statistics is crucial for understanding and interpreting data. Together, they enable data scientists to perform quantitative analysis and predictive modeling.\n\n\n1.3.1.1.4 Econometrics\nEconometrics applies statistical methods to economic data, enabling the analysis of economic relationships. In data science, econometrics helps in forecasting economic trends and evaluating economic policies through data.\n\n\n1.3.1.1.5 Social Science\nThe incorporation of social science principles allows data scientists to understand human behavior and societal trends, which is invaluable for businesses looking to market their products or services effectively.\n\n\n\n1.3.1.2 Importance of Data Science in Businesses\nData Science has become a cornerstone in the business world, offering unprecedented opportunities for growth, efficiency, and innovation. Here’s how:\n\n1.3.1.2.1 Data-Driven Decision Making\nBusinesses can make more informed decisions by analyzing vast amounts of data. This leads to better strategic planning, operational efficiency, and competitive advantage.\n\n\n1.3.1.2.2 Customer Insights\nThrough data analysis, companies can gain deep insights into customer behavior, preferences, and trends. This information is critical for tailoring marketing strategies, improving customer experiences, and boosting customer loyalty.\n\n\n1.3.1.2.3 Risk Management\nData science enables businesses to identify and mitigate risks by analyzing historical data, detecting patterns, and predicting future outcomes. This is particularly useful in finance, insurance, and cybersecurity.\n\n\n1.3.1.2.4 Innovation and Product Development\nBy understanding market needs and customer feedback through data analysis, businesses can innovate and develop new products and services that meet or exceed customer expectations.\n\n\n1.3.1.2.5 Operational Efficiency\nData science can optimize business operations, reduce costs, and improve service delivery through the analysis of operational data, leading to streamlined processes and better resource management.\n\n\n1.3.1.2.6 Competitive Edge\nIn a data-driven world, businesses that leverage data science effectively can stay ahead of the curve, identifying trends and opportunities before their competitors.\nIn conclusion, data science is an essential academic discipline that integrates various fields to analyze and interpret complex data. Its importance in the business sector cannot be overstated, as it enables organizations to make informed decisions, understand their customers, manage risks, foster innovation, improve operations, and maintain a competitive edge in the market. As we delve deeper into this course, we will explore the methodologies, tools, and applications of data science in greater detail, equipping you with the knowledge and skills to leverage data science in your future management roles.\n\n\n\n\n1.3.2 Emerging Trends in a Data-Driven Business Environment\nIn this chapter, we explore the dynamic landscape of a data-driven business environment. The rapid evolution of technology and its integration into business processes have led to the emergence of new trends that are shaping the future of industries across the globe. Let’s dive into these trends and understand their significance.\n\n1.3.2.1 Evolution of Computers, Computing, and Data Processing\nThe journey from the first generation of computers to the modern era of quantum computing marks a significant evolution in technology. Initially, computers were large, expensive, and limited in their capabilities. Over the decades, advancements in semiconductor technology, the invention of the microprocessor, and the development of personal computers transformed computing into an accessible and essential tool for businesses. Today, cloud computing and edge computing have further revolutionized data processing, allowing for more efficient data storage, access, and analysis.\n\n\n1.3.2.2 Business Intelligence\nBusiness Intelligence (BI) refers to the use of data analysis in business to support decision-making processes. BI tools analyze historical and current data to provide actionable insights, helping businesses to improve their performance. Performance marketing, a subset of BI, focuses on analyzing marketing campaigns in real time to optimize marketing strategies and expenditures for better ROI.\n\n\n1.3.2.3 Artificial Intelligence, Machine Learning, Deep Learning, and Algorithms\nArtificial Intelligence (AI) and its subsets, Machine Learning (ML) and Deep Learning (DL), are at the forefront of technological innovation. AI involves creating systems capable of performing tasks that typically require human intelligence. ML and DL are about teaching computers to learn from data, improving their accuracy over time without being explicitly programmed. These technologies are transforming business operations, from customer service automation and predictive analytics to personalized marketing and decision-making processes.\n\n\n1.3.2.4 Big Data\nBig Data refers to the vast volumes of data generated every second from various sources like social media, business transactions, and IoT devices. The ability to process and analyze this data has unlocked new opportunities for businesses to gain insights into customer behavior, market trends, and operational efficiency. Big data analytics is now a crucial tool for strategic planning and competitive analysis.\n\n\n1.3.2.5 Internet of Things, Cloud Computing, Blockchain\n\nInternet of Things (IoT): IoT technology connects everyday devices to the internet, enabling them to send and receive data. This interconnectivity offers businesses real-time insights into their operations, asset tracking, and supply chain management.\nCloud Computing: Cloud computing provides businesses with scalable computing resources over the internet, facilitating remote data storage, processing, and management. It supports flexibility, reduces IT costs, and enhances collaboration.\nBlockchain: Blockchain technology offers a secure, decentralized platform for transactions. It’s particularly valuable for enhancing transparency, security, and efficiency in business operations, supply chain management, and financial services.\n\n\n\n1.3.2.6 Industry 4.0 and Remote Working\nIndustry 4.0, also known as the fourth industrial revolution, integrates digital technologies into manufacturing and industry, including IoT, AI, and robotics. It represents a shift towards smart, automated production processes and data exchange. Coupled with the rise of remote working, Industry 4.0 technologies enable businesses to operate more flexibly, with teams collaborating effectively from various locations, leveraging digital tools and platforms for communication and project management.\nIn conclusion, the evolution of technology and its integration into business practices have brought about significant changes in the way companies operate. From the way data is processed and analyzed to the automation of manufacturing processes and the flexibility of remote working, these emerging trends are shaping the future of a data-driven business environment. As we move forward, understanding and adapting to these trends will be crucial for businesses looking to thrive in the digital age.\n\n\n\n1.3.3 Data Science Process in Business\nThis chapter delves into the structured approach behind the application of data science in business settings. We will explore various data science workflows and life cycles that guide the process from raw data to actionable insights. Additionally, we will outline the different roles within a data science team and their contributions to this process.\n\n1.3.3.1 Workflows and Data Science Life Cycles\nData science projects in business environments follow structured workflows and life cycles to ensure that the analysis is efficient, reproducible, and scalable. Several frameworks guide these processes, each with its strengths and applications.\n\n1.3.3.1.1 OSEMN Framework\nOSEMN (Obtain, Scrub, Explore, Model, iNterpret) is a streamlined approach to data science projects:\n\nObtain: Acquiring the data from various sources.\nScrub: Cleaning the data to ensure it is accurate and usable.\nExplore: Analyzing the data to find patterns and relationships.\nModel: Applying statistical models to predict or classify data.\nInterpret: Drawing conclusions and making recommendations based on the model’s results.\n\n\nStrengths: The OSEMN (Obtain, Scrub, Explore, Model, iNterpret) framework is straightforward and easy to understand, making it accessible for teams of all skill levels. It covers the essential steps of a data science project in a logical sequence.\nWeaknesses: Its simplicity may overlook the complexity of certain stages, such as model validation or deployment.\nUse-Cases: Ideal for small to medium-sized projects where the primary goal is to gain insights from data through exploration and modeling.\n\n\n\n1.3.3.1.2 CRISP-DM\nCRISP-DM stands for Cross-Industry Standard Process for Data Mining. It’s a comprehensive framework that includes six phases:\n\nBusiness Understanding: Define the project objectives and requirements.\nData Understanding: Collect and explore the data.\nData Preparation: Clean and preprocess the data.\nModeling: Select and apply modeling techniques.\nEvaluation: Assess the model’s performance.\nDeployment: Implement the model in a real-world setting.\n\n\nStrengths: CRISP-DM (Cross-Industry Standard Process for Data Mining) is industry-agnostic and provides a detailed structure that includes understanding the business problem and deploying the solution. It encourages iterative learning and refinement.\nWeaknesses: Can be perceived as too rigid for projects requiring rapid development and deployment. The model doesn’t explicitly address the updating or maintenance of deployed solutions.\nUse-Cases: Suitable for projects that require close alignment with business objectives and thorough consideration of deployment strategies.\n\n\n\n1.3.3.1.3 Kanban\nKanban is a lean method to manage and improve work across human systems. In data science, it helps in visualizing work, limiting work-in-progress, and maximizing efficiency.\n\nStrengths: Kanban is highly flexible and promotes continuous delivery. It allows teams to adapt quickly to changes and prioritize tasks effectively.\nWeaknesses: Without strict stages or phases, projects might lack direction or oversight, potentially leading to inefficiencies.\nUse-Cases: Best for dynamic environments where priorities shift frequently and teams must remain agile to respond to business needs.\n\n\n\n1.3.3.1.4 TDSP (Team Data Science Process)\nTDSP is a standardized approach to data science projects that helps teams to improve quality and efficiency. It includes:\n\nStrengths: TDSP offers a structured approach with a strong emphasis on standardized documentation and project management methodologies, facilitating collaboration and scalability.\nWeaknesses: Its comprehensive nature might introduce overhead and slow down smaller projects.\nUse-Cases: Ideal for larger teams working on complex projects that require coordination across different roles and departments.\n\n\n\n\n1.3.3.2 Types of Data Science Roles\nIn a business environment, a data science team might consist of various specialized roles, each contributing uniquely to the data science process.\n\n1.3.3.2.1 Data Engineer\nFocuses on the design, construction, and maintenance of the systems that data analysts and data scientists use for their work. They ensure that data flows smoothly from source to database to analytics.\n\n\n1.3.3.2.2 Data Analyst\nWorks on processing and performing statistical analysis on existing datasets. They interpret the data to help the business make more informed decisions.\n\n\n1.3.3.2.3 Machine Learning Engineer\nDevelops algorithms and predictive models to solve specific business problems using machine learning techniques.\n\n\n1.3.3.2.4 Business Intelligence Analyst\nAnalyzes data to provide insights that help businesses with strategic planning. They use BI tools to convert data into understandable reports and dashboards.\n\n\n1.3.3.2.5 Database Administrator\nResponsible for managing, backing up, and ensuring the availability of the data stored in an organization’s databases.\n\n\n1.3.3.2.6 Data Product Manager\nOversees the development of data-driven products or services, ensuring that they meet the users’ needs and the business objectives.\nIn summary, the data science process in business involves a structured approach to turning data into actionable insights. This process is supported by various frameworks and relies on the collaboration of professionals in specialized roles. Understanding these aspects of data science is crucial for anyone looking to leverage this discipline in a business context.\n\n\n\n\n1.3.4 Data Literacy\nData literacy is the ability to read, understand, create, and communicate data as information. It encompasses a broad range of skills necessary for effectively working with data, from the initial stages of data collection to the final stages of analyzing and sharing findings. In this chapter, we will break down the conceptual framework of data literacy and explore its various components in detail.\n\n1.3.4.1 Conceptual Framework\nAt the heart of data literacy is a deep knowledge and understanding of how data can be used to make decisions, solve problems, and communicate ideas. This conceptual framework involves:\n\nUnderstanding the nature of data: Recognizing different types of data (quantitative vs. qualitative) and their sources.\nComprehending the applications of data: Knowing how data can be used in various contexts to derive insights and inform decisions.\n\n\n\n1.3.4.2 Data Collection\nThe first step in the data lifecycle involves identifying, collecting, and assessing data:\n\nIdentify: Determining the data needed to answer a question or solve a problem.\nCollect: Gathering data from various sources, whether they are existing datasets or new data collected through surveys, experiments, or observations.\nAssess: Evaluating the quality of the data, including its relevance, accuracy, and completeness.\n\n\n\n1.3.4.3 Data Management\nOnce data is collected, it must be managed effectively:\n\nOrganize: Arranging data in a structured format that facilitates analysis.\nClean: Removing errors or inconsistencies in the data.\nConvert: Transforming data into a format suitable for analysis.\nCurate: Selecting, annotating, and maintaining valuable data for current and future use.\nPreserve: Ensuring that data remains accessible and usable over time.\n\n\n\n1.3.4.4 Data Evaluation\nEvaluation is critical to understanding what the data signifies:\n\nPlan: Designing a methodology for analyzing the data.\nConduct: Performing the analysis using appropriate statistical methods and tools.\nEvaluate: Assessing the quality and reliability of the analysis.\nAssess: Interpreting the results in the context of the research question or business problem.\n\n\n\n1.3.4.5 Data Application\nThe final step involves applying the insights gained from data analysis:\n\nShare: Communicating findings to stakeholders through reports, presentations, or visualizations.\nReflect: Considering the implications of the results and how they can inform future actions.\nEvaluate results: Comparing findings with those from other studies or data analyses to draw broader conclusions.\nEthical considerations: Ensuring that the use of data respects privacy, confidentiality, and ethical standards.\nScientific standards: Adhering to rigorous standards of validity, reliability, and objectivity in data handling and analysis.\n\nIn summary, data literacy is a comprehensive set of skills that enable individuals to navigate the complex world of data from collection to application. By understanding and applying the concepts outlined in this chapter, individuals can enhance their ability to make informed decisions, solve problems, and communicate effectively using data.\n\n\n\n1.3.5 Overview of Data Science Methods\nData science encompasses a wide array of methods and techniques for analyzing data, drawing insights, and making predictions. This chapter provides an overview of some core data science methods, including data exploration, data mining, machine learning approaches, and various types of analyses.\n\n1.3.5.1 Data Exploration and Data Mining\n\n1.3.5.1.1 Data Exploration\nData exploration involves analyzing data sets to find initial patterns, characteristics, and points of interest without making any prior assumptions. It typically includes summarizing the main characteristics of the data through visualizations and statistics.\n\n\n1.3.5.1.2 Data Mining\nData mining is the process of discovering patterns and knowledge from large amounts of data. The data sources can include databases, data warehouses, the internet, and other sources. Data mining techniques include clustering, classification, regression, and association rule learning.\n\n\n\n1.3.5.2 Supervised and Unsupervised Learning\n\n1.3.5.2.1 Supervised Learning\nSupervised learning is a type of machine learning where the algorithm learns from labeled training data, helping to predict outcomes for unforeseen data. It is divided into two main categories: regression and classification.\n\n\n1.3.5.2.2 Unsupervised Learning\nUnsupervised learning involves training on data without labeled responses. The system tries to learn the patterns and the structure from the data without any supervision. Common unsupervised learning methods include clustering and dimensionality reduction.\n\n\n\n1.3.5.3 Regression and Classification\n\n1.3.5.3.1 Regression\nRegression methods are used to predict a continuous outcome variable based on one or more predictor variables. The goal is to find the relationship between variables and forecast an outcome. Linear regression is one of the most basic types of regression analysis.\n\n\n1.3.5.3.2 Classification\nClassification methods are used to predict or identify the category to which a new observation belongs. Examples include spam detection in email service providers and customer churn prediction.\n\n\n\n1.3.5.4 Predictive Analysis\nPredictive analysis uses statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. It’s used in various fields, including finance, healthcare, marketing, and weather forecasting, to make more informed decisions.\n\n\n1.3.5.5 Causal Analysis\nCausal analysis seeks to identify and understand the cause-and-effect relationships between variables. Unlike correlation, which merely indicates that two variables move together, causation establishes that a change in one variable is responsible for a change in another.\nIn conclusion, these methods and techniques form the backbone of data science, enabling professionals to extract valuable insights, make predictions, and inform decision-making processes. Understanding these methods is crucial for anyone looking to delve into data science or apply its principles in their field.\n\n\n\n1.3.6 Introduction to Data Scientific Tools\nThe practice of data science requires not only a solid understanding of theories and methodologies but also proficiency in a variety of tools and technologies. This chapter introduces essential tools for writing and publishing reports, collaborating in teams, programming, as well as no-code and low-code platforms, and development environments.\n\n1.3.6.1 Writing and Publishing Reports\n\n1.3.6.1.1 Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. Its simplicity and ease of conversion to HTML and other formats make it an ideal choice for writing and publishing reports, documentation, and articles.\n\n\n1.3.6.1.2 Quarto\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It enables users to create dynamic and reproducible reports and articles that can include executable code from various programming languages, such as R and Python.\n\n\n\n1.3.6.2 Collaborating in Teams Using a Version Control System\n\n1.3.6.2.1 Git\nGit is a distributed version control system that enables multiple developers to work together on the same project efficiently. It tracks changes in source code during software development, supporting collaboration and fostering code integrity.\n\n\n\n1.3.6.3 Overview of Programming Languages\n\n1.3.6.3.1 R\nR is a programming language and free software environment for statistical computing and graphics, widely used among statisticians and data miners.\n\n\n1.3.6.3.2 Python\nPython is a high-level, interpreted programming language known for its simplicity and versatility. It has a wide range of libraries for data analysis, machine learning, and data visualization, making it a popular choice in data science.\n\n\n1.3.6.3.3 SQL\nSQL (Structured Query Language) is the standard language for managing and manipulating relational databases. It allows users to query, update, and manage data.\n\n\n\n1.3.6.4 Overview of No-Code and Low-Code Tools for Data Science\n\n1.3.6.4.1 makeML\nA no-code platform for machine learning, makeML simplifies the process of training and deploying ML models without writing extensive code.\n\n\n1.3.6.4.2 PyCaret\nPyCaret is a low-code machine learning library in Python that automates machine learning workflows. It enables data scientists to perform end-to-end experiments quickly and efficiently.\n\n\n1.3.6.4.3 Rapidminer\nRapidminer is a data science platform that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics.\n\n\n1.3.6.4.4 KNIME\nKNIME is an open-source, graphical workbench for the entire analysis process: data access, data transformation, initial investigation, powerful predictive analytics, visualization, and reporting.\n\n\n\n1.3.6.5 Development Environments\n\n1.3.6.5.1 Unix-like Systems\nUnix-like operating systems, including Linux and macOS, provide powerful tools and environments for software development and data science.\n\n\n1.3.6.5.2 Containers\nContainers, such as Docker, allow for the packaging of applications and their dependencies in a virtual container that can run on any Linux server, enabling easy deployment and scalability.\n\n\n1.3.6.5.3 APIs\nApplication Programming Interfaces (APIs) enable different software applications to communicate with each other, facilitating data exchange and integration.\n\n\n1.3.6.5.4 Jupyter\nJupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text.\n\n\n1.3.6.5.5 RStudio\nRStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for coding, debugging, and visualizing data.\nIn summary, the array of tools and technologies available to data scientists is broad and varied, catering to different aspects of the data science workflow. From data manipulation and analysis to collaboration and report writing, mastering these tools is essential for effective data science practice.\n\n\n\n\nOpenAI. (2024). ChatGPT (april 18, 2024). Large language model. https://chat.openai.comt",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>(Extended) Syllabus</span>"
    ]
  },
  {
    "objectID": "54_quarto.html",
    "href": "54_quarto.html",
    "title": "2  Markdown and Quarto",
    "section": "",
    "text": "Verbal and non-verbal communication is important in business. This section is about writing and publishing texts, leaving out body language and writing skills. I will introduce some applications (Markdown, RMarkdown, Quarto) that data scientists often use to write and publish their work. I will also discuss the version control system git and the online platform GitHub, which can be used to create, store, manage and share files. These tools are the backbone of most data science collaborations. Once you master these tools, they can significantly enhance your efficiency and make your presentations more impactful, even if you are not directly involved in the field of data science.\n\nQuarto, a modern documentation system, is an excellent choice for writing, especially for projects that require rigorous data analysis, visualization, and reproducibility. This tutorial will guide you through producing various forms of text with Quarto. You can write reports, articles, theses, books, websites and many more with Quarto.\nStep 1: Learn Markdown\nMarkdown is a lightweight markup language with plain-text formatting syntax. It’s an essential skill for using Quarto effectively. Start by learning enough Markdown to structure your thesis, including headings, lists, links, and code blocks.\nYou can learn Markdown (not R Markdown!) in 10 minutes. Just go to https://www.markdowntutorial.com and work throught the interactive lessons.\nStep 2: Learn Quarto\nRead Telford (2023): Enough Markdown to Write a Thesis. This resource covers the basics and some advanced Markdown features that are useful for academic writing.\nMore extensive resources on how to do things with Quarto can be found at quarto.org.\n\n\n\n\n\n\nQuarto and R markdown\n\n\n\nQuarto is a relatively new tool. It can be considered the successor to R Markdown, as it is built upon R Markdown. Consequently, almost all R Markdown documents are compatible with Quarto. However, Quarto includes several improvements over R Markdown that enhance its ease of use. For a detailed description of all the differences and similarities between the two, you can read this article.\n\n\n\n\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/",
    "crumbs": [
      "Writing and publishing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Markdown and Quarto</span>"
    ]
  },
  {
    "objectID": "website.html",
    "href": "website.html",
    "title": "3  Create and host a website",
    "section": "",
    "text": "3.1 Creating a website with Quarto\nThis tutorial guides you through creating a simple, yet professional-looking website using Quarto.\nStep 1: Install Quarto\nEnsure Quarto is installed on your system. If not, download and install it from Quarto’s official website.\nStep 2: Create a website\nFollow the tutorial that you find here.",
    "crumbs": [
      "Writing and publishing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "website.html#hosting-the-website-on-github",
    "href": "website.html#hosting-the-website-on-github",
    "title": "3  Create and host a website",
    "section": "3.2 Hosting the website on GitHub",
    "text": "3.2 Hosting the website on GitHub\nR Studio and Quarto offers you various ways to publish the website. I explain you a way that worked out well for me.\nStep 1: Create a GitHub account\nGitHub will host your thesis website and manage version control for your thesis project. If you don’t already have a GitHub account, you’ll need to create one: Sign up at GitHub.\nStep 2: Create a reopository\nCreate a repository. Name the repo with your username followed by github.io. You find a tutorial here.\nStep 3: Obtain a personal access token\nA personal access token (PAT) is required to authenticate with GitHub from Quarto and RStudio. This token allows you to push changes to your repository securely. Follow the instructions to create a personal access token on GitHub.\nMake sure to note down your token and keep it secure. You’ll use this token in RStudio and Quarto to authenticate your GitHub operations.\nStep 4: Install and Learn Git\nTo install the version control system Git, follow the instructions here.\nFamiliarize yourself with Git using the resources available here. Although Git might seem complex, it’s actually not too challenging for most users. Many people use Git primarily to track their work and to host and share files conveniently.\nA humorous figure Figure 3.1 illustrates that while Git is a robust system with many capabilities, you don’t need to remember all the commands. In fact, you typically use just a few basic commands. The table Table 3.1 lists the most important ones.\n\n\n\nFigure 3.1: Memorizing six git commands\n\n\n\nSource: DEV Community on GitHub\n\n\n\n\n\n\nTable 3.1: Most important git commands\n\n\n\n\n\n\n\n\n\nGit Command\nDescription\n\n\n\n\ngit init\nInitialize a new Git repository in the current directory.\n\n\ngit clone &lt;url&gt;\nClone a repository from a remote URL to your local machine.\n\n\ngit add &lt;file&gt;\nAdd a specific file to the staging area in preparation for committing.\n\n\ngit add .\nAdd all changed files in the current directory to the staging area.\n\n\ngit commit -m \"message\"\nCommit the staged changes to the repository with a descriptive message.\n\n\ngit status\nDisplay the status of the working directory and staging area.\n\n\ngit push &lt;remote&gt; &lt;branch&gt;\nPush committed changes in your local branch to the remote repository.\n\n\ngit pull &lt;remote&gt; &lt;branch&gt;\nPull changes from the remote repository into your current branch and merge them.\n\n\ngit branch &lt;name&gt;\nCreate a new branch with the specified name.\n\n\ngit checkout &lt;branch&gt;\nSwitch to another branch and update the working directory.\n\n\ngit merge &lt;branch&gt;\nMerge a specified branch into the current branch.\n\n\n\n\n\n\nStep 5: Upload the website to GitHub\nUse the Terminal of R Studio. Go to the docs folder that is in the folder in which you have created the website. Then create a new repository on the command line:\necho \"# test\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin https://github.com/test-hsf/test.git\ngit push -u origin main\nAlternatively, you can clone a repository, make some changes, and then push those changes back to GitHub. Here are the Bash commands to accomplish this:\n# Clone the repository\ngit clone https://github.com/your-username/your-repository.git\n\n# Make changes, here adding a new file as an example\necho \"Some content for the new file\" &gt; newfile.txt\n\n# Add the new file to the repository\ngit add newfile.txt\n\n# Commit the changes\ngit commit -m \"Add new file\"\n\n# Push the changes back to GitHub\ngit push origin main\n\n\n\nFigure 3.1: Memorizing six git commands",
    "crumbs": [
      "Writing and publishing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Create and host a website</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "OpenAI. (2024). ChatGPT (april 18, 2024). Large language model.\nhttps://chat.openai.comt\n\n\nTelford, R. J. (2023). Enough markdown to write a thesis. https://biostats-r.github.io/biostats/quarto/",
    "crumbs": [
      "References"
    ]
  }
]